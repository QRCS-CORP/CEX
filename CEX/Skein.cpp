#include "Skein.h"
#include "IntUtils.h"
#include "MemUtils.h"

NAMESPACE_DIGEST

using Utility::IntUtils;
using Utility::MemUtils;

//~~~Skein-256~~~//

void Skein::PemuteR72P256C(std::array<ulong, 4> &Input, std::array<ulong, 4> &State, std::array<ulong, 2> &Tweak)
{
	std::array<ulong, 4> B;
	std::array<ulong, 5> K;
	std::array<ulong, 3> T;
	size_t i;
	size_t x;
	size_t y;

	MemUtils::Copy(Input, 0, B, 0, 4 * sizeof(ulong));
	MemUtils::Copy(State, 0, K, 0, 4 * sizeof(ulong));
	MemUtils::Copy(Tweak, 0, T, 0, 2 * sizeof(ulong));

	x = 1;
	y = 0;
	K[4] = K[0] ^ K[1] ^ K[2] ^ K[3] ^ 0x1BD11BDAA9FC1A22ULL;
	T[2] = T[0] ^ T[1];

	for (i = 0; i < 9; ++i)
	{
		// round n+8+8, inject k
		B[1] += K[x] + T[y];
		x != 0 ? x -= 1 : x += 4;
		B[0] += B[1] + K[x];
		B[1] = Utility::IntUtils::RotL64(B[1], 14) ^ B[0];
		// mix
		x > 1 ? x -= 2 : x += 3;
		B[3] += K[x] + (i * 2);
		x > 0 ? x -= 1 : x += 4;
		y != 2 ? y += 1 : y -= 2;
		B[2] += B[3] + K[x] + T[y];
		B[3] = Utility::IntUtils::RotL64(B[3], 16) ^ B[2];
		B[0] += B[3];
		B[3] = Utility::IntUtils::RotL64(B[3], 52) ^ B[0];
		B[2] += B[1];
		B[1] = Utility::IntUtils::RotL64(B[1], 57) ^ B[2];
		B[0] += B[1];
		B[1] = Utility::IntUtils::RotL64(B[1], 23) ^ B[0];
		B[2] += B[3];
		B[3] = Utility::IntUtils::RotL64(B[3], 40) ^ B[2];
		B[0] += B[3];
		B[3] = Utility::IntUtils::RotL64(B[3], 5) ^ B[0];
		B[2] += B[1];
		B[1] = Utility::IntUtils::RotL64(B[1], 37) ^ B[2];
		// inject
		B[1] += K[x] + T[y];
		x != 0 ? x -= 1 : x += 4;
		B[0] += B[1] + K[x];
		B[1] = Utility::IntUtils::RotL64(B[1], 25) ^ B[0];
		// mix
		x > 1 ? x -= 2 : x += 3;
		B[3] += K[x] + (i * 2) + 1;
		x != 0 ? x -= 1 : x += 4;
		y != 2 ? y += 1 : y -= 2;
		B[2] += B[3] + K[x] + T[y];
		B[3] = Utility::IntUtils::RotL64(B[3], 33) ^ B[2];
		B[0] += B[3];
		B[3] = Utility::IntUtils::RotL64(B[3], 46) ^ B[0];
		B[2] += B[1];
		B[1] = Utility::IntUtils::RotL64(B[1], 12) ^ B[2];
		B[0] += B[1];
		B[1] = Utility::IntUtils::RotL64(B[1], 58) ^ B[0];
		B[2] += B[3];
		B[3] = Utility::IntUtils::RotL64(B[3], 22) ^ B[2];
		B[0] += B[3];
		B[3] = Utility::IntUtils::RotL64(B[3], 32) ^ B[0];
		B[2] += B[1];
		B[1] = Utility::IntUtils::RotL64(B[1], 32) ^ B[2];
	}

	B[0] += K[3];
	B[1] += K[4] + T[0];
	B[2] += K[0] + T[1];
	B[3] += K[1] + 18;

	MemUtils::Copy(B, 0, State, 0, 4 * sizeof(ulong));
}

void Skein::PemuteR72P256U(std::array<ulong, 4> &Input, std::array<ulong, 4> &State, std::array<ulong, 2> &Tweak)
{
	ulong B0 = Input[0];
	ulong B1 = Input[1];
	ulong B2 = Input[2];
	ulong B3 = Input[3];
	ulong K0 = State[0];
	ulong K1 = State[1];
	ulong K2 = State[2];
	ulong K3 = State[3];
	ulong K4 = K0 ^ K1 ^ K2 ^ K3 ^ 0x1BD11BDAA9FC1A22ULL;
	ulong T0 = Tweak[0];
	ulong T1 = Tweak[1];
	ulong T2 = Tweak[0] ^ Tweak[1];

	// rounds 0-7, inject k
	B1 += K1 + T0;
	B0 += B1 + K0;
	B1 = Utility::IntUtils::RotL64(B1, 14) ^ B0;
	// mix
	B3 += K3;
	B2 += B3 + K2 + T1;
	B3 = Utility::IntUtils::RotL64(B3, 16) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 52) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 57) ^ B2;
	B0 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 23) ^ B0;
	B2 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 40) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 5) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 37) ^ B2;
	// inject
	B1 += K2 + T1;
	B0 += B1 + K1;
	B1 = Utility::IntUtils::RotL64(B1, 25) ^ B0;
	// mix
	B3 += K4 + 1;
	B2 += B3 + K3 + T2;
	B3 = Utility::IntUtils::RotL64(B3, 33) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 46) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 12) ^ B2;
	B0 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 58) ^ B0;
	B2 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 22) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 32) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 32) ^ B2;
	// rounds 8-15
	B1 += K3 + T2;
	B0 += B1 + K2;
	B1 = Utility::IntUtils::RotL64(B1, 14) ^ B0;
	B3 += K0 + 2;
	B2 += B3 + K4 + T0;
	B3 = Utility::IntUtils::RotL64(B3, 16) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 52) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 57) ^ B2;
	B0 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 23) ^ B0;
	B2 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 40) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 5) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 37) ^ B2;
	B1 += K4 + T0;
	B0 += B1 + K3;
	B1 = Utility::IntUtils::RotL64(B1, 25) ^ B0;
	B3 += K1 + 3;
	B2 += B3 + K0 + T1;
	B3 = Utility::IntUtils::RotL64(B3, 33) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 46) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 12) ^ B2;
	B0 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 58) ^ B0;
	B2 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 22) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 32) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 32) ^ B2;
	// rounds 16-23
	B1 += K0 + T1;
	B0 += B1 + K4;
	B1 = Utility::IntUtils::RotL64(B1, 14) ^ B0;
	B3 += K2 + 4;
	B2 += B3 + K1 + T2;
	B3 = Utility::IntUtils::RotL64(B3, 16) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 52) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 57) ^ B2;
	B0 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 23) ^ B0;
	B2 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 40) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 5) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 37) ^ B2;
	B1 += K1 + T2;
	B0 += B1 + K0;
	B1 = Utility::IntUtils::RotL64(B1, 25) ^ B0;
	B3 += K3 + 5;
	B2 += B3 + K2 + T0;
	B3 = Utility::IntUtils::RotL64(B3, 33) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 46) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 12) ^ B2;
	B0 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 58) ^ B0;
	B2 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 22) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 32) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 32) ^ B2;
	// rounds 24-31
	B1 += K2 + T0;
	B0 += B1 + K1;
	B1 = Utility::IntUtils::RotL64(B1, 14) ^ B0;
	B3 += K4 + 6;
	B2 += B3 + K3 + T1;
	B3 = Utility::IntUtils::RotL64(B3, 16) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 52) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 57) ^ B2;
	B0 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 23) ^ B0;
	B2 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 40) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 5) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 37) ^ B2;
	B1 += K3 + T1;
	B0 += B1 + K2;
	B1 = Utility::IntUtils::RotL64(B1, 25) ^ B0;
	B3 += K0 + 7;
	B2 += B3 + K4 + T2;
	B3 = Utility::IntUtils::RotL64(B3, 33) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 46) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 12) ^ B2;
	B0 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 58) ^ B0;
	B2 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 22) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 32) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 32) ^ B2;
	// rounds 32-39
	B1 += K4 + T2;
	B0 += B1 + K3;
	B1 = Utility::IntUtils::RotL64(B1, 14) ^ B0;
	B3 += K1 + 8;
	B2 += B3 + K0 + T0;
	B3 = Utility::IntUtils::RotL64(B3, 16) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 52) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 57) ^ B2;
	B0 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 23) ^ B0;
	B2 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 40) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 5) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 37) ^ B2;
	B1 += K0 + T0;
	B0 += B1 + K4;
	B1 = Utility::IntUtils::RotL64(B1, 25) ^ B0;
	B3 += K2 + 9;
	B2 += B3 + K1 + T1;
	B3 = Utility::IntUtils::RotL64(B3, 33) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 46) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 12) ^ B2;
	B0 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 58) ^ B0;
	B2 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 22) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 32) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 32) ^ B2;
	// rounds 40-47
	B1 += K1 + T1;
	B0 += B1 + K0;
	B1 = Utility::IntUtils::RotL64(B1, 14) ^ B0;
	B3 += K3 + 10;
	B2 += B3 + K2 + T2;
	B3 = Utility::IntUtils::RotL64(B3, 16) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 52) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 57) ^ B2;
	B0 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 23) ^ B0;
	B2 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 40) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 5) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 37) ^ B2;
	B1 += K2 + T2;
	B0 += B1 + K1;
	B1 = Utility::IntUtils::RotL64(B1, 25) ^ B0;
	B3 += K4 + 11;
	B2 += B3 + K3 + T0;
	B3 = Utility::IntUtils::RotL64(B3, 33) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 46) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 12) ^ B2;
	B0 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 58) ^ B0;
	B2 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 22) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 32) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 32) ^ B2;
	// rounds 48-55
	B1 += K3 + T0;
	B0 += B1 + K2;
	B1 = Utility::IntUtils::RotL64(B1, 14) ^ B0;
	B3 += K0 + 12;
	B2 += B3 + K4 + T1;
	B3 = Utility::IntUtils::RotL64(B3, 16) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 52) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 57) ^ B2;
	B0 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 23) ^ B0;
	B2 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 40) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 5) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 37) ^ B2;
	B1 += K4 + T1;
	B0 += B1 + K3;
	B1 = Utility::IntUtils::RotL64(B1, 25) ^ B0;
	B3 += K1 + 13;
	B2 += B3 + K0 + T2;
	B3 = Utility::IntUtils::RotL64(B3, 33) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 46) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 12) ^ B2;
	B0 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 58) ^ B0;
	B2 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 22) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 32) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 32) ^ B2;
	// rounds 56-63
	B1 += K0 + T2;
	B0 += B1 + K4;
	B1 = Utility::IntUtils::RotL64(B1, 14) ^ B0;
	B3 += K2 + 14;
	B2 += B3 + K1 + T0;
	B3 = Utility::IntUtils::RotL64(B3, 16) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 52) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 57) ^ B2;
	B0 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 23) ^ B0;
	B2 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 40) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 5) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 37) ^ B2;
	B1 += K1 + T0;
	B0 += B1 + K0;
	B1 = Utility::IntUtils::RotL64(B1, 25) ^ B0;
	B3 += K3 + 15;
	B2 += B3 + K2 + T1;
	B3 = Utility::IntUtils::RotL64(B3, 33) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 46) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 12) ^ B2;
	B0 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 58) ^ B0;
	B2 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 22) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 32) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 32) ^ B2;
	// rounds 64-71
	B1 += K2 + T1;
	B0 += B1 + K1;
	B1 = Utility::IntUtils::RotL64(B1, 14) ^ B0;
	B3 += K4 + 16;
	B2 += B3 + K3 + T2;
	B3 = Utility::IntUtils::RotL64(B3, 16) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 52) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 57) ^ B2;
	B0 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 23) ^ B0;
	B2 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 40) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 5) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 37) ^ B2;
	B1 += K3 + T2;
	B0 += B1 + K2;
	B1 = Utility::IntUtils::RotL64(B1, 25) ^ B0;
	B3 += K0 + 17;
	B2 += B3 + K4 + T0;
	B3 = Utility::IntUtils::RotL64(B3, 33) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 46) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 12) ^ B2;
	B0 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 58) ^ B0;
	B2 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 22) ^ B2;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 32) ^ B0;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 32) ^ B2;

	State[0] = B0 + K3;
	State[1] = B1 + K4 + T0;
	State[2] = B2 + K0 + T1;
	State[3] = B3 + K1 + 18;
}

#if defined(__AVX2__)

void Skein::PemuteR72P1024H(std::vector<byte> &Input, size_t InOffset, std::vector<ULong256> &State, std::vector<ULong256> &Tweak)
{
	std::array<ULong256, 4> B;
	std::array<ULong256, 5> K;
	std::array<ULong256, 3> T;
	size_t i;
	size_t x;
	size_t y;

	MemUtils::Copy(Input, InOffset, B, 0, 4 * sizeof(ULong256));
	MemUtils::Copy(State, 0, K, 0, 4 * sizeof(ULong256));
	MemUtils::Copy(Tweak, 0, T, 0, 2 * sizeof(ULong256));

	x = 1;
	y = 0;
	K[4] = K[0] ^ K[1] ^ K[2] ^ K[3] ^ ULong256(0x1BD11BDAA9FC1A22ULL);
	T[2] = T[0] ^ T[1];

	for (i = 0; i < 9; ++i)
	{
		// round n+8+8, inject k
		B[1] += K[x] + T[y];
		x != 0 ? x -= 1 : x += 4;
		B[0] += B[1] + K[x];
		B[1] = ULong256::RotL64(B[1], 14) ^ B[0];
		// mix
		x > 1 ? x -= 2 : x += 3;
		B[3] += K[x] + ULong256(i * 2);
		x > 0 ? x -= 1 : x += 4;
		y != 2 ? y += 1 : y -= 2;
		B[2] += B[3] + K[x] + T[y];
		B[3] = ULong256::RotL64(B[3], 16) ^ B[2];
		B[0] += B[3];
		B[3] = ULong256::RotL64(B[3], 52) ^ B[0];
		B[2] += B[1];
		B[1] = ULong256::RotL64(B[1], 57) ^ B[2];
		B[0] += B[1];
		B[1] = ULong256::RotL64(B[1], 23) ^ B[0];
		B[2] += B[3];
		B[3] = ULong256::RotL64(B[3], 40) ^ B[2];
		B[0] += B[3];
		B[3] = ULong256::RotL64(B[3], 5) ^ B[0];
		B[2] += B[1];
		B[1] = ULong256::RotL64(B[1], 37) ^ B[2];
		// inject
		B[1] += K[x] + T[y];
		x != 0 ? x -= 1 : x += 4;
		B[0] += B[1] + K[x];
		B[1] = ULong256::RotL64(B[1], 25) ^ B[0];
		// mix
		x > 1 ? x -= 2 : x += 3;
		B[3] += K[x] + ULong256((i * 2) + 1);
		x != 0 ? x -= 1 : x += 4;
		y != 2 ? y += 1 : y -= 2;
		B[2] += B[3] + K[x] + T[y];
		B[3] = ULong256::RotL64(B[3], 33) ^ B[2];
		B[0] += B[3];
		B[3] = ULong256::RotL64(B[3], 46) ^ B[0];
		B[2] += B[1];
		B[1] = ULong256::RotL64(B[1], 12) ^ B[2];
		B[0] += B[1];
		B[1] = ULong256::RotL64(B[1], 58) ^ B[0];
		B[2] += B[3];
		B[3] = ULong256::RotL64(B[3], 22) ^ B[2];
		B[0] += B[3];
		B[3] = ULong256::RotL64(B[3], 32) ^ B[0];
		B[2] += B[1];
		B[1] = ULong256::RotL64(B[1], 32) ^ B[2];
	}

	B[0] += K[3];
	B[1] += K[4] + T[0];
	B[2] += K[0] + T[1];
	B[3] += K[1] + ULong256(18);

	MemUtils::Copy(B, 0, State, 0, 4 * sizeof(ULong256));
}

#endif

#if defined(__AVX512__)

void Skein::PemuteR72P2048H(std::vector<byte> &Input, size_t InOffset, std::vector<ULong512> &State, std::vector<ULong512> &Tweak)
{
	std::array<ULong512, 4> B;
	std::array<ULong512, 5> K;
	std::array<ULong512, 3> T;
	size_t i;
	size_t x;
	size_t y;

	MemUtils::Copy(Input, InOffset, B, 0, 4 * sizeof(ULong512));
	MemUtils::Copy(State, 0, K, 0, 4 * sizeof(ULong512));
	MemUtils::Copy(Tweak, 0, T, 0, 2 * sizeof(ULong512));

	x = 1;
	y = 0;
	K[4] = K[0] ^ K[1] ^ K[2] ^ K[3] ^ ULong512(0x1BD11BDAA9FC1A22ULL);
	T[2] = T[0] ^ T[1];

	for (i = 0; i < 9; ++i)
	{
		// round n+8+8, inject k
		B[1] += K[x] + T[y];
		x != 0 ? x -= 1 : x += 4;
		B[0] += B[1] + K[x];
		B[1] = ULong512::RotL64(B[1], 14) ^ B[0];
		// mix
		x > 1 ? x -= 2 : x += 3;
		B[3] += K[x] + ULong512(i * 2);
		x > 0 ? x -= 1 : x += 4;
		y != 2 ? y += 1 : y -= 2;
		B[2] += B[3] + K[x] + T[y];
		B[3] = ULong512::RotL64(B[3], 16) ^ B[2];
		B[0] += B[3];
		B[3] = ULong512::RotL64(B[3], 52) ^ B[0];
		B[2] += B[1];
		B[1] = ULong512::RotL64(B[1], 57) ^ B[2];
		B[0] += B[1];
		B[1] = ULong512::RotL64(B[1], 23) ^ B[0];
		B[2] += B[3];
		B[3] = ULong512::RotL64(B[3], 40) ^ B[2];
		B[0] += B[3];
		B[3] = ULong512::RotL64(B[3], 5) ^ B[0];
		B[2] += B[1];
		B[1] = ULong512::RotL64(B[1], 37) ^ B[2];
		// inject
		B[1] += K[x] + T[y];
		x != 0 ? x -= 1 : x += 4;
		B[0] += B[1] + K[x];
		B[1] = ULong512::RotL64(B[1], 25) ^ B[0];
		// mix
		x > 1 ? x -= 2 : x += 3;
		B[3] += K[x] + ULong512((i * 2) + 1);
		x != 0 ? x -= 1 : x += 4;
		y != 2 ? y += 1 : y -= 2;
		B[2] += B[3] + K[x] + T[y];
		B[3] = ULong512::RotL64(B[3], 33) ^ B[2];
		B[0] += B[3];
		B[3] = ULong512::RotL64(B[3], 46) ^ B[0];
		B[2] += B[1];
		B[1] = ULong512::RotL64(B[1], 12) ^ B[2];
		B[0] += B[1];
		B[1] = ULong512::RotL64(B[1], 58) ^ B[0];
		B[2] += B[3];
		B[3] = ULong512::RotL64(B[3], 22) ^ B[2];
		B[0] += B[3];
		B[3] = ULong512::RotL64(B[3], 32) ^ B[0];
		B[2] += B[1];
		B[1] = ULong512::RotL64(B[1], 32) ^ B[2];
	}

	B[0] += K[3];
	B[1] += K[4] + T[0];
	B[2] += K[0] + T[1];
	B[3] += K[1] + ULong512(18);

	MemUtils::Copy(B, 0, State, 0, 4 * sizeof(ULong512));
}

#endif

//~~~Skein-512~~~//

void Skein::PemuteR72P512C(std::array<ulong, 8> &Input, std::array<ulong, 8> &State, std::array<ulong, 2> &Tweak)
{
	std::array<ulong, 8> B;
	std::array<ulong, 9> K;
	std::array<ulong, 3> T;
	size_t i;
	size_t x;
	size_t y;

	MemUtils::Copy(Input, 0, B, 0, 8 * sizeof(ulong));
	MemUtils::Copy(State, 0, K, 0, 8 * sizeof(ulong));
	MemUtils::Copy(Tweak, 0, T, 0, 2 * sizeof(ulong));

	x = 1;
	y = 0;
	K[8] = K[0] ^ K[1] ^ K[2] ^ K[3] ^ K[4] ^ K[5] ^ K[6] ^ K[7] ^ 0x1BD11BDAA9FC1A22ULL;
	T[2] = T[0] ^ T[1];

	for (i = 0; i < 9; ++i)
	{
		// round n+8+8, inject k
		B[1] += K[x];
		x != 0 ? x -= 1 : x += 8;
		B[0] += B[1] + K[x];
		B[1] = Utility::IntUtils::RotL64(B[1], 46) ^ B[0];
		x < 6 ? x += 3 : x -= 6;
		B[3] += K[x];
		x != 0 ? x -= 1 : x += 8;
		B[2] += B[3] + K[x];
		B[3] = Utility::IntUtils::RotL64(B[3], 36) ^ B[2];
		x < 6 ? x += 3 : x -= 6;
		B[5] += K[x] + T[y];
		x != 0 ? x -= 1 : x += 8;
		B[4] += B[5] + K[x];
		B[5] = Utility::IntUtils::RotL64(B[5], 19) ^ B[4];
		// mix
		x < 6 ? x += 3 : x -= 6;
		B[7] += K[x] + (i * 2);
		x != 0 ? x -= 1 : x += 8;
		y != 2 ? y += 1 : y -= 2;
		B[6] += B[7] + K[x] + T[y];
		B[7] = Utility::IntUtils::RotL64(B[7], 37) ^ B[6];
		B[2] += B[1];
		B[1] = Utility::IntUtils::RotL64(B[1], 33) ^ B[2];
		B[4] += B[7];
		B[7] = Utility::IntUtils::RotL64(B[7], 27) ^ B[4];
		B[6] += B[5];
		B[5] = Utility::IntUtils::RotL64(B[5], 14) ^ B[6];
		B[0] += B[3];
		B[3] = Utility::IntUtils::RotL64(B[3], 42) ^ B[0];
		B[4] += B[1];
		B[1] = Utility::IntUtils::RotL64(B[1], 17) ^ B[4];
		B[6] += B[3];
		B[3] = Utility::IntUtils::RotL64(B[3], 49) ^ B[6];
		B[0] += B[5];
		B[5] = Utility::IntUtils::RotL64(B[5], 36) ^ B[0];
		B[2] += B[7];
		B[7] = Utility::IntUtils::RotL64(B[7], 39) ^ B[2];
		B[6] += B[1];
		B[1] = Utility::IntUtils::RotL64(B[1], 44) ^ B[6];
		B[0] += B[7];
		B[7] = Utility::IntUtils::RotL64(B[7], 9) ^ B[0];
		B[2] += B[5];
		B[5] = Utility::IntUtils::RotL64(B[5], 54) ^ B[2];
		B[4] += B[3];
		B[3] = Utility::IntUtils::RotL64(B[3], 56) ^ B[4];
		// inject
		x > 3 ? x -= 4 : x += 5;
		B[1] += K[x];
		x != 0 ? x -= 1 : x += 8;
		B[0] += B[1] + K[x];
		B[1] = Utility::IntUtils::RotL64(B[1], 39) ^ B[0];
		x < 6 ? x += 3 : x -= 6;
		B[3] += K[x];
		x != 0 ? x -= 1 : x += 8;
		B[2] += B[3] + K[x];
		B[3] = Utility::IntUtils::RotL64(B[3], 30) ^ B[2];
		x < 6 ? x += 3 : x -= 6;
		B[5] += K[x] + T[y];
		x != 0 ? x -= 1 : x += 8;
		B[4] += B[5] + K[x];
		B[5] = Utility::IntUtils::RotL64(B[5], 34) ^ B[4];
		// mix
		x < 6 ? x += 3 : x -= 6;
		B[7] += K[x] + (i * 2) + 1;
		x != 0 ? x -= 1 : x += 8;
		y != 2 ? y += 1 : y -= 2;
		B[6] += B[7] + K[x] + T[y];
		B[7] = Utility::IntUtils::RotL64(B[7], 24) ^ B[6];
		B[2] += B[1];
		B[1] = Utility::IntUtils::RotL64(B[1], 13) ^ B[2];
		B[4] += B[7];
		B[7] = Utility::IntUtils::RotL64(B[7], 50) ^ B[4];
		B[6] += B[5];
		B[5] = Utility::IntUtils::RotL64(B[5], 10) ^ B[6];
		B[0] += B[3];
		B[3] = Utility::IntUtils::RotL64(B[3], 17) ^ B[0];
		B[4] += B[1];
		B[1] = Utility::IntUtils::RotL64(B[1], 25) ^ B[4];
		B[6] += B[3];
		B[3] = Utility::IntUtils::RotL64(B[3], 29) ^ B[6];
		B[0] += B[5];
		B[5] = Utility::IntUtils::RotL64(B[5], 39) ^ B[0];
		B[2] += B[7];
		B[7] = Utility::IntUtils::RotL64(B[7], 43) ^ B[2];
		B[6] += B[1];
		B[1] = Utility::IntUtils::RotL64(B[1], 8) ^ B[6];
		B[0] += B[7];
		B[7] = Utility::IntUtils::RotL64(B[7], 35) ^ B[0];
		B[2] += B[5];
		B[5] = Utility::IntUtils::RotL64(B[5], 56) ^ B[2];
		B[4] += B[3];
		B[3] = Utility::IntUtils::RotL64(B[3], 22) ^ B[4];
		x > 3 ? x -= 4 : x += 5;
	}

	State[0] = B[0] + K[0];
	State[1] = B[1] + K[1];
	State[2] = B[2] + K[2];
	State[3] = B[3] + K[3];
	State[4] = B[4] + K[4];
	State[5] = B[5] + K[5] + T[0];
	State[6] = B[6] + K[6] + T[1];
	State[7] = B[7] + K[7] + 18;
}

void Skein::PemuteR72P512U(std::array<ulong, 8> &Input, std::array<ulong, 8> &State, std::array<ulong, 2> &Tweak)
{
	ulong B0 = Input[0];
	ulong B1 = Input[1];
	ulong B2 = Input[2];
	ulong B3 = Input[3];
	ulong B4 = Input[4];
	ulong B5 = Input[5];
	ulong B6 = Input[6];
	ulong B7 = Input[7];
	ulong K0 = State[0];
	ulong K1 = State[1];
	ulong K2 = State[2];
	ulong K3 = State[3];
	ulong K4 = State[4];
	ulong K5 = State[5];
	ulong K6 = State[6];
	ulong K7 = State[7];
	ulong K8 = K0 ^ K1 ^ K2 ^ K3 ^ K4 ^ K5 ^ K6 ^ K7 ^ 0x1BD11BDAA9FC1A22ULL;
	ulong T0 = Tweak[0];
	ulong T1 = Tweak[1];
	ulong T2 = Tweak[0] ^ Tweak[1];

	// rounds 0-7, inject k
	B1 += K1;
	B0 += B1 + K0;
	B1 = Utility::IntUtils::RotL64(B1, 46) ^ B0;
	B3 += K3;
	B2 += B3 + K2;
	B3 = Utility::IntUtils::RotL64(B3, 36) ^ B2;
	B5 += K5 + T0;
	B4 += B5 + K4;
	B5 = Utility::IntUtils::RotL64(B5, 19) ^ B4;
	// mix 
	B7 += K7;
	B6 += B7 + K6 + T1;
	B7 = Utility::IntUtils::RotL64(B7, 37) ^ B6;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 33) ^ B2;
	B4 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 27) ^ B4;
	B6 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 14) ^ B6;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 42) ^ B0;
	B4 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 17) ^ B4;
	B6 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 49) ^ B6;
	B0 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 36) ^ B0;
	B2 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 39) ^ B2;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 44) ^ B6;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 9) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 54) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 56) ^ B4;
	// inject
	B1 += K2;
	B0 += B1 + K1;
	B1 = Utility::IntUtils::RotL64(B1, 39) ^ B0;
	B3 += K4;
	B2 += B3 + K3;
	B3 = Utility::IntUtils::RotL64(B3, 30) ^ B2;
	B5 += K6 + T1;
	B4 += B5 + K5;
	B5 = Utility::IntUtils::RotL64(B5, 34) ^ B4;
	// mix
	B7 += K8 + 1;
	B6 += B7 + K7 + T2;
	B7 = Utility::IntUtils::RotL64(B7, 24) ^ B6;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 13) ^ B2;
	B4 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 50) ^ B4;
	B6 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 10) ^ B6;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 17) ^ B0;
	B4 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 25) ^ B4;
	B6 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 29) ^ B6;
	B0 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 39) ^ B0;
	B2 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 43) ^ B2;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 8) ^ B6;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 35) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 56) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 22) ^ B4;
	// rounds 8-15
	B1 += K3;
	B0 += B1 + K2;
	B1 = Utility::IntUtils::RotL64(B1, 46) ^ B0;
	B3 += K5;
	B2 += B3 + K4;
	B3 = Utility::IntUtils::RotL64(B3, 36) ^ B2;
	B5 += K7 + T2;
	B4 += B5 + K6;
	B5 = Utility::IntUtils::RotL64(B5, 19) ^ B4;
	B7 += K0 + 2;
	B6 += B7 + K8 + T0;
	B7 = Utility::IntUtils::RotL64(B7, 37) ^ B6;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 33) ^ B2;
	B4 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 27) ^ B4;
	B6 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 14) ^ B6;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 42) ^ B0;
	B4 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 17) ^ B4;
	B6 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 49) ^ B6;
	B0 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 36) ^ B0;
	B2 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 39) ^ B2;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 44) ^ B6;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 9) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 54) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 56) ^ B4;
	B1 += K4;
	B0 += B1 + K3;
	B1 = Utility::IntUtils::RotL64(B1, 39) ^ B0;
	B3 += K6;
	B2 += B3 + K5;
	B3 = Utility::IntUtils::RotL64(B3, 30) ^ B2;
	B5 += K8 + T0;
	B4 += B5 + K7;
	B5 = Utility::IntUtils::RotL64(B5, 34) ^ B4;
	B7 += K1 + 3;
	B6 += B7 + K0 + T1;
	B7 = Utility::IntUtils::RotL64(B7, 24) ^ B6;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 13) ^ B2;
	B4 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 50) ^ B4;
	B6 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 10) ^ B6;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 17) ^ B0;
	B4 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 25) ^ B4;
	B6 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 29) ^ B6;
	B0 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 39) ^ B0;
	B2 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 43) ^ B2;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 8) ^ B6;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 35) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 56) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 22) ^ B4;
	// rounds 16-23
	B1 += K5;
	B0 += B1 + K4;
	B1 = Utility::IntUtils::RotL64(B1, 46) ^ B0;
	B3 += K7;
	B2 += B3 + K6;
	B3 = Utility::IntUtils::RotL64(B3, 36) ^ B2;
	B5 += K0 + T1;
	B4 += B5 + K8;
	B5 = Utility::IntUtils::RotL64(B5, 19) ^ B4;
	B7 += K2 + 4;
	B6 += B7 + K1 + T2;
	B7 = Utility::IntUtils::RotL64(B7, 37) ^ B6;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 33) ^ B2;
	B4 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 27) ^ B4;
	B6 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 14) ^ B6;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 42) ^ B0;
	B4 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 17) ^ B4;
	B6 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 49) ^ B6;
	B0 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 36) ^ B0;
	B2 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 39) ^ B2;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 44) ^ B6;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 9) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 54) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 56) ^ B4;
	B1 += K6;
	B0 += B1 + K5;
	B1 = Utility::IntUtils::RotL64(B1, 39) ^ B0;
	B3 += K8;
	B2 += B3 + K7;
	B3 = Utility::IntUtils::RotL64(B3, 30) ^ B2;
	B5 += K1 + T2;
	B4 += B5 + K0;
	B5 = Utility::IntUtils::RotL64(B5, 34) ^ B4;
	B7 += K3 + 5;
	B6 += B7 + K2 + T0;
	B7 = Utility::IntUtils::RotL64(B7, 24) ^ B6;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 13) ^ B2;
	B4 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 50) ^ B4;
	B6 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 10) ^ B6;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 17) ^ B0;
	B4 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 25) ^ B4;
	B6 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 29) ^ B6;
	B0 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 39) ^ B0;
	B2 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 43) ^ B2;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 8) ^ B6;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 35) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 56) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 22) ^ B4;
	// rounds 24-31
	B1 += K7;
	B0 += B1 + K6;
	B1 = Utility::IntUtils::RotL64(B1, 46) ^ B0;
	B3 += K0;
	B2 += B3 + K8;
	B3 = Utility::IntUtils::RotL64(B3, 36) ^ B2;
	B5 += K2 + T0;
	B4 += B5 + K1;
	B5 = Utility::IntUtils::RotL64(B5, 19) ^ B4;
	B7 += K4 + 6;
	B6 += B7 + K3 + T1;
	B7 = Utility::IntUtils::RotL64(B7, 37) ^ B6;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 33) ^ B2;
	B4 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 27) ^ B4;
	B6 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 14) ^ B6;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 42) ^ B0;
	B4 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 17) ^ B4;
	B6 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 49) ^ B6;
	B0 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 36) ^ B0;
	B2 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 39) ^ B2;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 44) ^ B6;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 9) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 54) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 56) ^ B4;
	B1 += K8;
	B0 += B1 + K7;
	B1 = Utility::IntUtils::RotL64(B1, 39) ^ B0;
	B3 += K1;
	B2 += B3 + K0;
	B3 = Utility::IntUtils::RotL64(B3, 30) ^ B2;
	B5 += K3 + T1;
	B4 += B5 + K2;
	B5 = Utility::IntUtils::RotL64(B5, 34) ^ B4;
	B7 += K5 + 7;
	B6 += B7 + K4 + T2;
	B7 = Utility::IntUtils::RotL64(B7, 24) ^ B6;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 13) ^ B2;
	B4 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 50) ^ B4;
	B6 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 10) ^ B6;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 17) ^ B0;
	B4 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 25) ^ B4;
	B6 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 29) ^ B6;
	B0 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 39) ^ B0;
	B2 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 43) ^ B2;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 8) ^ B6;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 35) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 56) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 22) ^ B4;
	// rounds 32-39
	B1 += K0;
	B0 += B1 + K8;
	B1 = Utility::IntUtils::RotL64(B1, 46) ^ B0;
	B3 += K2;
	B2 += B3 + K1;
	B3 = Utility::IntUtils::RotL64(B3, 36) ^ B2;
	B5 += K4 + T2;
	B4 += B5 + K3;
	B5 = Utility::IntUtils::RotL64(B5, 19) ^ B4;
	B7 += K6 + 8;
	B6 += B7 + K5 + T0;
	B7 = Utility::IntUtils::RotL64(B7, 37) ^ B6;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 33) ^ B2;
	B4 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 27) ^ B4;
	B6 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 14) ^ B6;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 42) ^ B0;
	B4 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 17) ^ B4;
	B6 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 49) ^ B6;
	B0 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 36) ^ B0;
	B2 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 39) ^ B2;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 44) ^ B6;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 9) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 54) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 56) ^ B4;
	B1 += K1;
	B0 += B1 + K0;
	B1 = Utility::IntUtils::RotL64(B1, 39) ^ B0;
	B3 += K3;
	B2 += B3 + K2;
	B3 = Utility::IntUtils::RotL64(B3, 30) ^ B2;
	B5 += K5 + T0;
	B4 += B5 + K4;
	B5 = Utility::IntUtils::RotL64(B5, 34) ^ B4;
	B7 += K7 + 9;
	B6 += B7 + K6 + T1;
	B7 = Utility::IntUtils::RotL64(B7, 24) ^ B6;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 13) ^ B2;
	B4 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 50) ^ B4;
	B6 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 10) ^ B6;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 17) ^ B0;
	B4 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 25) ^ B4;
	B6 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 29) ^ B6;
	B0 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 39) ^ B0;
	B2 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 43) ^ B2;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 8) ^ B6;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 35) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 56) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 22) ^ B4;
	// rounds 40-47
	B1 += K2;
	B0 += B1 + K1;
	B1 = Utility::IntUtils::RotL64(B1, 46) ^ B0;
	B3 += K4;
	B2 += B3 + K3;
	B3 = Utility::IntUtils::RotL64(B3, 36) ^ B2;
	B5 += K6 + T1;
	B4 += B5 + K5;
	B5 = Utility::IntUtils::RotL64(B5, 19) ^ B4;
	B7 += K8 + 10;
	B6 += B7 + K7 + T2;
	B7 = Utility::IntUtils::RotL64(B7, 37) ^ B6;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 33) ^ B2;
	B4 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 27) ^ B4;
	B6 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 14) ^ B6;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 42) ^ B0;
	B4 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 17) ^ B4;
	B6 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 49) ^ B6;
	B0 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 36) ^ B0;
	B2 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 39) ^ B2;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 44) ^ B6;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 9) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 54) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 56) ^ B4;
	B1 += K3;
	B0 += B1 + K2;
	B1 = Utility::IntUtils::RotL64(B1, 39) ^ B0;
	B3 += K5;
	B2 += B3 + K4;
	B3 = Utility::IntUtils::RotL64(B3, 30) ^ B2;
	B5 += K7 + T2;
	B4 += B5 + K6;
	B5 = Utility::IntUtils::RotL64(B5, 34) ^ B4;
	B7 += K0 + 11;
	B6 += B7 + K8 + T0;
	B7 = Utility::IntUtils::RotL64(B7, 24) ^ B6;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 13) ^ B2;
	B4 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 50) ^ B4;
	B6 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 10) ^ B6;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 17) ^ B0;
	B4 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 25) ^ B4;
	B6 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 29) ^ B6;
	B0 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 39) ^ B0;
	B2 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 43) ^ B2;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 8) ^ B6;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 35) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 56) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 22) ^ B4;
	// rounds 48-55
	B1 += K4;
	B0 += B1 + K3;
	B1 = Utility::IntUtils::RotL64(B1, 46) ^ B0;
	B3 += K6;
	B2 += B3 + K5;
	B3 = Utility::IntUtils::RotL64(B3, 36) ^ B2;
	B5 += K8 + T0;
	B4 += B5 + K7;
	B5 = Utility::IntUtils::RotL64(B5, 19) ^ B4;
	B7 += K1 + 12;
	B6 += B7 + K0 + T1;
	B7 = Utility::IntUtils::RotL64(B7, 37) ^ B6;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 33) ^ B2;
	B4 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 27) ^ B4;
	B6 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 14) ^ B6;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 42) ^ B0;
	B4 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 17) ^ B4;
	B6 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 49) ^ B6;
	B0 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 36) ^ B0;
	B2 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 39) ^ B2;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 44) ^ B6;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 9) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 54) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 56) ^ B4;
	B1 += K5;
	B0 += B1 + K4;
	B1 = Utility::IntUtils::RotL64(B1, 39) ^ B0;
	B3 += K7;
	B2 += B3 + K6;
	B3 = Utility::IntUtils::RotL64(B3, 30) ^ B2;
	B5 += K0 + T1;
	B4 += B5 + K8;
	B5 = Utility::IntUtils::RotL64(B5, 34) ^ B4;
	B7 += K2 + 13;
	B6 += B7 + K1 + T2;
	B7 = Utility::IntUtils::RotL64(B7, 24) ^ B6;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 13) ^ B2;
	B4 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 50) ^ B4;
	B6 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 10) ^ B6;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 17) ^ B0;
	B4 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 25) ^ B4;
	B6 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 29) ^ B6;
	B0 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 39) ^ B0;
	B2 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 43) ^ B2;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 8) ^ B6;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 35) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 56) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 22) ^ B4;
	// rounds 56-63
	B1 += K6;
	B0 += B1 + K5;
	B1 = Utility::IntUtils::RotL64(B1, 46) ^ B0;
	B3 += K8;
	B2 += B3 + K7;
	B3 = Utility::IntUtils::RotL64(B3, 36) ^ B2;
	B5 += K1 + T2;
	B4 += B5 + K0;
	B5 = Utility::IntUtils::RotL64(B5, 19) ^ B4;
	B7 += K3 + 14;
	B6 += B7 + K2 + T0;
	B7 = Utility::IntUtils::RotL64(B7, 37) ^ B6;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 33) ^ B2;
	B4 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 27) ^ B4;
	B6 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 14) ^ B6;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 42) ^ B0;
	B4 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 17) ^ B4;
	B6 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 49) ^ B6;
	B0 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 36) ^ B0;
	B2 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 39) ^ B2;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 44) ^ B6;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 9) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 54) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 56) ^ B4;
	B1 += K7;
	B0 += B1 + K6;
	B1 = Utility::IntUtils::RotL64(B1, 39) ^ B0;
	B3 += K0;
	B2 += B3 + K8;
	B3 = Utility::IntUtils::RotL64(B3, 30) ^ B2;
	B5 += K2 + T0;
	B4 += B5 + K1;
	B5 = Utility::IntUtils::RotL64(B5, 34) ^ B4;
	B7 += K4 + 15;
	B6 += B7 + K3 + T1;
	B7 = Utility::IntUtils::RotL64(B7, 24) ^ B6;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 13) ^ B2;
	B4 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 50) ^ B4;
	B6 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 10) ^ B6;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 17) ^ B0;
	B4 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 25) ^ B4;
	B6 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 29) ^ B6;
	B0 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 39) ^ B0;
	B2 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 43) ^ B2;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 8) ^ B6;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 35) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 56) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 22) ^ B4;
	// rounds 64-71
	B1 += K8;
	B0 += B1 + K7;
	B1 = Utility::IntUtils::RotL64(B1, 46) ^ B0;
	B3 += K1;
	B2 += B3 + K0;
	B3 = Utility::IntUtils::RotL64(B3, 36) ^ B2;
	B5 += K3 + T1;
	B4 += B5 + K2;
	B5 = Utility::IntUtils::RotL64(B5, 19) ^ B4;
	B7 += K5 + 16;
	B6 += B7 + K4 + T2;
	B7 = Utility::IntUtils::RotL64(B7, 37) ^ B6;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 33) ^ B2;
	B4 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 27) ^ B4;
	B6 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 14) ^ B6;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 42) ^ B0;
	B4 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 17) ^ B4;
	B6 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 49) ^ B6;
	B0 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 36) ^ B0;
	B2 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 39) ^ B2;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 44) ^ B6;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 9) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 54) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 56) ^ B4;
	B1 += K0;
	B0 += B1 + K8;
	B1 = Utility::IntUtils::RotL64(B1, 39) ^ B0;
	B3 += K2;
	B2 += B3 + K1;
	B3 = Utility::IntUtils::RotL64(B3, 30) ^ B2;
	B5 += K4 + T2;
	B4 += B5 + K3;
	B5 = Utility::IntUtils::RotL64(B5, 34) ^ B4;
	B7 += K6 + 17;
	B6 += B7 + K5 + T0;
	B7 = Utility::IntUtils::RotL64(B7, 24) ^ B6;
	B2 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 13) ^ B2;
	B4 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 50) ^ B4;
	B6 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 10) ^ B6;
	B0 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 17) ^ B0;
	B4 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 25) ^ B4;
	B6 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 29) ^ B6;
	B0 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 39) ^ B0;
	B2 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 43) ^ B2;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 8) ^ B6;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 35) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 56) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 22) ^ B4;

	State[0] = B0 + K0;
	State[1] = B1 + K1;
	State[2] = B2 + K2;
	State[3] = B3 + K3;
	State[4] = B4 + K4;
	State[5] = B5 + K5 + T0;
	State[6] = B6 + K6 + T1;
	State[7] = B7 + K7 + 18;
}

#if defined(__AVX2__)

void Skein::PemuteR72P512V(std::array<ulong, 8> &Input, std::array<ulong, 8> &State, std::array<ulong, 2> &Tweak)
{
	const __m256i R1 = _mm256_set_epi64x(37, 19, 36, 46);
	const __m256i R2 = _mm256_set_epi64x(42, 14, 27, 33);
	const __m256i R3 = _mm256_set_epi64x(39, 36, 49, 17);
	const __m256i R4 = _mm256_set_epi64x(56, 54, 9, 44);
	const __m256i R5 = _mm256_set_epi64x(24, 34, 30, 39);
	const __m256i R6 = _mm256_set_epi64x(17, 10, 50, 13);
	const __m256i R7 = _mm256_set_epi64x(43, 39, 29, 25);
	const __m256i R8 = _mm256_set_epi64x(22, 56, 35, 8);
	const __m256i RFN = _mm256_set_epi64x(1, 0, 0, 0);
	const ulong KS = State[0] ^ State[1] ^ State[2] ^ State[3] ^ State[4] ^ State[5] ^ State[6] ^ State[7] ^ 0x1BD11BDAA9FC1A22ULL;
	const __m256i K0 = _mm256_set_epi64x(State[6], State[4], State[2], State[0]);
	const __m256i K1 = _mm256_set_epi64x(State[7], State[5], State[3], State[1]);
	const __m256i K2 = _mm256_set_epi64x(KS, State[6], State[4], State[2]);
	const __m256i K3 = _mm256_set_epi64x(State[0], State[7], State[5], State[3]);
	const __m256i K4 = _mm256_set_epi64x(State[1], KS, State[6], State[4]);
	const __m256i K5 = _mm256_set_epi64x(State[2], State[0], State[7], State[5]);
	const __m256i K6 = _mm256_set_epi64x(State[3], State[1], KS, State[6]);
	const __m256i K7 = _mm256_set_epi64x(State[4], State[2], State[0], State[7]);
	const __m256i K8 = _mm256_set_epi64x(State[5], State[3], State[1], KS);
	const __m256i TS = _mm256_set_epi64x(Tweak[0], Tweak[1], Tweak[0] ^ Tweak[1], 0);

	__m256i R0 = _mm256_set_epi64x(0, 0, 0, 0);
	__m256i SHR;
	__m256i X0 = _mm256_set_epi64x(Input[6], Input[4], Input[2], Input[0]);
	__m256i X1 = _mm256_set_epi64x(Input[7], Input[5], Input[3], Input[1]);
	__m256i T0 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(2, 0, 0, 0));
	__m256i T1 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(0, 3, 0, 0));

	// rounds 0-7
	X0 = _mm256_add_epi64(X0, K0);
	X1 = _mm256_add_epi64(X1, K1);
	X1 = _mm256_add_epi64(X1, R0);
	X0 = _mm256_add_epi64(X0, T0);
	X1 = _mm256_add_epi64(X1, T1);
	R0 = _mm256_add_epi64(R0, RFN);
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R1);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R1), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R2);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R2), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R3);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R3), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R4);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R4), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	T0 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(1, 0, 0, 0));
	T1 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(0, 2, 0, 0));
	X0 = _mm256_add_epi64(X0, K1);
	X1 = _mm256_add_epi64(X1, K2);
	X1 = _mm256_add_epi64(X1, R0);
	X0 = _mm256_add_epi64(X0, T0);
	X1 = _mm256_add_epi64(X1, T1);
	R0 = _mm256_add_epi64(R0, RFN);
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R5);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R5), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R6);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R6), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R7);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R7), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R8);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R8), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	T0 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(3, 0, 0, 0));
	T1 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(0, 1, 0, 0));
	// rounds 8-15
	X0 = _mm256_add_epi64(X0, K2);
	X1 = _mm256_add_epi64(X1, K3);
	X1 = _mm256_add_epi64(X1, R0);
	X0 = _mm256_add_epi64(X0, T0);
	X1 = _mm256_add_epi64(X1, T1);
	R0 = _mm256_add_epi64(R0, RFN);
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R1);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R1), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R2);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R2), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R3);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R3), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R4);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R4), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	T0 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(2, 0, 0, 0));
	T1 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(0, 3, 0, 0));
	X0 = _mm256_add_epi64(X0, K3);
	X1 = _mm256_add_epi64(X1, K4);
	X1 = _mm256_add_epi64(X1, R0);
	X0 = _mm256_add_epi64(X0, T0);
	X1 = _mm256_add_epi64(X1, T1);
	R0 = _mm256_add_epi64(R0, RFN);
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R5);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R5), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R6);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R6), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R7);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R7), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R8);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R8), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	// rounds 16-23
	T0 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(1, 0, 0, 0));
	T1 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(0, 2, 0, 0));
	X0 = _mm256_add_epi64(X0, K4);
	X1 = _mm256_add_epi64(X1, K5);
	X1 = _mm256_add_epi64(X1, R0);
	X0 = _mm256_add_epi64(X0, T0);
	X1 = _mm256_add_epi64(X1, T1);
	R0 = _mm256_add_epi64(R0, RFN);
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R1);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R1), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R2);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R2), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R3);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R3), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R4);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R4), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	T0 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(3, 0, 0, 0));
	T1 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(0, 1, 0, 0));
	X0 = _mm256_add_epi64(X0, K5);
	X1 = _mm256_add_epi64(X1, K6);
	X1 = _mm256_add_epi64(X1, R0);
	X0 = _mm256_add_epi64(X0, T0);
	X1 = _mm256_add_epi64(X1, T1);
	R0 = _mm256_add_epi64(R0, RFN);
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R5);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R5), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R6);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R6), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R7);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R7), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R8);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R8), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	// rounds 24-31
	T0 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(2, 0, 0, 0));
	T1 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(0, 3, 0, 0));
	X0 = _mm256_add_epi64(X0, K6);
	X1 = _mm256_add_epi64(X1, K7);
	X1 = _mm256_add_epi64(X1, R0);
	X0 = _mm256_add_epi64(X0, T0);
	X1 = _mm256_add_epi64(X1, T1);
	R0 = _mm256_add_epi64(R0, RFN);
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R1);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R1), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R2);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R2), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R3);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R3), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R4);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R4), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	T0 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(1, 0, 0, 0));
	T1 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(0, 2, 0, 0));
	X0 = _mm256_add_epi64(X0, K7);
	X1 = _mm256_add_epi64(X1, K8);
	X1 = _mm256_add_epi64(X1, R0);
	X0 = _mm256_add_epi64(X0, T0);
	X1 = _mm256_add_epi64(X1, T1);
	R0 = _mm256_add_epi64(R0, RFN);
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R5);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R5), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R6);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R6), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R7);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R7), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R8);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R8), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	// rounds 32-39
	T0 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(3, 0, 0, 0));
	T1 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(0, 1, 0, 0));
	X0 = _mm256_add_epi64(X0, K8);
	X1 = _mm256_add_epi64(X1, K0);
	X1 = _mm256_add_epi64(X1, R0);
	X0 = _mm256_add_epi64(X0, T0);
	X1 = _mm256_add_epi64(X1, T1);
	R0 = _mm256_add_epi64(R0, RFN);
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R1);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R1), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R2);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R2), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R3);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R3), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R4);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R4), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	T0 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(2, 0, 0, 0));
	T1 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(0, 3, 0, 0));
	X0 = _mm256_add_epi64(X0, K0);
	X1 = _mm256_add_epi64(X1, K1);
	X1 = _mm256_add_epi64(X1, R0);
	X0 = _mm256_add_epi64(X0, T0);
	X1 = _mm256_add_epi64(X1, T1);
	R0 = _mm256_add_epi64(R0, RFN);
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R5);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R5), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R6);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R6), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R7);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R7), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R8);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R8), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	// rounds 40-47
	T0 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(1, 0, 0, 0));
	T1 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(0, 2, 0, 0));
	X0 = _mm256_add_epi64(X0, K1);
	X1 = _mm256_add_epi64(X1, K2);
	X1 = _mm256_add_epi64(X1, R0);
	X0 = _mm256_add_epi64(X0, T0);
	X1 = _mm256_add_epi64(X1, T1);
	R0 = _mm256_add_epi64(R0, RFN);
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R1);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R1), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R2);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R2), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R3);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R3), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R4);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R4), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	T0 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(3, 0, 0, 0));
	T1 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(0, 1, 0, 0));
	X0 = _mm256_add_epi64(X0, K2);
	X1 = _mm256_add_epi64(X1, K3);
	X1 = _mm256_add_epi64(X1, R0);
	X0 = _mm256_add_epi64(X0, T0);
	X1 = _mm256_add_epi64(X1, T1);
	R0 = _mm256_add_epi64(R0, RFN);
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R5);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R5), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R6);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R6), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R7);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R7), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R8);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R8), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	// rounds 48-55
	T0 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(2, 0, 0, 0));
	T1 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(0, 3, 0, 0));
	X0 = _mm256_add_epi64(X0, K3);
	X1 = _mm256_add_epi64(X1, K4);
	X1 = _mm256_add_epi64(X1, R0);
	X0 = _mm256_add_epi64(X0, T0);
	X1 = _mm256_add_epi64(X1, T1);
	R0 = _mm256_add_epi64(R0, RFN);
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R1);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R1), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R2);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R2), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R3);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R3), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R4);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R4), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	T0 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(1, 0, 0, 0));
	T1 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(0, 2, 0, 0));
	X0 = _mm256_add_epi64(X0, K4);
	X1 = _mm256_add_epi64(X1, K5);
	X1 = _mm256_add_epi64(X1, R0);
	X0 = _mm256_add_epi64(X0, T0);
	X1 = _mm256_add_epi64(X1, T1);
	R0 = _mm256_add_epi64(R0, RFN);
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R5);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R5), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R6);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R6), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R7);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R7), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R8);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R8), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	// rounds 56-63
	T0 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(3, 0, 0, 0));
	T1 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(0, 1, 0, 0));
	X0 = _mm256_add_epi64(X0, K5);
	X1 = _mm256_add_epi64(X1, K6);
	X1 = _mm256_add_epi64(X1, R0);
	X0 = _mm256_add_epi64(X0, T0);
	X1 = _mm256_add_epi64(X1, T1);
	R0 = _mm256_add_epi64(R0, RFN);
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R1);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R1), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R2);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R2), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R3);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R3), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R4);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R4), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	T0 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(2, 0, 0, 0));
	T1 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(0, 3, 0, 0));
	X0 = _mm256_add_epi64(X0, K6);
	X1 = _mm256_add_epi64(X1, K7);
	X1 = _mm256_add_epi64(X1, R0);
	X0 = _mm256_add_epi64(X0, T0);
	X1 = _mm256_add_epi64(X1, T1);
	R0 = _mm256_add_epi64(R0, RFN);
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R5);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R5), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R6);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R6), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R7);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R7), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R8);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R8), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	// rounds 64-71
	T0 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(1, 0, 0, 0));
	T1 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(0, 2, 0, 0));
	X0 = _mm256_add_epi64(X0, K7);
	X1 = _mm256_add_epi64(X1, K8);
	X1 = _mm256_add_epi64(X1, R0);
	X0 = _mm256_add_epi64(X0, T0);
	X1 = _mm256_add_epi64(X1, T1);
	R0 = _mm256_add_epi64(R0, RFN);
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R1);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R1), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R2);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R2), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R3);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R3), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R4);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R4), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	T0 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(3, 0, 0, 0));
	T1 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(0, 1, 0, 0));
	X0 = _mm256_add_epi64(X0, K8);
	X1 = _mm256_add_epi64(X1, K0);
	X1 = _mm256_add_epi64(X1, R0);
	X0 = _mm256_add_epi64(X0, T0);
	X1 = _mm256_add_epi64(X1, T1);
	R0 = _mm256_add_epi64(R0, RFN);
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R5);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R5), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R6);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R6), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R7);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R7), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));

	SHR = _mm256_sub_epi64(_mm256_set1_epi64x(64), R8);
	X0 = _mm256_add_epi64(X0, X1);
	X1 = _mm256_or_si256(_mm256_sllv_epi64(X1, R8), _mm256_srlv_epi64(X1, SHR));
	X1 = _mm256_xor_si256(X1, X0);
	X0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(0, 3, 2, 1));
	X1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(1, 2, 3, 0));
	T0 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(2, 0, 0, 0));
	T1 = _mm256_permute4x64_epi64(TS, _MM_SHUFFLE(0, 3, 0, 0));
	X0 = _mm256_add_epi64(X0, K0);
	X1 = _mm256_add_epi64(X1, K1);
	X1 = _mm256_add_epi64(X1, R0);
	X0 = _mm256_add_epi64(X0, T0);
	X1 = _mm256_add_epi64(X1, T1);
	R0 = _mm256_add_epi64(R0, RFN);
	T0 = _mm256_permute4x64_epi64(X0, _MM_SHUFFLE(3, 1, 2, 0));
	T1 = _mm256_permute4x64_epi64(X1, _MM_SHUFFLE(3, 1, 2, 0));
	X0 = _mm256_unpacklo_epi64(T0, T1);
	X1 = _mm256_unpackhi_epi64(T0, T1);

	_mm256_storeu_si256(reinterpret_cast<__m256i*>(&State[0]), X0);
	_mm256_storeu_si256(reinterpret_cast<__m256i*>(&State[4]), X1);
}

#endif

#if defined(__AVX2__)

void Skein::PemuteR72P2048H(std::vector<byte> &Input, size_t InOffset, std::vector<ULong256> &State, std::vector<ULong256> &Tweak)
{
	std::array<ULong256, 8> B;
	std::array<ULong256, 9> K;
	std::array<ULong256, 3> T;
	size_t i;
	size_t x;
	size_t y;

	MemUtils::Copy(Input, InOffset, B, 0, 8 * sizeof(ULong256));
	MemUtils::Copy(State, 0, K, 0, 8 * sizeof(ULong256));
	MemUtils::Copy(Tweak, 0, T, 0, 2 * sizeof(ULong256));

	x = 1;
	y = 0;
	K[8] = K[0] ^ K[1] ^ K[2] ^ K[3] ^ K[4] ^ K[5] ^ K[6] ^ K[7] ^ ULong256(0x1BD11BDAA9FC1A22ULL);
	T[2] = T[0] ^ T[1];

	for (i = 0; i < 9; ++i)
	{
		// round n+8+8, inject k
		B[1] += K[x];
		x != 0 ? x -= 1 : x += 8;
		B[0] += B[1] + K[x];
		B[1] = ULong256::RotL64(B[1], 46) ^ B[0];
		x < 6 ? x += 3 : x -= 6;
		B[3] += K[x];
		x != 0 ? x -= 1 : x += 8;
		B[2] += B[3] + K[x];
		B[3] = ULong256::RotL64(B[3], 36) ^ B[2];
		x < 6 ? x += 3 : x -= 6;
		B[5] += K[x] + T[y];
		x != 0 ? x -= 1 : x += 8;
		B[4] += B[5] + K[x];
		B[5] = ULong256::RotL64(B[5], 19) ^ B[4];
		// mix
		x < 6 ? x += 3 : x -= 6;
		B[7] += K[x] + ULong256(i * 2);
		x != 0 ? x -= 1 : x += 8;
		y != 2 ? y += 1 : y -= 2;
		B[6] += B[7] + K[x] + T[y];
		B[7] = ULong256::RotL64(B[7], 37) ^ B[6];
		B[2] += B[1];
		B[1] = ULong256::RotL64(B[1], 33) ^ B[2];
		B[4] += B[7];
		B[7] = ULong256::RotL64(B[7], 27) ^ B[4];
		B[6] += B[5];
		B[5] = ULong256::RotL64(B[5], 14) ^ B[6];
		B[0] += B[3];
		B[3] = ULong256::RotL64(B[3], 42) ^ B[0];
		B[4] += B[1];
		B[1] = ULong256::RotL64(B[1], 17) ^ B[4];
		B[6] += B[3];
		B[3] = ULong256::RotL64(B[3], 49) ^ B[6];
		B[0] += B[5];
		B[5] = ULong256::RotL64(B[5], 36) ^ B[0];
		B[2] += B[7];
		B[7] = ULong256::RotL64(B[7], 39) ^ B[2];
		B[6] += B[1];
		B[1] = ULong256::RotL64(B[1], 44) ^ B[6];
		B[0] += B[7];
		B[7] = ULong256::RotL64(B[7], 9) ^ B[0];
		B[2] += B[5];
		B[5] = ULong256::RotL64(B[5], 54) ^ B[2];
		B[4] += B[3];
		B[3] = ULong256::RotL64(B[3], 56) ^ B[4];
		// inject
		x > 3 ? x -= 4 : x += 5;
		B[1] += K[x];
		x != 0 ? x -= 1 : x += 8;
		B[0] += B[1] + K[x];
		B[1] = ULong256::RotL64(B[1], 39) ^ B[0];
		x < 6 ? x += 3 : x -= 6;
		B[3] += K[x];
		x != 0 ? x -= 1 : x += 8;
		B[2] += B[3] + K[x];
		B[3] = ULong256::RotL64(B[3], 30) ^ B[2];
		x < 6 ? x += 3 : x -= 6;
		B[5] += K[x] + T[y];
		x != 0 ? x -= 1 : x += 8;
		B[4] += B[5] + K[x];
		B[5] = ULong256::RotL64(B[5], 34) ^ B[4];
		// mix
		x < 6 ? x += 3 : x -= 6;
		B[7] += K[x] + ULong256((i * 2) + 1);
		x != 0 ? x -= 1 : x += 8;
		y != 2 ? y += 1 : y -= 2;
		B[6] += B[7] + K[x] + T[y];
		B[7] = ULong256::RotL64(B[7], 24) ^ B[6];
		B[2] += B[1];
		B[1] = ULong256::RotL64(B[1], 13) ^ B[2];
		B[4] += B[7];
		B[7] = ULong256::RotL64(B[7], 50) ^ B[4];
		B[6] += B[5];
		B[5] = ULong256::RotL64(B[5], 10) ^ B[6];
		B[0] += B[3];
		B[3] = ULong256::RotL64(B[3], 17) ^ B[0];
		B[4] += B[1];
		B[1] = ULong256::RotL64(B[1], 25) ^ B[4];
		B[6] += B[3];
		B[3] = ULong256::RotL64(B[3], 29) ^ B[6];
		B[0] += B[5];
		B[5] = ULong256::RotL64(B[5], 39) ^ B[0];
		B[2] += B[7];
		B[7] = ULong256::RotL64(B[7], 43) ^ B[2];
		B[6] += B[1];
		B[1] = ULong256::RotL64(B[1], 8) ^ B[6];
		B[0] += B[7];
		B[7] = ULong256::RotL64(B[7], 35) ^ B[0];
		B[2] += B[5];
		B[5] = ULong256::RotL64(B[5], 56) ^ B[2];
		B[4] += B[3];
		B[3] = ULong256::RotL64(B[3], 22) ^ B[4];
		x > 3 ? x -= 4 : x += 5;
	}

	State[0] = B[0] + K[0];
	State[1] = B[1] + K[1];
	State[2] = B[2] + K[2];
	State[3] = B[3] + K[3];
	State[4] = B[4] + K[4];
	State[5] = B[5] + K[5] + T[0];
	State[6] = B[6] + K[6] + T[1];
	State[7] = B[7] + K[7] + ULong256(18);
}

#endif

#if defined(__AVX512__)

void Skein::PemuteR72P4096H(std::vector<byte> &Input, size_t InOffset, std::vector<ULong512> &State, std::vector<ULong512> &Tweak)
{
	std::array<ULong512, 8> B;
	std::array<ULong512, 9> K;
	std::array<ULong512, 3> T;
	size_t i;
	size_t x;
	size_t y;

	MemUtils::Copy(Input, InOffset, B, 0, 8 * sizeof(ULong512));
	MemUtils::Copy(State, 0, K, 0, 8 * sizeof(ULong512));
	MemUtils::Copy(Tweak, 0, T, 0, 2 * sizeof(ULong512));

	x = 1;
	y = 0;
	K[8] = K[0] ^ K[1] ^ K[2] ^ K[3] ^ K[4] ^ K[5] ^ K[6] ^ K[7] ^ ULong512(0x1BD11BDAA9FC1A22ULL);
	T[2] = T[0] ^ T[1];

	for (i = 0; i < 9; ++i)
	{
		// round n+8+8, inject k
		B[1] += K[x];
		x != 0 ? x -= 1 : x += 8;
		B[0] += B[1] + K[x];
		B[1] = ULong512::RotL64(B[1], 46) ^ B[0];
		x < 6 ? x += 3 : x -= 6;
		B[3] += K[x];
		x != 0 ? x -= 1 : x += 8;
		B[2] += B[3] + K[x];
		B[3] = ULong512::RotL64(B[3], 36) ^ B[2];
		x < 6 ? x += 3 : x -= 6;
		B[5] += K[x] + T[y];
		x != 0 ? x -= 1 : x += 8;
		B[4] += B[5] + K[x];
		B[5] = ULong512::RotL64(B[5], 19) ^ B[4];
		// mix
		x < 6 ? x += 3 : x -= 6;
		B[7] += K[x] + ULong512(i * 2);
		x != 0 ? x -= 1 : x += 8;
		y != 2 ? y += 1 : y -= 2;
		B[6] += B[7] + K[x] + T[y];
		B[7] = ULong512::RotL64(B[7], 37) ^ B[6];
		B[2] += B[1];
		B[1] = ULong512::RotL64(B[1], 33) ^ B[2];
		B[4] += B[7];
		B[7] = ULong512::RotL64(B[7], 27) ^ B[4];
		B[6] += B[5];
		B[5] = ULong512::RotL64(B[5], 14) ^ B[6];
		B[0] += B[3];
		B[3] = ULong512::RotL64(B[3], 42) ^ B[0];
		B[4] += B[1];
		B[1] = ULong512::RotL64(B[1], 17) ^ B[4];
		B[6] += B[3];
		B[3] = ULong512::RotL64(B[3], 49) ^ B[6];
		B[0] += B[5];
		B[5] = ULong512::RotL64(B[5], 36) ^ B[0];
		B[2] += B[7];
		B[7] = ULong512::RotL64(B[7], 39) ^ B[2];
		B[6] += B[1];
		B[1] = ULong512::RotL64(B[1], 44) ^ B[6];
		B[0] += B[7];
		B[7] = ULong512::RotL64(B[7], 9) ^ B[0];
		B[2] += B[5];
		B[5] = ULong512::RotL64(B[5], 54) ^ B[2];
		B[4] += B[3];
		B[3] = ULong512::RotL64(B[3], 56) ^ B[4];
		// inject
		x > 3 ? x -= 4 : x += 5;
		B[1] += K[x];
		x != 0 ? x -= 1 : x += 8;
		B[0] += B[1] + K[x];
		B[1] = ULong512::RotL64(B[1], 39) ^ B[0];
		x < 6 ? x += 3 : x -= 6;
		B[3] += K[x];
		x != 0 ? x -= 1 : x += 8;
		B[2] += B[3] + K[x];
		B[3] = ULong512::RotL64(B[3], 30) ^ B[2];
		x < 6 ? x += 3 : x -= 6;
		B[5] += K[x] + T[y];
		x != 0 ? x -= 1 : x += 8;
		B[4] += B[5] + K[x];
		B[5] = ULong512::RotL64(B[5], 34) ^ B[4];
		// mix
		x < 6 ? x += 3 : x -= 6;
		B[7] += K[x] + ULong512((i * 2) + 1);
		x != 0 ? x -= 1 : x += 8;
		y != 2 ? y += 1 : y -= 2;
		B[6] += B[7] + K[x] + T[y];
		B[7] = ULong512::RotL64(B[7], 24) ^ B[6];
		B[2] += B[1];
		B[1] = ULong512::RotL64(B[1], 13) ^ B[2];
		B[4] += B[7];
		B[7] = ULong512::RotL64(B[7], 50) ^ B[4];
		B[6] += B[5];
		B[5] = ULong512::RotL64(B[5], 10) ^ B[6];
		B[0] += B[3];
		B[3] = ULong512::RotL64(B[3], 17) ^ B[0];
		B[4] += B[1];
		B[1] = ULong512::RotL64(B[1], 25) ^ B[4];
		B[6] += B[3];
		B[3] = ULong512::RotL64(B[3], 29) ^ B[6];
		B[0] += B[5];
		B[5] = ULong512::RotL64(B[5], 39) ^ B[0];
		B[2] += B[7];
		B[7] = ULong512::RotL64(B[7], 43) ^ B[2];
		B[6] += B[1];
		B[1] = ULong512::RotL64(B[1], 8) ^ B[6];
		B[0] += B[7];
		B[7] = ULong512::RotL64(B[7], 35) ^ B[0];
		B[2] += B[5];
		B[5] = ULong512::RotL64(B[5], 56) ^ B[2];
		B[4] += B[3];
		B[3] = ULong512::RotL64(B[3], 22) ^ B[4];
		x > 3 ? x -= 4 : x += 5;
	}

	State[0] = B[0] + K[0];
	State[1] = B[1] + K[1];
	State[2] = B[2] + K[2];
	State[3] = B[3] + K[3];
	State[4] = B[4] + K[4];
	State[5] = B[5] + K[5] + T[0];
	State[6] = B[6] + K[6] + T[1];
	State[7] = B[7] + K[7] + ULong512(18);
}

#endif

//~~~Skein-1024~~~//

void Skein::PemuteR80P1024C(std::array<ulong, 16> &Input, std::array<ulong, 16> &State, std::array<ulong, 2> &Tweak)
{
	std::array<ulong, 16> B;
	std::array<ulong, 17> K;
	std::array<ulong, 3> T;
	size_t i;
	size_t x;
	size_t y;

	MemUtils::Copy(Input, 0, B, 0, 16 * sizeof(ulong));
	MemUtils::Copy(State, 0, K, 0, 16 * sizeof(ulong));
	MemUtils::Copy(Tweak, 0, T, 0, 2 * sizeof(ulong));

	x = 1;
	y = 0;
	K[16] = K[0] ^ K[1] ^ K[2] ^ K[3] ^ K[4] ^ K[5] ^ K[6] ^ K[7] ^ K[8] ^ K[9] ^ K[10] ^ K[11] ^ K[12] ^ K[13] ^ K[14] ^ K[15] ^ 0x1BD11BDAA9FC1A22ULL;
	T[2] = T[0] ^ T[1];

	for (i = 0; i < 10; ++i)
	{
		// round n+8+8, inject k
		B[1] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[0] += B[1] + K[x];
		B[1] = Utility::IntUtils::RotL64(B[1], 24) ^ B[0];
		x < 14 ? x += 3 : x -= 14;
		B[3] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[2] += B[3] + K[x];
		B[3] = Utility::IntUtils::RotL64(B[3], 13) ^ B[2];
		x < 14 ? x += 3 : x -= 14;
		B[5] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[4] += B[5] + K[x];
		B[5] = Utility::IntUtils::RotL64(B[5], 8) ^ B[4];
		x < 14 ? x += 3 : x -= 14;
		B[7] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[6] += B[7] + K[x];
		B[7] = Utility::IntUtils::RotL64(B[7], 47) ^ B[6];
		x < 14 ? x += 3 : x -= 14;
		B[9] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[8] += B[9] + K[x];
		B[9] = Utility::IntUtils::RotL64(B[9], 8) ^ B[8];
		x < 14 ? x += 3 : x -= 14;
		B[11] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[10] += B[11] + K[x];
		B[11] = Utility::IntUtils::RotL64(B[11], 17) ^ B[10];
		x < 14 ? x += 3 : x -= 14;
		B[13] += K[x] + T[y];
		x != 0 ? x -= 1 : x += 16;
		B[12] += B[13] + K[x];
		B[13] = Utility::IntUtils::RotL64(B[13], 22) ^ B[12];
		// mix
		x < 14 ? x += 3 : x -= 14;
		B[15] += K[x] + (i * 2);
		x != 0 ? x -= 1 : x += 16;
		y != 2 ? y += 1 : y -= 2;
		B[14] += B[15] + K[x] + T[y];
		B[15] = Utility::IntUtils::RotL64(B[15], 37) ^ B[14];
		B[0] += B[9];
		B[9] = Utility::IntUtils::RotL64(B[9], 38) ^ B[0];
		B[2] += B[13];
		B[13] = Utility::IntUtils::RotL64(B[13], 19) ^ B[2];
		B[6] += B[11];
		B[11] = Utility::IntUtils::RotL64(B[11], 10) ^ B[6];
		B[4] += B[15];
		B[15] = Utility::IntUtils::RotL64(B[15], 55) ^ B[4];
		B[10] += B[7];
		B[7] = Utility::IntUtils::RotL64(B[7], 49) ^ B[10];
		B[12] += B[3];
		B[3] = Utility::IntUtils::RotL64(B[3], 18) ^ B[12];
		B[14] += B[5];
		B[5] = Utility::IntUtils::RotL64(B[5], 23) ^ B[14];
		B[8] += B[1];
		B[1] = Utility::IntUtils::RotL64(B[1], 52) ^ B[8];
		B[0] += B[7];
		B[7] = Utility::IntUtils::RotL64(B[7], 33) ^ B[0];
		B[2] += B[5];
		B[5] = Utility::IntUtils::RotL64(B[5], 4) ^ B[2];
		B[4] += B[3];
		B[3] = Utility::IntUtils::RotL64(B[3], 51) ^ B[4];
		B[6] += B[1];
		B[1] = Utility::IntUtils::RotL64(B[1], 13) ^ B[6];
		B[12] += B[15];
		B[15] = Utility::IntUtils::RotL64(B[15], 34) ^ B[12];
		B[14] += B[13];
		B[13] = Utility::IntUtils::RotL64(B[13], 41) ^ B[14];
		B[8] += B[11];
		B[11] = Utility::IntUtils::RotL64(B[11], 59) ^ B[8];
		B[10] += B[9];
		B[9] = Utility::IntUtils::RotL64(B[9], 17) ^ B[10];
		B[0] += B[15];
		B[15] = Utility::IntUtils::RotL64(B[15], 5) ^ B[0];
		B[2] += B[11];
		B[11] = Utility::IntUtils::RotL64(B[11], 20) ^ B[2];
		B[6] += B[13];
		B[13] = Utility::IntUtils::RotL64(B[13], 48) ^ B[6];
		B[4] += B[9];
		B[9] = Utility::IntUtils::RotL64(B[9], 41) ^ B[4];
		B[14] += B[1];
		B[1] = Utility::IntUtils::RotL64(B[1], 47) ^ B[14];
		B[8] += B[5];
		B[5] = Utility::IntUtils::RotL64(B[5], 28) ^ B[8];
		B[10] += B[3];
		B[3] = Utility::IntUtils::RotL64(B[3], 16) ^ B[10];
		B[12] += B[7];
		B[7] = Utility::IntUtils::RotL64(B[7], 25) ^ B[12];
		// inject
		x > 11 ? x -= 12 : x += 5;
		B[1] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[0] += B[1] + K[x];
		B[1] = Utility::IntUtils::RotL64(B[1], 41) ^ B[0];
		x < 14 ? x += 3 : x -= 14;
		B[3] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[2] += B[3] + K[x];
		B[3] = Utility::IntUtils::RotL64(B[3], 9) ^ B[2];
		x < 14 ? x += 3 : x -= 14;
		B[5] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[4] += B[5] + K[x];
		B[5] = Utility::IntUtils::RotL64(B[5], 37) ^ B[4];
		x < 14 ? x += 3 : x -= 14;
		B[7] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[6] += B[7] + K[x];
		B[7] = Utility::IntUtils::RotL64(B[7], 31) ^ B[6];
		x < 14 ? x += 3 : x -= 14;
		B[9] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[8] += B[9] + K[x];
		B[9] = Utility::IntUtils::RotL64(B[9], 12) ^ B[8];
		x < 14 ? x += 3 : x -= 14;
		B[11] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[10] += B[11] + K[x];
		B[11] = Utility::IntUtils::RotL64(B[11], 47) ^ B[10];
		x < 14 ? x += 3 : x -= 14;
		B[13] += K[x] + T[y];
		x != 0 ? x -= 1 : x += 16;
		B[12] += B[13] + K[x];
		B[13] = Utility::IntUtils::RotL64(B[13], 44) ^ B[12];
		// mix
		x < 14 ? x += 3 : x -= 14;
		B[15] += K[x] + (i * 2) + 1;
		x != 0 ? x -= 1 : x += 16;
		y != 2 ? y += 1 : y -= 2;
		B[14] += B[15] + K[x] + T[y];
		B[15] = Utility::IntUtils::RotL64(B[15], 30) ^ B[14];
		B[0] += B[9];
		B[9] = Utility::IntUtils::RotL64(B[9], 16) ^ B[0];
		B[2] += B[13];
		B[13] = Utility::IntUtils::RotL64(B[13], 34) ^ B[2];
		B[6] += B[11];
		B[11] = Utility::IntUtils::RotL64(B[11], 56) ^ B[6];
		B[4] += B[15];
		B[15] = Utility::IntUtils::RotL64(B[15], 51) ^ B[4];
		B[10] += B[7];
		B[7] = Utility::IntUtils::RotL64(B[7], 4) ^ B[10];
		B[12] += B[3];
		B[3] = Utility::IntUtils::RotL64(B[3], 53) ^ B[12];
		B[14] += B[5];
		B[5] = Utility::IntUtils::RotL64(B[5], 42) ^ B[14];
		B[8] += B[1];
		B[1] = Utility::IntUtils::RotL64(B[1], 41) ^ B[8];
		B[0] += B[7];
		B[7] = Utility::IntUtils::RotL64(B[7], 31) ^ B[0];
		B[2] += B[5];
		B[5] = Utility::IntUtils::RotL64(B[5], 44) ^ B[2];
		B[4] += B[3];
		B[3] = Utility::IntUtils::RotL64(B[3], 47) ^ B[4];
		B[6] += B[1];
		B[1] = Utility::IntUtils::RotL64(B[1], 46) ^ B[6];
		B[12] += B[15];
		B[15] = Utility::IntUtils::RotL64(B[15], 19) ^ B[12];
		B[14] += B[13];
		B[13] = Utility::IntUtils::RotL64(B[13], 42) ^ B[14];
		B[8] += B[11];
		B[11] = Utility::IntUtils::RotL64(B[11], 44) ^ B[8];
		B[10] += B[9];
		B[9] = Utility::IntUtils::RotL64(B[9], 25) ^ B[10];
		B[0] += B[15];
		B[15] = Utility::IntUtils::RotL64(B[15], 9) ^ B[0];
		B[2] += B[11];
		B[11] = Utility::IntUtils::RotL64(B[11], 48) ^ B[2];
		B[6] += B[13];
		B[13] = Utility::IntUtils::RotL64(B[13], 35) ^ B[6];
		B[4] += B[9];
		B[9] = Utility::IntUtils::RotL64(B[9], 52) ^ B[4];
		B[14] += B[1];
		B[1] = Utility::IntUtils::RotL64(B[1], 23) ^ B[14];
		B[8] += B[5];
		B[5] = Utility::IntUtils::RotL64(B[5], 31) ^ B[8];
		B[10] += B[3];
		B[3] = Utility::IntUtils::RotL64(B[3], 37) ^ B[10];
		B[12] += B[7];
		B[7] = Utility::IntUtils::RotL64(B[7], 20) ^ B[12];
		x > 11 ? x -= 12 : x += 5;
	}

	State[0] = B[0] + K[3];
	State[1] = B[1] + K[4];
	State[2] = B[2] + K[5];
	State[3] = B[3] + K[6];
	State[4] = B[4] + K[7];
	State[5] = B[5] + K[8];
	State[6] = B[6] + K[9];
	State[7] = B[7] + K[10];
	State[8] = B[8] + K[11];
	State[9] = B[9] + K[12];
	State[10] = B[10] + K[13];
	State[11] = B[11] + K[14];
	State[12] = B[12] + K[15];
	State[13] = B[13] + K[16] + T[2];
	State[14] = B[14] + K[0] + T[0];
	State[15] = B[15] + K[1] + 20;
}

void Skein::PemuteR80P1024U(std::array<ulong, 16> &Input, std::array<ulong, 16> &State, std::array<ulong, 2> &Tweak)
{
	ulong B0 = Input[0];
	ulong B1 = Input[1];
	ulong B2 = Input[2];
	ulong B3 = Input[3];
	ulong B4 = Input[4];
	ulong B5 = Input[5];
	ulong B6 = Input[6];
	ulong B7 = Input[7];
	ulong B8 = Input[8];
	ulong B9 = Input[9];
	ulong B10 = Input[10];
	ulong B11 = Input[11];
	ulong B12 = Input[12];
	ulong B13 = Input[13];
	ulong B14 = Input[14];
	ulong B15 = Input[15];
	ulong K0 = State[0];
	ulong K1 = State[1];
	ulong K2 = State[2];
	ulong K3 = State[3];
	ulong K4 = State[4];
	ulong K5 = State[5];
	ulong K6 = State[6];
	ulong K7 = State[7];
	ulong K8 = State[8];
	ulong K9 = State[9];
	ulong K10 = State[10];
	ulong K11 = State[11];
	ulong K12 = State[12];
	ulong K13 = State[13];
	ulong K14 = State[14];
	ulong K15 = State[15];
	ulong K16 = K0 ^ K1 ^ K2 ^ K3 ^ K4 ^ K5 ^ K6 ^ K7 ^ K8 ^ K9 ^ K10 ^ K11 ^ K12 ^ K13 ^ K14 ^ K15 ^ 0x1BD11BDAA9FC1A22ULL;
	ulong T0 = Tweak[0];
	ulong T1 = Tweak[1];
	ulong T2 = Tweak[0] ^ Tweak[1];

	// rounds 0-7, inject k
	B1 += K1;
	B0 += B1 + K0;
	B1 = Utility::IntUtils::RotL64(B1, 24) ^ B0;
	B3 += K3;
	B2 += B3 + K2;
	B3 = Utility::IntUtils::RotL64(B3, 13) ^ B2;
	B5 += K5;
	B4 += B5 + K4;
	B5 = Utility::IntUtils::RotL64(B5, 8) ^ B4;
	B7 += K7;
	B6 += B7 + K6;
	B7 = Utility::IntUtils::RotL64(B7, 47) ^ B6;
	B9 += K9;
	B8 += B9 + K8;
	B9 = Utility::IntUtils::RotL64(B9, 8) ^ B8;
	B11 += K11;
	B10 += B11 + K10;
	B11 = Utility::IntUtils::RotL64(B11, 17) ^ B10;
	B13 += K13 + T0;
	B12 += B13 + K12;
	B13 = Utility::IntUtils::RotL64(B13, 22) ^ B12;
	// mix
	B15 += K15;
	B14 += B15 + K14 + T1;
	B15 = Utility::IntUtils::RotL64(B15, 37) ^ B14;
	B0 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 38) ^ B0;
	B2 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 19) ^ B2;
	B6 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 10) ^ B6;
	B4 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 55) ^ B4;
	B10 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 49) ^ B10;
	B12 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 18) ^ B12;
	B14 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 23) ^ B14;
	B8 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 52) ^ B8;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 33) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 4) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 51) ^ B4;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 13) ^ B6;
	B12 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 34) ^ B12;
	B14 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 41) ^ B14;
	B8 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 59) ^ B8;
	B10 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 17) ^ B10;
	B0 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 5) ^ B0;
	B2 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 20) ^ B2;
	B6 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 48) ^ B6;
	B4 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 41) ^ B4;
	B14 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 47) ^ B14;
	B8 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 28) ^ B8;
	B10 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 16) ^ B10;
	B12 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 25) ^ B12;
	// inject 
	B1 += K2;
	B0 += B1 + K1;
	B1 = Utility::IntUtils::RotL64(B1, 41) ^ B0;
	B3 += K4;
	B2 += B3 + K3;
	B3 = Utility::IntUtils::RotL64(B3, 9) ^ B2;
	B5 += K6;
	B4 += B5 + K5;
	B5 = Utility::IntUtils::RotL64(B5, 37) ^ B4;
	B7 += K8;
	B6 += B7 + K7;
	B7 = Utility::IntUtils::RotL64(B7, 31) ^ B6;
	B9 += K10;
	B8 += B9 + K9;
	B9 = Utility::IntUtils::RotL64(B9, 12) ^ B8;
	B11 += K12;
	B10 += B11 + K11;
	B11 = Utility::IntUtils::RotL64(B11, 47) ^ B10;
	B13 += K14 + T1;
	B12 += B13 + K13;
	B13 = Utility::IntUtils::RotL64(B13, 44) ^ B12;
	// mix
	B15 += K16 + 1;
	B14 += B15 + K15 + T2;
	B15 = Utility::IntUtils::RotL64(B15, 30) ^ B14;
	B0 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 16) ^ B0;
	B2 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 34) ^ B2;
	B6 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 56) ^ B6;
	B4 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 51) ^ B4;
	B10 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 4) ^ B10;
	B12 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 53) ^ B12;
	B14 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 42) ^ B14;
	B8 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 41) ^ B8;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 31) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 44) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 47) ^ B4;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 46) ^ B6;
	B12 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 19) ^ B12;
	B14 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 42) ^ B14;
	B8 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 44) ^ B8;
	B10 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 25) ^ B10;
	B0 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 9) ^ B0;
	B2 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 48) ^ B2;
	B6 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 35) ^ B6;
	B4 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 52) ^ B4;
	B14 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 23) ^ B14;
	B8 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 31) ^ B8;
	B10 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 37) ^ B10;
	B12 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 20) ^ B12;
	// rounds 8-15
	B1 += K3;
	B0 += B1 + K2;
	B1 = Utility::IntUtils::RotL64(B1, 24) ^ B0;
	B3 += K5;
	B2 += B3 + K4;
	B3 = Utility::IntUtils::RotL64(B3, 13) ^ B2;
	B5 += K7;
	B4 += B5 + K6;
	B5 = Utility::IntUtils::RotL64(B5, 8) ^ B4;
	B7 += K9;
	B6 += B7 + K8;
	B7 = Utility::IntUtils::RotL64(B7, 47) ^ B6;
	B9 += K11;
	B8 += B9 + K10;
	B9 = Utility::IntUtils::RotL64(B9, 8) ^ B8;
	B11 += K13;
	B10 += B11 + K12;
	B11 = Utility::IntUtils::RotL64(B11, 17) ^ B10;
	B13 += K15 + T2;
	B12 += B13 + K14;
	B13 = Utility::IntUtils::RotL64(B13, 22) ^ B12;
	B15 += K0 + 2;
	B14 += B15 + K16 + T0;
	B15 = Utility::IntUtils::RotL64(B15, 37) ^ B14;
	B0 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 38) ^ B0;
	B2 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 19) ^ B2;
	B6 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 10) ^ B6;
	B4 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 55) ^ B4;
	B10 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 49) ^ B10;
	B12 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 18) ^ B12;
	B14 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 23) ^ B14;
	B8 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 52) ^ B8;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 33) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 4) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 51) ^ B4;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 13) ^ B6;
	B12 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 34) ^ B12;
	B14 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 41) ^ B14;
	B8 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 59) ^ B8;
	B10 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 17) ^ B10;
	B0 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 5) ^ B0;
	B2 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 20) ^ B2;
	B6 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 48) ^ B6;
	B4 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 41) ^ B4;
	B14 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 47) ^ B14;
	B8 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 28) ^ B8;
	B10 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 16) ^ B10;
	B12 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 25) ^ B12;
	B1 += K4;
	B0 += B1 + K3;
	B1 = Utility::IntUtils::RotL64(B1, 41) ^ B0;
	B3 += K6;
	B2 += B3 + K5;
	B3 = Utility::IntUtils::RotL64(B3, 9) ^ B2;
	B5 += K8;
	B4 += B5 + K7;
	B5 = Utility::IntUtils::RotL64(B5, 37) ^ B4;
	B7 += K10;
	B6 += B7 + K9;
	B7 = Utility::IntUtils::RotL64(B7, 31) ^ B6;
	B9 += K12;
	B8 += B9 + K11;
	B9 = Utility::IntUtils::RotL64(B9, 12) ^ B8;
	B11 += K14;
	B10 += B11 + K13;
	B11 = Utility::IntUtils::RotL64(B11, 47) ^ B10;
	B13 += K16 + T0;
	B12 += B13 + K15;
	B13 = Utility::IntUtils::RotL64(B13, 44) ^ B12;
	B15 += K1 + 3;
	B14 += B15 + K0 + T1;
	B15 = Utility::IntUtils::RotL64(B15, 30) ^ B14;
	B0 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 16) ^ B0;
	B2 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 34) ^ B2;
	B6 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 56) ^ B6;
	B4 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 51) ^ B4;
	B10 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 4) ^ B10;
	B12 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 53) ^ B12;
	B14 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 42) ^ B14;
	B8 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 41) ^ B8;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 31) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 44) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 47) ^ B4;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 46) ^ B6;
	B12 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 19) ^ B12;
	B14 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 42) ^ B14;
	B8 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 44) ^ B8;
	B10 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 25) ^ B10;
	B0 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 9) ^ B0;
	B2 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 48) ^ B2;
	B6 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 35) ^ B6;
	B4 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 52) ^ B4;
	B14 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 23) ^ B14;
	B8 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 31) ^ B8;
	B10 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 37) ^ B10;
	B12 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 20) ^ B12;
	// rounds 16-23
	B1 += K5;
	B0 += B1 + K4;
	B1 = Utility::IntUtils::RotL64(B1, 24) ^ B0;
	B3 += K7;
	B2 += B3 + K6;
	B3 = Utility::IntUtils::RotL64(B3, 13) ^ B2;
	B5 += K9;
	B4 += B5 + K8;
	B5 = Utility::IntUtils::RotL64(B5, 8) ^ B4;
	B7 += K11;
	B6 += B7 + K10;
	B7 = Utility::IntUtils::RotL64(B7, 47) ^ B6;
	B9 += K13;
	B8 += B9 + K12;
	B9 = Utility::IntUtils::RotL64(B9, 8) ^ B8;
	B11 += K15;
	B10 += B11 + K14;
	B11 = Utility::IntUtils::RotL64(B11, 17) ^ B10;
	B13 += K0 + T1;
	B12 += B13 + K16;
	B13 = Utility::IntUtils::RotL64(B13, 22) ^ B12;
	B15 += K2 + 4;
	B14 += B15 + K1 + T2;
	B15 = Utility::IntUtils::RotL64(B15, 37) ^ B14;
	B0 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 38) ^ B0;
	B2 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 19) ^ B2;
	B6 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 10) ^ B6;
	B4 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 55) ^ B4;
	B10 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 49) ^ B10;
	B12 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 18) ^ B12;
	B14 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 23) ^ B14;
	B8 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 52) ^ B8;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 33) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 4) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 51) ^ B4;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 13) ^ B6;
	B12 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 34) ^ B12;
	B14 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 41) ^ B14;
	B8 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 59) ^ B8;
	B10 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 17) ^ B10;
	B0 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 5) ^ B0;
	B2 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 20) ^ B2;
	B6 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 48) ^ B6;
	B4 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 41) ^ B4;
	B14 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 47) ^ B14;
	B8 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 28) ^ B8;
	B10 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 16) ^ B10;
	B12 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 25) ^ B12;
	B1 += K6;
	B0 += B1 + K5;
	B1 = Utility::IntUtils::RotL64(B1, 41) ^ B0;
	B3 += K8;
	B2 += B3 + K7;
	B3 = Utility::IntUtils::RotL64(B3, 9) ^ B2;
	B5 += K10;
	B4 += B5 + K9;
	B5 = Utility::IntUtils::RotL64(B5, 37) ^ B4;
	B7 += K12;
	B6 += B7 + K11;
	B7 = Utility::IntUtils::RotL64(B7, 31) ^ B6;
	B9 += K14;
	B8 += B9 + K13;
	B9 = Utility::IntUtils::RotL64(B9, 12) ^ B8;
	B11 += K16;
	B10 += B11 + K15;
	B11 = Utility::IntUtils::RotL64(B11, 47) ^ B10;
	B13 += K1 + T2;
	B12 += B13 + K0;
	B13 = Utility::IntUtils::RotL64(B13, 44) ^ B12;
	B15 += K3 + 5;
	B14 += B15 + K2 + T0;
	B15 = Utility::IntUtils::RotL64(B15, 30) ^ B14;
	B0 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 16) ^ B0;
	B2 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 34) ^ B2;
	B6 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 56) ^ B6;
	B4 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 51) ^ B4;
	B10 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 4) ^ B10;
	B12 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 53) ^ B12;
	B14 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 42) ^ B14;
	B8 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 41) ^ B8;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 31) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 44) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 47) ^ B4;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 46) ^ B6;
	B12 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 19) ^ B12;
	B14 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 42) ^ B14;
	B8 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 44) ^ B8;
	B10 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 25) ^ B10;
	B0 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 9) ^ B0;
	B2 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 48) ^ B2;
	B6 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 35) ^ B6;
	B4 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 52) ^ B4;
	B14 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 23) ^ B14;
	B8 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 31) ^ B8;
	B10 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 37) ^ B10;
	B12 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 20) ^ B12;
	// rounds 24-31
	B1 += K7;
	B0 += B1 + K6;
	B1 = Utility::IntUtils::RotL64(B1, 24) ^ B0;
	B3 += K9;
	B2 += B3 + K8;
	B3 = Utility::IntUtils::RotL64(B3, 13) ^ B2;
	B5 += K11;
	B4 += B5 + K10;
	B5 = Utility::IntUtils::RotL64(B5, 8) ^ B4;
	B7 += K13;
	B6 += B7 + K12;
	B7 = Utility::IntUtils::RotL64(B7, 47) ^ B6;
	B9 += K15;
	B8 += B9 + K14;
	B9 = Utility::IntUtils::RotL64(B9, 8) ^ B8;
	B11 += K0;
	B10 += B11 + K16;
	B11 = Utility::IntUtils::RotL64(B11, 17) ^ B10;
	B13 += K2 + T0;
	B12 += B13 + K1;
	B13 = Utility::IntUtils::RotL64(B13, 22) ^ B12;
	B15 += K4 + 6;
	B14 += B15 + K3 + T1;
	B15 = Utility::IntUtils::RotL64(B15, 37) ^ B14;
	B0 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 38) ^ B0;
	B2 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 19) ^ B2;
	B6 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 10) ^ B6;
	B4 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 55) ^ B4;
	B10 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 49) ^ B10;
	B12 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 18) ^ B12;
	B14 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 23) ^ B14;
	B8 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 52) ^ B8;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 33) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 4) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 51) ^ B4;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 13) ^ B6;
	B12 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 34) ^ B12;
	B14 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 41) ^ B14;
	B8 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 59) ^ B8;
	B10 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 17) ^ B10;
	B0 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 5) ^ B0;
	B2 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 20) ^ B2;
	B6 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 48) ^ B6;
	B4 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 41) ^ B4;
	B14 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 47) ^ B14;
	B8 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 28) ^ B8;
	B10 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 16) ^ B10;
	B12 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 25) ^ B12;
	B1 += K8;
	B0 += B1 + K7;
	B1 = Utility::IntUtils::RotL64(B1, 41) ^ B0;
	B3 += K10;
	B2 += B3 + K9;
	B3 = Utility::IntUtils::RotL64(B3, 9) ^ B2;
	B5 += K12;
	B4 += B5 + K11;
	B5 = Utility::IntUtils::RotL64(B5, 37) ^ B4;
	B7 += K14;
	B6 += B7 + K13;
	B7 = Utility::IntUtils::RotL64(B7, 31) ^ B6;
	B9 += K16;
	B8 += B9 + K15;
	B9 = Utility::IntUtils::RotL64(B9, 12) ^ B8;
	B11 += K1;
	B10 += B11 + K0;
	B11 = Utility::IntUtils::RotL64(B11, 47) ^ B10;
	B13 += K3 + T1;
	B12 += B13 + K2;
	B13 = Utility::IntUtils::RotL64(B13, 44) ^ B12;
	B15 += K5 + 7;
	B14 += B15 + K4 + T2;
	B15 = Utility::IntUtils::RotL64(B15, 30) ^ B14;
	B0 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 16) ^ B0;
	B2 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 34) ^ B2;
	B6 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 56) ^ B6;
	B4 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 51) ^ B4;
	B10 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 4) ^ B10;
	B12 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 53) ^ B12;
	B14 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 42) ^ B14;
	B8 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 41) ^ B8;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 31) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 44) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 47) ^ B4;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 46) ^ B6;
	B12 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 19) ^ B12;
	B14 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 42) ^ B14;
	B8 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 44) ^ B8;
	B10 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 25) ^ B10;
	B0 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 9) ^ B0;
	B2 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 48) ^ B2;
	B6 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 35) ^ B6;
	B4 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 52) ^ B4;
	B14 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 23) ^ B14;
	B8 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 31) ^ B8;
	B10 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 37) ^ B10;
	B12 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 20) ^ B12;
	// rounds 32-39
	B1 += K9;
	B0 += B1 + K8;
	B1 = Utility::IntUtils::RotL64(B1, 24) ^ B0;
	B3 += K11;
	B2 += B3 + K10;
	B3 = Utility::IntUtils::RotL64(B3, 13) ^ B2;
	B5 += K13;
	B4 += B5 + K12;
	B5 = Utility::IntUtils::RotL64(B5, 8) ^ B4;
	B7 += K15;
	B6 += B7 + K14;
	B7 = Utility::IntUtils::RotL64(B7, 47) ^ B6;
	B9 += K0;
	B8 += B9 + K16;
	B9 = Utility::IntUtils::RotL64(B9, 8) ^ B8;
	B11 += K2;
	B10 += B11 + K1;
	B11 = Utility::IntUtils::RotL64(B11, 17) ^ B10;
	B13 += K4 + T2;
	B12 += B13 + K3;
	B13 = Utility::IntUtils::RotL64(B13, 22) ^ B12;
	B15 += K6 + 8;
	B14 += B15 + K5 + T0;
	B15 = Utility::IntUtils::RotL64(B15, 37) ^ B14;
	B0 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 38) ^ B0;
	B2 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 19) ^ B2;
	B6 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 10) ^ B6;
	B4 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 55) ^ B4;
	B10 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 49) ^ B10;
	B12 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 18) ^ B12;
	B14 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 23) ^ B14;
	B8 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 52) ^ B8;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 33) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 4) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 51) ^ B4;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 13) ^ B6;
	B12 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 34) ^ B12;
	B14 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 41) ^ B14;
	B8 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 59) ^ B8;
	B10 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 17) ^ B10;
	B0 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 5) ^ B0;
	B2 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 20) ^ B2;
	B6 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 48) ^ B6;
	B4 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 41) ^ B4;
	B14 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 47) ^ B14;
	B8 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 28) ^ B8;
	B10 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 16) ^ B10;
	B12 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 25) ^ B12;
	B1 += K10;
	B0 += B1 + K9;
	B1 = Utility::IntUtils::RotL64(B1, 41) ^ B0;
	B3 += K12;
	B2 += B3 + K11;
	B3 = Utility::IntUtils::RotL64(B3, 9) ^ B2;
	B5 += K14;
	B4 += B5 + K13;
	B5 = Utility::IntUtils::RotL64(B5, 37) ^ B4;
	B7 += K16;
	B6 += B7 + K15;
	B7 = Utility::IntUtils::RotL64(B7, 31) ^ B6;
	B9 += K1;
	B8 += B9 + K0;
	B9 = Utility::IntUtils::RotL64(B9, 12) ^ B8;
	B11 += K3;
	B10 += B11 + K2;
	B11 = Utility::IntUtils::RotL64(B11, 47) ^ B10;
	B13 += K5 + T0;
	B12 += B13 + K4;
	B13 = Utility::IntUtils::RotL64(B13, 44) ^ B12;
	B15 += K7 + 9;
	B14 += B15 + K6 + T1;
	B15 = Utility::IntUtils::RotL64(B15, 30) ^ B14;
	B0 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 16) ^ B0;
	B2 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 34) ^ B2;
	B6 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 56) ^ B6;
	B4 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 51) ^ B4;
	B10 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 4) ^ B10;
	B12 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 53) ^ B12;
	B14 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 42) ^ B14;
	B8 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 41) ^ B8;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 31) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 44) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 47) ^ B4;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 46) ^ B6;
	B12 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 19) ^ B12;
	B14 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 42) ^ B14;
	B8 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 44) ^ B8;
	B10 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 25) ^ B10;
	B0 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 9) ^ B0;
	B2 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 48) ^ B2;
	B6 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 35) ^ B6;
	B4 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 52) ^ B4;
	B14 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 23) ^ B14;
	B8 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 31) ^ B8;
	B10 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 37) ^ B10;
	B12 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 20) ^ B12;
	// rounds 40-47
	B1 += K11;
	B0 += B1 + K10;
	B1 = Utility::IntUtils::RotL64(B1, 24) ^ B0;
	B3 += K13;
	B2 += B3 + K12;
	B3 = Utility::IntUtils::RotL64(B3, 13) ^ B2;
	B5 += K15;
	B4 += B5 + K14;
	B5 = Utility::IntUtils::RotL64(B5, 8) ^ B4;
	B7 += K0;
	B6 += B7 + K16;
	B7 = Utility::IntUtils::RotL64(B7, 47) ^ B6;
	B9 += K2;
	B8 += B9 + K1;
	B9 = Utility::IntUtils::RotL64(B9, 8) ^ B8;
	B11 += K4;
	B10 += B11 + K3;
	B11 = Utility::IntUtils::RotL64(B11, 17) ^ B10;
	B13 += K6 + T1;
	B12 += B13 + K5;
	B13 = Utility::IntUtils::RotL64(B13, 22) ^ B12;
	B15 += K8 + 10;
	B14 += B15 + K7 + T2;
	B15 = Utility::IntUtils::RotL64(B15, 37) ^ B14;
	B0 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 38) ^ B0;
	B2 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 19) ^ B2;
	B6 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 10) ^ B6;
	B4 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 55) ^ B4;
	B10 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 49) ^ B10;
	B12 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 18) ^ B12;
	B14 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 23) ^ B14;
	B8 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 52) ^ B8;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 33) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 4) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 51) ^ B4;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 13) ^ B6;
	B12 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 34) ^ B12;
	B14 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 41) ^ B14;
	B8 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 59) ^ B8;
	B10 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 17) ^ B10;
	B0 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 5) ^ B0;
	B2 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 20) ^ B2;
	B6 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 48) ^ B6;
	B4 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 41) ^ B4;
	B14 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 47) ^ B14;
	B8 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 28) ^ B8;
	B10 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 16) ^ B10;
	B12 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 25) ^ B12;
	B1 += K12;
	B0 += B1 + K11;
	B1 = Utility::IntUtils::RotL64(B1, 41) ^ B0;
	B3 += K14;
	B2 += B3 + K13;
	B3 = Utility::IntUtils::RotL64(B3, 9) ^ B2;
	B5 += K16;
	B4 += B5 + K15;
	B5 = Utility::IntUtils::RotL64(B5, 37) ^ B4;
	B7 += K1;
	B6 += B7 + K0;
	B7 = Utility::IntUtils::RotL64(B7, 31) ^ B6;
	B9 += K3;
	B8 += B9 + K2;
	B9 = Utility::IntUtils::RotL64(B9, 12) ^ B8;
	B11 += K5;
	B10 += B11 + K4;
	B11 = Utility::IntUtils::RotL64(B11, 47) ^ B10;
	B13 += K7 + T2;
	B12 += B13 + K6;
	B13 = Utility::IntUtils::RotL64(B13, 44) ^ B12;
	B15 += K9 + 11;
	B14 += B15 + K8 + T0;
	B15 = Utility::IntUtils::RotL64(B15, 30) ^ B14;
	B0 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 16) ^ B0;
	B2 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 34) ^ B2;
	B6 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 56) ^ B6;
	B4 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 51) ^ B4;
	B10 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 4) ^ B10;
	B12 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 53) ^ B12;
	B14 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 42) ^ B14;
	B8 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 41) ^ B8;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 31) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 44) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 47) ^ B4;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 46) ^ B6;
	B12 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 19) ^ B12;
	B14 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 42) ^ B14;
	B8 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 44) ^ B8;
	B10 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 25) ^ B10;
	B0 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 9) ^ B0;
	B2 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 48) ^ B2;
	B6 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 35) ^ B6;
	B4 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 52) ^ B4;
	B14 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 23) ^ B14;
	B8 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 31) ^ B8;
	B10 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 37) ^ B10;
	B12 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 20) ^ B12;
	// rounds 48-55
	B1 += K13;
	B0 += B1 + K12;
	B1 = Utility::IntUtils::RotL64(B1, 24) ^ B0;
	B3 += K15;
	B2 += B3 + K14;
	B3 = Utility::IntUtils::RotL64(B3, 13) ^ B2;
	B5 += K0;
	B4 += B5 + K16;
	B5 = Utility::IntUtils::RotL64(B5, 8) ^ B4;
	B7 += K2;
	B6 += B7 + K1;
	B7 = Utility::IntUtils::RotL64(B7, 47) ^ B6;
	B9 += K4;
	B8 += B9 + K3;
	B9 = Utility::IntUtils::RotL64(B9, 8) ^ B8;
	B11 += K6;
	B10 += B11 + K5;
	B11 = Utility::IntUtils::RotL64(B11, 17) ^ B10;
	B13 += K8 + T0;
	B12 += B13 + K7;
	B13 = Utility::IntUtils::RotL64(B13, 22) ^ B12;
	B15 += K10 + 12;
	B14 += B15 + K9 + T1;
	B15 = Utility::IntUtils::RotL64(B15, 37) ^ B14;
	B0 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 38) ^ B0;
	B2 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 19) ^ B2;
	B6 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 10) ^ B6;
	B4 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 55) ^ B4;
	B10 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 49) ^ B10;
	B12 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 18) ^ B12;
	B14 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 23) ^ B14;
	B8 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 52) ^ B8;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 33) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 4) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 51) ^ B4;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 13) ^ B6;
	B12 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 34) ^ B12;
	B14 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 41) ^ B14;
	B8 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 59) ^ B8;
	B10 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 17) ^ B10;
	B0 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 5) ^ B0;
	B2 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 20) ^ B2;
	B6 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 48) ^ B6;
	B4 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 41) ^ B4;
	B14 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 47) ^ B14;
	B8 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 28) ^ B8;
	B10 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 16) ^ B10;
	B12 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 25) ^ B12;
	B1 += K14;
	B0 += B1 + K13;
	B1 = Utility::IntUtils::RotL64(B1, 41) ^ B0;
	B3 += K16;
	B2 += B3 + K15;
	B3 = Utility::IntUtils::RotL64(B3, 9) ^ B2;
	B5 += K1;
	B4 += B5 + K0;
	B5 = Utility::IntUtils::RotL64(B5, 37) ^ B4;
	B7 += K3;
	B6 += B7 + K2;
	B7 = Utility::IntUtils::RotL64(B7, 31) ^ B6;
	B9 += K5;
	B8 += B9 + K4;
	B9 = Utility::IntUtils::RotL64(B9, 12) ^ B8;
	B11 += K7;
	B10 += B11 + K6;
	B11 = Utility::IntUtils::RotL64(B11, 47) ^ B10;
	B13 += K9 + T1;
	B12 += B13 + K8;
	B13 = Utility::IntUtils::RotL64(B13, 44) ^ B12;
	B15 += K11 + 13;
	B14 += B15 + K10 + T2;
	B15 = Utility::IntUtils::RotL64(B15, 30) ^ B14;
	B0 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 16) ^ B0;
	B2 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 34) ^ B2;
	B6 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 56) ^ B6;
	B4 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 51) ^ B4;
	B10 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 4) ^ B10;
	B12 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 53) ^ B12;
	B14 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 42) ^ B14;
	B8 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 41) ^ B8;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 31) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 44) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 47) ^ B4;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 46) ^ B6;
	B12 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 19) ^ B12;
	B14 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 42) ^ B14;
	B8 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 44) ^ B8;
	B10 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 25) ^ B10;
	B0 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 9) ^ B0;
	B2 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 48) ^ B2;
	B6 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 35) ^ B6;
	B4 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 52) ^ B4;
	B14 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 23) ^ B14;
	B8 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 31) ^ B8;
	B10 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 37) ^ B10;
	B12 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 20) ^ B12;
	// rounds 56-63
	B1 += K15;
	B0 += B1 + K14;
	B1 = Utility::IntUtils::RotL64(B1, 24) ^ B0;
	B3 += K0;
	B2 += B3 + K16;
	B3 = Utility::IntUtils::RotL64(B3, 13) ^ B2;
	B5 += K2;
	B4 += B5 + K1;
	B5 = Utility::IntUtils::RotL64(B5, 8) ^ B4;
	B7 += K4;
	B6 += B7 + K3;
	B7 = Utility::IntUtils::RotL64(B7, 47) ^ B6;
	B9 += K6;
	B8 += B9 + K5;
	B9 = Utility::IntUtils::RotL64(B9, 8) ^ B8;
	B11 += K8;
	B10 += B11 + K7;
	B11 = Utility::IntUtils::RotL64(B11, 17) ^ B10;
	B13 += K10 + T2;
	B12 += B13 + K9;
	B13 = Utility::IntUtils::RotL64(B13, 22) ^ B12;
	B15 += K12 + 14;
	B14 += B15 + K11 + T0;
	B15 = Utility::IntUtils::RotL64(B15, 37) ^ B14;
	B0 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 38) ^ B0;
	B2 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 19) ^ B2;
	B6 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 10) ^ B6;
	B4 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 55) ^ B4;
	B10 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 49) ^ B10;
	B12 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 18) ^ B12;
	B14 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 23) ^ B14;
	B8 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 52) ^ B8;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 33) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 4) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 51) ^ B4;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 13) ^ B6;
	B12 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 34) ^ B12;
	B14 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 41) ^ B14;
	B8 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 59) ^ B8;
	B10 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 17) ^ B10;
	B0 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 5) ^ B0;
	B2 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 20) ^ B2;
	B6 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 48) ^ B6;
	B4 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 41) ^ B4;
	B14 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 47) ^ B14;
	B8 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 28) ^ B8;
	B10 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 16) ^ B10;
	B12 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 25) ^ B12;
	B1 += K16;
	B0 += B1 + K15;
	B1 = Utility::IntUtils::RotL64(B1, 41) ^ B0;
	B3 += K1;
	B2 += B3 + K0;
	B3 = Utility::IntUtils::RotL64(B3, 9) ^ B2;
	B5 += K3;
	B4 += B5 + K2;
	B5 = Utility::IntUtils::RotL64(B5, 37) ^ B4;
	B7 += K5;
	B6 += B7 + K4;
	B7 = Utility::IntUtils::RotL64(B7, 31) ^ B6;
	B9 += K7;
	B8 += B9 + K6;
	B9 = Utility::IntUtils::RotL64(B9, 12) ^ B8;
	B11 += K9;
	B10 += B11 + K8;
	B11 = Utility::IntUtils::RotL64(B11, 47) ^ B10;
	B13 += K11 + T0;
	B12 += B13 + K10;
	B13 = Utility::IntUtils::RotL64(B13, 44) ^ B12;
	B15 += K13 + 15;
	B14 += B15 + K12 + T1;
	B15 = Utility::IntUtils::RotL64(B15, 30) ^ B14;
	B0 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 16) ^ B0;
	B2 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 34) ^ B2;
	B6 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 56) ^ B6;
	B4 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 51) ^ B4;
	B10 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 4) ^ B10;
	B12 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 53) ^ B12;
	B14 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 42) ^ B14;
	B8 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 41) ^ B8;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 31) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 44) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 47) ^ B4;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 46) ^ B6;
	B12 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 19) ^ B12;
	B14 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 42) ^ B14;
	B8 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 44) ^ B8;
	B10 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 25) ^ B10;
	B0 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 9) ^ B0;
	B2 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 48) ^ B2;
	B6 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 35) ^ B6;
	B4 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 52) ^ B4;
	B14 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 23) ^ B14;
	B8 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 31) ^ B8;
	B10 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 37) ^ B10;
	B12 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 20) ^ B12;
	// rounds 64-71
	B1 += K0;
	B0 += B1 + K16;
	B1 = Utility::IntUtils::RotL64(B1, 24) ^ B0;
	B3 += K2;
	B2 += B3 + K1;
	B3 = Utility::IntUtils::RotL64(B3, 13) ^ B2;
	B5 += K4;
	B4 += B5 + K3;
	B5 = Utility::IntUtils::RotL64(B5, 8) ^ B4;
	B7 += K6;
	B6 += B7 + K5;
	B7 = Utility::IntUtils::RotL64(B7, 47) ^ B6;
	B9 += K8;
	B8 += B9 + K7;
	B9 = Utility::IntUtils::RotL64(B9, 8) ^ B8;
	B11 += K10;
	B10 += B11 + K9;
	B11 = Utility::IntUtils::RotL64(B11, 17) ^ B10;
	B13 += K12 + T1;
	B12 += B13 + K11;
	B13 = Utility::IntUtils::RotL64(B13, 22) ^ B12;
	B15 += K14 + 16;
	B14 += B15 + K13 + T2;
	B15 = Utility::IntUtils::RotL64(B15, 37) ^ B14;
	B0 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 38) ^ B0;
	B2 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 19) ^ B2;
	B6 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 10) ^ B6;
	B4 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 55) ^ B4;
	B10 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 49) ^ B10;
	B12 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 18) ^ B12;
	B14 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 23) ^ B14;
	B8 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 52) ^ B8;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 33) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 4) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 51) ^ B4;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 13) ^ B6;
	B12 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 34) ^ B12;
	B14 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 41) ^ B14;
	B8 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 59) ^ B8;
	B10 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 17) ^ B10;
	B0 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 5) ^ B0;
	B2 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 20) ^ B2;
	B6 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 48) ^ B6;
	B4 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 41) ^ B4;
	B14 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 47) ^ B14;
	B8 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 28) ^ B8;
	B10 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 16) ^ B10;
	B12 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 25) ^ B12;
	B1 += K1;
	B0 += B1 + K0;
	B1 = Utility::IntUtils::RotL64(B1, 41) ^ B0;
	B3 += K3;
	B2 += B3 + K2;
	B3 = Utility::IntUtils::RotL64(B3, 9) ^ B2;
	B5 += K5;
	B4 += B5 + K4;
	B5 = Utility::IntUtils::RotL64(B5, 37) ^ B4;
	B7 += K7;
	B6 += B7 + K6;
	B7 = Utility::IntUtils::RotL64(B7, 31) ^ B6;
	B9 += K9;
	B8 += B9 + K8;
	B9 = Utility::IntUtils::RotL64(B9, 12) ^ B8;
	B11 += K11;
	B10 += B11 + K10;
	B11 = Utility::IntUtils::RotL64(B11, 47) ^ B10;
	B13 += K13 + T2;
	B12 += B13 + K12;
	B13 = Utility::IntUtils::RotL64(B13, 44) ^ B12;
	B15 += K15 + 17;
	B14 += B15 + K14 + T0;
	B15 = Utility::IntUtils::RotL64(B15, 30) ^ B14;
	B0 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 16) ^ B0;
	B2 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 34) ^ B2;
	B6 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 56) ^ B6;
	B4 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 51) ^ B4;
	B10 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 4) ^ B10;
	B12 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 53) ^ B12;
	B14 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 42) ^ B14;
	B8 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 41) ^ B8;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 31) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 44) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 47) ^ B4;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 46) ^ B6;
	B12 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 19) ^ B12;
	B14 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 42) ^ B14;
	B8 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 44) ^ B8;
	B10 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 25) ^ B10;
	B0 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 9) ^ B0;
	B2 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 48) ^ B2;
	B6 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 35) ^ B6;
	B4 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 52) ^ B4;
	B14 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 23) ^ B14;
	B8 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 31) ^ B8;
	B10 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 37) ^ B10;
	B12 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 20) ^ B12;
	// rounds 72-79
	B1 += K2;
	B0 += B1 + K1;
	B1 = Utility::IntUtils::RotL64(B1, 24) ^ B0;
	B3 += K4;
	B2 += B3 + K3;
	B3 = Utility::IntUtils::RotL64(B3, 13) ^ B2;
	B5 += K6;
	B4 += B5 + K5;
	B5 = Utility::IntUtils::RotL64(B5, 8) ^ B4;
	B7 += K8;
	B6 += B7 + K7;
	B7 = Utility::IntUtils::RotL64(B7, 47) ^ B6;
	B9 += K10;
	B8 += B9 + K9;
	B9 = Utility::IntUtils::RotL64(B9, 8) ^ B8;
	B11 += K12;
	B10 += B11 + K11;
	B11 = Utility::IntUtils::RotL64(B11, 17) ^ B10;
	B13 += K14 + T0;
	B12 += B13 + K13;
	B13 = Utility::IntUtils::RotL64(B13, 22) ^ B12;
	B15 += K16 + 18;
	B14 += B15 + K15 + T1;
	B15 = Utility::IntUtils::RotL64(B15, 37) ^ B14;
	B0 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 38) ^ B0;
	B2 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 19) ^ B2;
	B6 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 10) ^ B6;
	B4 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 55) ^ B4;
	B10 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 49) ^ B10;
	B12 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 18) ^ B12;
	B14 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 23) ^ B14;
	B8 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 52) ^ B8;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 33) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 4) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 51) ^ B4;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 13) ^ B6;
	B12 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 34) ^ B12;
	B14 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 41) ^ B14;
	B8 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 59) ^ B8;
	B10 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 17) ^ B10;
	B0 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 5) ^ B0;
	B2 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 20) ^ B2;
	B6 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 48) ^ B6;
	B4 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 41) ^ B4;
	B14 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 47) ^ B14;
	B8 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 28) ^ B8;
	B10 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 16) ^ B10;
	B12 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 25) ^ B12;
	B1 += K3;
	B0 += B1 + K2;
	B1 = Utility::IntUtils::RotL64(B1, 41) ^ B0;
	B3 += K5;
	B2 += B3 + K4;
	B3 = Utility::IntUtils::RotL64(B3, 9) ^ B2;
	B5 += K7;
	B4 += B5 + K6;
	B5 = Utility::IntUtils::RotL64(B5, 37) ^ B4;
	B7 += K9;
	B6 += B7 + K8;
	B7 = Utility::IntUtils::RotL64(B7, 31) ^ B6;
	B9 += K11;
	B8 += B9 + K10;
	B9 = Utility::IntUtils::RotL64(B9, 12) ^ B8;
	B11 += K13;
	B10 += B11 + K12;
	B11 = Utility::IntUtils::RotL64(B11, 47) ^ B10;
	B13 += K15 + T1;
	B12 += B13 + K14;
	B13 = Utility::IntUtils::RotL64(B13, 44) ^ B12;
	B15 += K0 + 19;
	B14 += B15 + K16 + T2;
	B15 = Utility::IntUtils::RotL64(B15, 30) ^ B14;
	B0 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 16) ^ B0;
	B2 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 34) ^ B2;
	B6 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 56) ^ B6;
	B4 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 51) ^ B4;
	B10 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 4) ^ B10;
	B12 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 53) ^ B12;
	B14 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 42) ^ B14;
	B8 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 41) ^ B8;
	B0 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 31) ^ B0;
	B2 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 44) ^ B2;
	B4 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 47) ^ B4;
	B6 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 46) ^ B6;
	B12 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 19) ^ B12;
	B14 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 42) ^ B14;
	B8 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 44) ^ B8;
	B10 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 25) ^ B10;
	B0 += B15;
	B15 = Utility::IntUtils::RotL64(B15, 9) ^ B0;
	B2 += B11;
	B11 = Utility::IntUtils::RotL64(B11, 48) ^ B2;
	B6 += B13;
	B13 = Utility::IntUtils::RotL64(B13, 35) ^ B6;
	B4 += B9;
	B9 = Utility::IntUtils::RotL64(B9, 52) ^ B4;
	B14 += B1;
	B1 = Utility::IntUtils::RotL64(B1, 23) ^ B14;
	B8 += B5;
	B5 = Utility::IntUtils::RotL64(B5, 31) ^ B8;
	B10 += B3;
	B3 = Utility::IntUtils::RotL64(B3, 37) ^ B10;
	B12 += B7;
	B7 = Utility::IntUtils::RotL64(B7, 20) ^ B12;

	State[0] = B0 + K3;
	State[1] = B1 + K4;
	State[2] = B2 + K5;
	State[3] = B3 + K6;
	State[4] = B4 + K7;
	State[5] = B5 + K8;
	State[6] = B6 + K9;
	State[7] = B7 + K10;
	State[8] = B8 + K11;
	State[9] = B9 + K12;
	State[10] = B10 + K13;
	State[11] = B11 + K14;
	State[12] = B12 + K15;
	State[13] = B13 + K16 + T2;
	State[14] = B14 + K0 + T0;
	State[15] = B15 + K1 + 20;
}

#if defined(__AVX2__)

void Skein::PemuteR80P4096H(std::vector<byte> &Input, size_t InOffset, std::vector<ULong256> &State, std::vector<ULong256> &Tweak)
{
	std::array<ULong256, 16> B;
	std::array<ULong256, 17> K;
	std::array<ULong256, 3> T;
	size_t i;
	size_t x;
	size_t y;

	MemUtils::Copy(Input, InOffset, B, 0, 16 * sizeof(ULong256));
	MemUtils::Copy(State, 0, K, 0, 16 * sizeof(ULong256));
	MemUtils::Copy(Tweak, 0, T, 0, 2 * sizeof(ULong256));

	x = 1;
	y = 0;
	K[16] = K[0] ^ K[1] ^ K[2] ^ K[3] ^ K[4] ^ K[5] ^ K[6] ^ K[7] ^ K[8] ^ K[9] ^ K[10] ^ K[11] ^ K[12] ^ K[13] ^ K[14] ^ K[15] ^ ULong256(0x1BD11BDAA9FC1A22ULL);
	T[2] = T[0] ^ T[1];

	for (i = 0; i < 10; ++i)
	{
		// round n+8+8, inject k
		B[1] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[0] += B[1] + K[x];
		B[1] = ULong256::RotL64(B[1], 24) ^ B[0];
		x < 14 ? x += 3 : x -= 14;
		B[3] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[2] += B[3] + K[x];
		B[3] = ULong256::RotL64(B[3], 13) ^ B[2];
		x < 14 ? x += 3 : x -= 14;
		B[5] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[4] += B[5] + K[x];
		B[5] = ULong256::RotL64(B[5], 8) ^ B[4];
		x < 14 ? x += 3 : x -= 14;
		B[7] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[6] += B[7] + K[x];
		B[7] = ULong256::RotL64(B[7], 47) ^ B[6];
		x < 14 ? x += 3 : x -= 14;
		B[9] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[8] += B[9] + K[x];
		B[9] = ULong256::RotL64(B[9], 8) ^ B[8];
		x < 14 ? x += 3 : x -= 14;
		B[11] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[10] += B[11] + K[x];
		B[11] = ULong256::RotL64(B[11], 17) ^ B[10];
		x < 14 ? x += 3 : x -= 14;
		B[13] += K[x] + T[y];
		x != 0 ? x -= 1 : x += 16;
		B[12] += B[13] + K[x];
		B[13] = ULong256::RotL64(B[13], 22) ^ B[12];
		// mix
		x < 14 ? x += 3 : x -= 14;
		B[15] += K[x] + ULong256(i * 2);
		x != 0 ? x -= 1 : x += 16;
		y != 2 ? y += 1 : y -= 2;
		B[14] += B[15] + K[x] + T[y];
		B[15] = ULong256::RotL64(B[15], 37) ^ B[14];
		B[0] += B[9];
		B[9] = ULong256::RotL64(B[9], 38) ^ B[0];
		B[2] += B[13];
		B[13] = ULong256::RotL64(B[13], 19) ^ B[2];
		B[6] += B[11];
		B[11] = ULong256::RotL64(B[11], 10) ^ B[6];
		B[4] += B[15];
		B[15] = ULong256::RotL64(B[15], 55) ^ B[4];
		B[10] += B[7];
		B[7] = ULong256::RotL64(B[7], 49) ^ B[10];
		B[12] += B[3];
		B[3] = ULong256::RotL64(B[3], 18) ^ B[12];
		B[14] += B[5];
		B[5] = ULong256::RotL64(B[5], 23) ^ B[14];
		B[8] += B[1];
		B[1] = ULong256::RotL64(B[1], 52) ^ B[8];
		B[0] += B[7];
		B[7] = ULong256::RotL64(B[7], 33) ^ B[0];
		B[2] += B[5];
		B[5] = ULong256::RotL64(B[5], 4) ^ B[2];
		B[4] += B[3];
		B[3] = ULong256::RotL64(B[3], 51) ^ B[4];
		B[6] += B[1];
		B[1] = ULong256::RotL64(B[1], 13) ^ B[6];
		B[12] += B[15];
		B[15] = ULong256::RotL64(B[15], 34) ^ B[12];
		B[14] += B[13];
		B[13] = ULong256::RotL64(B[13], 41) ^ B[14];
		B[8] += B[11];
		B[11] = ULong256::RotL64(B[11], 59) ^ B[8];
		B[10] += B[9];
		B[9] = ULong256::RotL64(B[9], 17) ^ B[10];
		B[0] += B[15];
		B[15] = ULong256::RotL64(B[15], 5) ^ B[0];
		B[2] += B[11];
		B[11] = ULong256::RotL64(B[11], 20) ^ B[2];
		B[6] += B[13];
		B[13] = ULong256::RotL64(B[13], 48) ^ B[6];
		B[4] += B[9];
		B[9] = ULong256::RotL64(B[9], 41) ^ B[4];
		B[14] += B[1];
		B[1] = ULong256::RotL64(B[1], 47) ^ B[14];
		B[8] += B[5];
		B[5] = ULong256::RotL64(B[5], 28) ^ B[8];
		B[10] += B[3];
		B[3] = ULong256::RotL64(B[3], 16) ^ B[10];
		B[12] += B[7];
		B[7] = ULong256::RotL64(B[7], 25) ^ B[12];
		// inject
		x > 11 ? x -= 12 : x += 5;
		B[1] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[0] += B[1] + K[x];
		B[1] = ULong256::RotL64(B[1], 41) ^ B[0];
		x < 14 ? x += 3 : x -= 14;
		B[3] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[2] += B[3] + K[x];
		B[3] = ULong256::RotL64(B[3], 9) ^ B[2];
		x < 14 ? x += 3 : x -= 14;
		B[5] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[4] += B[5] + K[x];
		B[5] = ULong256::RotL64(B[5], 37) ^ B[4];
		x < 14 ? x += 3 : x -= 14;
		B[7] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[6] += B[7] + K[x];
		B[7] = ULong256::RotL64(B[7], 31) ^ B[6];
		x < 14 ? x += 3 : x -= 14;
		B[9] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[8] += B[9] + K[x];
		B[9] = ULong256::RotL64(B[9], 12) ^ B[8];
		x < 14 ? x += 3 : x -= 14;
		B[11] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[10] += B[11] + K[x];
		B[11] = ULong256::RotL64(B[11], 47) ^ B[10];
		x < 14 ? x += 3 : x -= 14;
		B[13] += K[x] + T[y];
		x != 0 ? x -= 1 : x += 16;
		B[12] += B[13] + K[x];
		B[13] = ULong256::RotL64(B[13], 44) ^ B[12];
		// mix
		x < 14 ? x += 3 : x -= 14;
		B[15] += K[x] + ULong256((i * 2) + 1);
		x != 0 ? x -= 1 : x += 16;
		y != 2 ? y += 1 : y -= 2;
		B[14] += B[15] + K[x] + T[y];
		B[15] = ULong256::RotL64(B[15], 30) ^ B[14];
		B[0] += B[9];
		B[9] = ULong256::RotL64(B[9], 16) ^ B[0];
		B[2] += B[13];
		B[13] = ULong256::RotL64(B[13], 34) ^ B[2];
		B[6] += B[11];
		B[11] = ULong256::RotL64(B[11], 56) ^ B[6];
		B[4] += B[15];
		B[15] = ULong256::RotL64(B[15], 51) ^ B[4];
		B[10] += B[7];
		B[7] = ULong256::RotL64(B[7], 4) ^ B[10];
		B[12] += B[3];
		B[3] = ULong256::RotL64(B[3], 53) ^ B[12];
		B[14] += B[5];
		B[5] = ULong256::RotL64(B[5], 42) ^ B[14];
		B[8] += B[1];
		B[1] = ULong256::RotL64(B[1], 41) ^ B[8];
		B[0] += B[7];
		B[7] = ULong256::RotL64(B[7], 31) ^ B[0];
		B[2] += B[5];
		B[5] = ULong256::RotL64(B[5], 44) ^ B[2];
		B[4] += B[3];
		B[3] = ULong256::RotL64(B[3], 47) ^ B[4];
		B[6] += B[1];
		B[1] = ULong256::RotL64(B[1], 46) ^ B[6];
		B[12] += B[15];
		B[15] = ULong256::RotL64(B[15], 19) ^ B[12];
		B[14] += B[13];
		B[13] = ULong256::RotL64(B[13], 42) ^ B[14];
		B[8] += B[11];
		B[11] = ULong256::RotL64(B[11], 44) ^ B[8];
		B[10] += B[9];
		B[9] = ULong256::RotL64(B[9], 25) ^ B[10];
		B[0] += B[15];
		B[15] = ULong256::RotL64(B[15], 9) ^ B[0];
		B[2] += B[11];
		B[11] = ULong256::RotL64(B[11], 48) ^ B[2];
		B[6] += B[13];
		B[13] = ULong256::RotL64(B[13], 35) ^ B[6];
		B[4] += B[9];
		B[9] = ULong256::RotL64(B[9], 52) ^ B[4];
		B[14] += B[1];
		B[1] = ULong256::RotL64(B[1], 23) ^ B[14];
		B[8] += B[5];
		B[5] = ULong256::RotL64(B[5], 31) ^ B[8];
		B[10] += B[3];
		B[3] = ULong256::RotL64(B[3], 37) ^ B[10];
		B[12] += B[7];
		B[7] = ULong256::RotL64(B[7], 20) ^ B[12];
		x > 11 ? x -= 12 : x += 5;
	}

	State[0] = B[0] + K[3];
	State[1] = B[1] + K[4];
	State[2] = B[2] + K[5];
	State[3] = B[3] + K[6];
	State[4] = B[4] + K[7];
	State[5] = B[5] + K[8];
	State[6] = B[6] + K[9];
	State[7] = B[7] + K[10];
	State[8] = B[8] + K[11];
	State[9] = B[9] + K[12];
	State[10] = B[10] + K[13];
	State[11] = B[11] + K[14];
	State[12] = B[12] + K[15];
	State[13] = B[13] + K[16] + T[2];
	State[14] = B[14] + K[0] + T[0];
	State[15] = B[15] + K[1] + ULong256(20);
}

#endif

#if defined(__AVX512__)

void Skein::PemuteR80P8192H(std::vector<byte> &Input, size_t InOffset, std::vector<ULong512> &State, std::vector<ULong512> &Tweak)
{
	std::array<ULong512, 16> B;
	std::array<ULong512, 17> K;
	std::array<ULong512, 3> T;
	size_t i;
	size_t x;
	size_t y;

	MemUtils::Copy(Input, InOffset, B, 0, 16 * sizeof(ULong512));
	MemUtils::Copy(State, 0, K, 0, 16 * sizeof(ULong512));
	MemUtils::Copy(Tweak, 0, T, 0, 2 * sizeof(ULong512));

	x = 1;
	y = 0;
	K[16] = K[0] ^ K[1] ^ K[2] ^ K[3] ^ K[4] ^ K[5] ^ K[6] ^ K[7] ^ K[8] ^ K[9] ^ K[10] ^ K[11] ^ K[12] ^ K[13] ^ K[14] ^ K[15] ^ ULong512(0x1BD11BDAA9FC1A22ULL);
	T[2] = T[0] ^ T[1];

	for (i = 0; i < 10; ++i)
	{
		// round n+8+8, inject k
		B[1] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[0] += B[1] + K[x];
		B[1] = ULong512::RotL64(B[1], 24) ^ B[0];
		x < 14 ? x += 3 : x -= 14;
		B[3] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[2] += B[3] + K[x];
		B[3] = ULong512::RotL64(B[3], 13) ^ B[2];
		x < 14 ? x += 3 : x -= 14;
		B[5] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[4] += B[5] + K[x];
		B[5] = ULong512::RotL64(B[5], 8) ^ B[4];
		x < 14 ? x += 3 : x -= 14;
		B[7] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[6] += B[7] + K[x];
		B[7] = ULong512::RotL64(B[7], 47) ^ B[6];
		x < 14 ? x += 3 : x -= 14;
		B[9] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[8] += B[9] + K[x];
		B[9] = ULong512::RotL64(B[9], 8) ^ B[8];
		x < 14 ? x += 3 : x -= 14;
		B[11] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[10] += B[11] + K[x];
		B[11] = ULong512::RotL64(B[11], 17) ^ B[10];
		x < 14 ? x += 3 : x -= 14;
		B[13] += K[x] + T[y];
		x != 0 ? x -= 1 : x += 16;
		B[12] += B[13] + K[x];
		B[13] = ULong512::RotL64(B[13], 22) ^ B[12];
		// mix
		x < 14 ? x += 3 : x -= 14;
		B[15] += K[x] + ULong512(i * 2);
		x != 0 ? x -= 1 : x += 16;
		y != 2 ? y += 1 : y -= 2;
		B[14] += B[15] + K[x] + T[y];
		B[15] = ULong512::RotL64(B[15], 37) ^ B[14];
		B[0] += B[9];
		B[9] = ULong512::RotL64(B[9], 38) ^ B[0];
		B[2] += B[13];
		B[13] = ULong512::RotL64(B[13], 19) ^ B[2];
		B[6] += B[11];
		B[11] = ULong512::RotL64(B[11], 10) ^ B[6];
		B[4] += B[15];
		B[15] = ULong512::RotL64(B[15], 55) ^ B[4];
		B[10] += B[7];
		B[7] = ULong512::RotL64(B[7], 49) ^ B[10];
		B[12] += B[3];
		B[3] = ULong512::RotL64(B[3], 18) ^ B[12];
		B[14] += B[5];
		B[5] = ULong512::RotL64(B[5], 23) ^ B[14];
		B[8] += B[1];
		B[1] = ULong512::RotL64(B[1], 52) ^ B[8];
		B[0] += B[7];
		B[7] = ULong512::RotL64(B[7], 33) ^ B[0];
		B[2] += B[5];
		B[5] = ULong512::RotL64(B[5], 4) ^ B[2];
		B[4] += B[3];
		B[3] = ULong512::RotL64(B[3], 51) ^ B[4];
		B[6] += B[1];
		B[1] = ULong512::RotL64(B[1], 13) ^ B[6];
		B[12] += B[15];
		B[15] = ULong512::RotL64(B[15], 34) ^ B[12];
		B[14] += B[13];
		B[13] = ULong512::RotL64(B[13], 41) ^ B[14];
		B[8] += B[11];
		B[11] = ULong512::RotL64(B[11], 59) ^ B[8];
		B[10] += B[9];
		B[9] = ULong512::RotL64(B[9], 17) ^ B[10];
		B[0] += B[15];
		B[15] = ULong512::RotL64(B[15], 5) ^ B[0];
		B[2] += B[11];
		B[11] = ULong512::RotL64(B[11], 20) ^ B[2];
		B[6] += B[13];
		B[13] = ULong512::RotL64(B[13], 48) ^ B[6];
		B[4] += B[9];
		B[9] = ULong512::RotL64(B[9], 41) ^ B[4];
		B[14] += B[1];
		B[1] = ULong512::RotL64(B[1], 47) ^ B[14];
		B[8] += B[5];
		B[5] = ULong512::RotL64(B[5], 28) ^ B[8];
		B[10] += B[3];
		B[3] = ULong512::RotL64(B[3], 16) ^ B[10];
		B[12] += B[7];
		B[7] = ULong512::RotL64(B[7], 25) ^ B[12];
		// inject
		x > 11 ? x -= 12 : x += 5;
		B[1] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[0] += B[1] + K[x];
		B[1] = ULong512::RotL64(B[1], 41) ^ B[0];
		x < 14 ? x += 3 : x -= 14;
		B[3] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[2] += B[3] + K[x];
		B[3] = ULong512::RotL64(B[3], 9) ^ B[2];
		x < 14 ? x += 3 : x -= 14;
		B[5] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[4] += B[5] + K[x];
		B[5] = ULong512::RotL64(B[5], 37) ^ B[4];
		x < 14 ? x += 3 : x -= 14;
		B[7] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[6] += B[7] + K[x];
		B[7] = ULong512::RotL64(B[7], 31) ^ B[6];
		x < 14 ? x += 3 : x -= 14;
		B[9] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[8] += B[9] + K[x];
		B[9] = ULong512::RotL64(B[9], 12) ^ B[8];
		x < 14 ? x += 3 : x -= 14;
		B[11] += K[x];
		x != 0 ? x -= 1 : x += 16;
		B[10] += B[11] + K[x];
		B[11] = ULong512::RotL64(B[11], 47) ^ B[10];
		x < 14 ? x += 3 : x -= 14;
		B[13] += K[x] + T[y];
		x != 0 ? x -= 1 : x += 16;
		B[12] += B[13] + K[x];
		B[13] = ULong512::RotL64(B[13], 44) ^ B[12];
		// mix
		x < 14 ? x += 3 : x -= 14;
		B[15] += K[x] + ULong512((i * 2) + 1);
		x != 0 ? x -= 1 : x += 16;
		y != 2 ? y += 1 : y -= 2;
		B[14] += B[15] + K[x] + T[y];
		B[15] = ULong512::RotL64(B[15], 30) ^ B[14];
		B[0] += B[9];
		B[9] = ULong512::RotL64(B[9], 16) ^ B[0];
		B[2] += B[13];
		B[13] = ULong512::RotL64(B[13], 34) ^ B[2];
		B[6] += B[11];
		B[11] = ULong512::RotL64(B[11], 56) ^ B[6];
		B[4] += B[15];
		B[15] = ULong512::RotL64(B[15], 51) ^ B[4];
		B[10] += B[7];
		B[7] = ULong512::RotL64(B[7], 4) ^ B[10];
		B[12] += B[3];
		B[3] = ULong512::RotL64(B[3], 53) ^ B[12];
		B[14] += B[5];
		B[5] = ULong512::RotL64(B[5], 42) ^ B[14];
		B[8] += B[1];
		B[1] = ULong512::RotL64(B[1], 41) ^ B[8];
		B[0] += B[7];
		B[7] = ULong512::RotL64(B[7], 31) ^ B[0];
		B[2] += B[5];
		B[5] = ULong512::RotL64(B[5], 44) ^ B[2];
		B[4] += B[3];
		B[3] = ULong512::RotL64(B[3], 47) ^ B[4];
		B[6] += B[1];
		B[1] = ULong512::RotL64(B[1], 46) ^ B[6];
		B[12] += B[15];
		B[15] = ULong512::RotL64(B[15], 19) ^ B[12];
		B[14] += B[13];
		B[13] = ULong512::RotL64(B[13], 42) ^ B[14];
		B[8] += B[11];
		B[11] = ULong512::RotL64(B[11], 44) ^ B[8];
		B[10] += B[9];
		B[9] = ULong512::RotL64(B[9], 25) ^ B[10];
		B[0] += B[15];
		B[15] = ULong512::RotL64(B[15], 9) ^ B[0];
		B[2] += B[11];
		B[11] = ULong512::RotL64(B[11], 48) ^ B[2];
		B[6] += B[13];
		B[13] = ULong512::RotL64(B[13], 35) ^ B[6];
		B[4] += B[9];
		B[9] = ULong512::RotL64(B[9], 52) ^ B[4];
		B[14] += B[1];
		B[1] = ULong512::RotL64(B[1], 23) ^ B[14];
		B[8] += B[5];
		B[5] = ULong512::RotL64(B[5], 31) ^ B[8];
		B[10] += B[3];
		B[3] = ULong512::RotL64(B[3], 37) ^ B[10];
		B[12] += B[7];
		B[7] = ULong512::RotL64(B[7], 20) ^ B[12];
		x > 11 ? x -= 12 : x += 5;
	}

	State[0] = B[0] + K[3];
	State[1] = B[1] + K[4];
	State[2] = B[2] + K[5];
	State[3] = B[3] + K[6];
	State[4] = B[4] + K[7];
	State[5] = B[5] + K[8];
	State[6] = B[6] + K[9];
	State[7] = B[7] + K[10];
	State[8] = B[8] + K[11];
	State[9] = B[9] + K[12];
	State[10] = B[10] + K[13];
	State[11] = B[11] + K[14];
	State[12] = B[12] + K[15];
	State[13] = B[13] + K[16] + T[2];
	State[14] = B[14] + K[0] + T[0];
	State[15] = B[15] + K[1] + ULong512(20);
}

#endif

NAMESPACE_DIGESTEND