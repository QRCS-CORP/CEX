#include "Blake2.h"
#include "IntUtils.h"

NAMESPACE_DIGEST

using Utility::IntUtils;
using Utility::MemUtils;

//~~~Constants~~~//

const std::vector<uint> Blake2::IV256 =
{
	0x6A09E667UL,
	0xBB67AE85UL,
	0x3C6EF372UL,
	0xA54FF53AUL,
	0x510E527FUL,
	0x9B05688CUL,
	0x1F83D9ABUL,
	0x5BE0CD19UL
};

const std::vector<ulong> Blake2::IV512 =
{
	0x6A09E667F3BCC908ULL,
	0xBB67AE8584CAA73BULL,
	0x3C6EF372FE94F82BULL,
	0xA54FF53A5F1D36F1ULL,
	0x510E527FADE682D1ULL,
	0x9B05688C2B3E6C1FULL,
	0x1F83D9ABFB41BD6BULL,
	0x5BE0CD19137E2179ULL
};

const std::vector<byte> Blake2::Sigma256 =
{
	0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07, 0x08, 0x09, 0x0A, 0x0B, 0x0C, 0x0D, 0x0E, 0x0F,
	0x0E, 0x0A, 0x04, 0x08, 0x09, 0x0F, 0x0D, 0x06, 0x01, 0x0C, 0x00, 0x02, 0x0B, 0x07, 0x05, 0x03,
	0x0B, 0x08, 0x0C, 0x00, 0x05, 0x02, 0x0F, 0x0D, 0x0A, 0x0E, 0x03, 0x06, 0x07, 0x01, 0x09, 0x04,
	0x07, 0x09, 0x03, 0x01, 0x0D, 0x0C, 0x0B, 0x0E, 0x02, 0x06, 0x05, 0x0A, 0x04, 0x00, 0x0F, 0x08,
	0x09, 0x00, 0x05, 0x07, 0x02, 0x04, 0x0A, 0x0F, 0x0E, 0x01, 0x0B, 0x0C, 0x06, 0x08, 0x03, 0x0D,
	0x02, 0x0C, 0x06, 0x0A, 0x00, 0x0B, 0x08, 0x03, 0x04, 0x0D, 0x07, 0x05, 0x0F, 0x0E, 0x01, 0x09,
	0x0C, 0x05, 0x01, 0x0F, 0x0E, 0x0D, 0x04, 0x0A, 0x00, 0x07, 0x06, 0x03, 0x09, 0x02, 0x08, 0x0B,
	0x0D, 0x0B, 0x07, 0x0E, 0x0C, 0x01, 0x03, 0x09, 0x05, 0x00, 0x0F, 0x04, 0x08, 0x06, 0x02, 0x0A,
	0x06, 0x0F, 0x0E, 0x09, 0x0B, 0x03, 0x00, 0x08, 0x0C, 0x02, 0x0D, 0x07, 0x01, 0x04, 0x0A, 0x05,
	0x0A, 0x02, 0x08, 0x04, 0x07, 0x06, 0x01, 0x05, 0x0F, 0x0B, 0x09, 0x0E, 0x03, 0x0C, 0x0D, 0x00,
	0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07, 0x08, 0x09, 0x0A, 0x0B, 0x0C, 0x0D, 0x0E, 0x0F,
	0x0E, 0x0A, 0x04, 0x08, 0x09, 0x0F, 0x0D, 0x06, 0x01, 0x0C, 0x00, 0x02, 0x0B, 0x07, 0x05, 0x03
};

const std::vector<byte> Blake2::Sigma512 =
{
	0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07, 0x08, 0x09, 0x0A, 0x0B, 0x0C, 0x0D, 0x0E, 0x0F,
	0x0E, 0x0A, 0x04, 0x08, 0x09, 0x0F, 0x0D, 0x06, 0x01, 0x0C, 0x00, 0x02, 0x0B, 0x07, 0x05, 0x03,
	0x0B, 0x08, 0x0C, 0x00, 0x05, 0x02, 0x0F, 0x0D, 0x0A, 0x0E, 0x03, 0x06, 0x07, 0x01, 0x09, 0x04,
	0x07, 0x09, 0x03, 0x01, 0x0D, 0x0C, 0x0B, 0x0E, 0x02, 0x06, 0x05, 0x0A, 0x04, 0x00, 0x0F, 0x08,
	0x09, 0x00, 0x05, 0x07, 0x02, 0x04, 0x0A, 0x0F, 0x0E, 0x01, 0x0B, 0x0C, 0x06, 0x08, 0x03, 0x0D,
	0x02, 0x0C, 0x06, 0x0A, 0x00, 0x0B, 0x08, 0x03, 0x04, 0x0D, 0x07, 0x05, 0x0F, 0x0E, 0x01, 0x09,
	0x0C, 0x05, 0x01, 0x0F, 0x0E, 0x0D, 0x04, 0x0A, 0x00, 0x07, 0x06, 0x03, 0x09, 0x02, 0x08, 0x0B,
	0x0D, 0x0B, 0x07, 0x0E, 0x0C, 0x01, 0x03, 0x09, 0x05, 0x00, 0x0F, 0x04, 0x08, 0x06, 0x02, 0x0A,
	0x06, 0x0F, 0x0E, 0x09, 0x0B, 0x03, 0x00, 0x08, 0x0C, 0x02, 0x0D, 0x07, 0x01, 0x04, 0x0A, 0x05,
	0x0A, 0x02, 0x08, 0x04, 0x07, 0x06, 0x01, 0x05, 0x0F, 0x0B, 0x09, 0x0E, 0x03, 0x0C, 0x0D, 0x00,
	0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07, 0x08, 0x09, 0x0A, 0x0B, 0x0C, 0x0D, 0x0E, 0x0F,
	0x0E, 0x0A, 0x04, 0x08, 0x09, 0x0F, 0x0D, 0x06, 0x01, 0x0C, 0x00, 0x02, 0x0B, 0x07, 0x05, 0x03
};

//~~~Public Functions~~~//

void Blake2::PermuteR10P512C(const std::vector<byte> &Input, size_t InOffset, std::array<uint, 8> &State, const std::array<uint, 8> &IV)
{
	std::array<uint, 16> M;
	std::array<uint, 16> R{
		State[0],
		State[1],
		State[2],
		State[3],
		State[4],
		State[5],
		State[6],
		State[7],
		IV[0],
		IV[1],
		IV[2],
		IV[3],
		IV[4],
		IV[5],
		IV[6],
		IV[7] };
	size_t i;

	IntUtils::LeBytesToUL512(Input, InOffset, M, 0);

	for (i = 0; i < 10; ++i)
	{
		// round n
		R[0] += R[4] + M[Sigma256[(i * 16)]];
		R[12] ^= R[0];
		R[12] = ((R[12] >> 16) | (R[12] << (32 - 16)));
		R[8] += R[12];
		R[4] ^= R[8];
		R[4] = ((R[4] >> 12) | (R[4] << (32 - 12)));
		R[0] += R[4] + M[Sigma256[(i * 16) + 1]];
		R[12] ^= R[0];
		R[12] = ((R[12] >> 8) | (R[12] << (32 - 8)));
		R[8] += R[12];
		R[4] ^= R[8];
		R[4] = ((R[4] >> 7) | (R[4] << (32 - 7)));

		R[1] += R[5] + M[Sigma256[(i * 16) + 2]];
		R[13] ^= R[1];
		R[13] = ((R[13] >> 16) | (R[13] << (32 - 16)));
		R[9] += R[13];
		R[5] ^= R[9];
		R[5] = ((R[5] >> 12) | (R[5] << (32 - 12)));
		R[1] += R[5] + M[Sigma256[(i * 16) + 3]];
		R[13] ^= R[1];
		R[13] = ((R[13] >> 8) | (R[13] << (32 - 8)));
		R[9] += R[13];
		R[5] ^= R[9];
		R[5] = ((R[5] >> 7) | (R[5] << (32 - 7)));

		R[2] += R[6] + M[Sigma256[(i * 16) + 4]];
		R[14] ^= R[2];
		R[14] = ((R[14] >> 16) | (R[14] << (32 - 16)));
		R[10] += R[14];
		R[6] ^= R[10];
		R[6] = ((R[6] >> 12) | (R[6] << (32 - 12)));
		R[2] += R[6] + M[Sigma256[(i * 16) + 5]];
		R[14] ^= R[2];
		R[14] = ((R[14] >> 8) | (R[14] << (32 - 8)));
		R[10] += R[14];
		R[6] ^= R[10];
		R[6] = ((R[6] >> 7) | (R[6] << (32 - 7)));

		R[3] += R[7] + M[Sigma256[(i * 16) + 6]];
		R[15] ^= R[3];
		R[15] = ((R[15] >> 16) | (R[15] << (32 - 16)));
		R[11] += R[15];
		R[7] ^= R[11];
		R[7] = ((R[7] >> 12) | (R[7] << (32 - 12)));
		R[3] += R[7] + M[Sigma256[(i * 16) + 7]];
		R[15] ^= R[3];
		R[15] = ((R[15] >> 8) | (R[15] << (32 - 8)));
		R[11] += R[15];
		R[7] ^= R[11];
		R[7] = ((R[7] >> 7) | (R[7] << (32 - 7)));

		R[0] += R[5] + M[Sigma256[(i * 16) + 8]];
		R[15] ^= R[0];
		R[15] = ((R[15] >> 16) | (R[15] << (32 - 16)));
		R[10] += R[15];
		R[5] ^= R[10];
		R[5] = ((R[5] >> 12) | (R[5] << (32 - 12)));
		R[0] += R[5] + M[Sigma256[(i * 16) + 9]];
		R[15] ^= R[0];
		R[15] = ((R[15] >> 8) | (R[15] << (32 - 8)));
		R[10] += R[15];
		R[5] ^= R[10];
		R[5] = ((R[5] >> 7) | (R[5] << (32 - 7)));

		R[1] += R[6] + M[Sigma256[(i * 16) + 10]];
		R[12] ^= R[1];
		R[12] = ((R[12] >> 16) | (R[12] << (32 - 16)));
		R[11] += R[12];
		R[6] ^= R[11];
		R[6] = ((R[6] >> 12) | (R[6] << (32 - 12)));
		R[1] += R[6] + M[Sigma256[(i * 16) + 11]];
		R[12] ^= R[1];
		R[12] = ((R[12] >> 8) | (R[12] << (32 - 8)));
		R[11] += R[12];
		R[6] ^= R[11];
		R[6] = ((R[6] >> 7) | (R[6] << (32 - 7)));

		R[2] += R[7] + M[Sigma256[(i * 16) + 12]];
		R[13] ^= R[2];
		R[13] = ((R[13] >> 16) | (R[13] << (32 - 16)));
		R[8] += R[13];
		R[7] ^= R[8];
		R[7] = ((R[7] >> 12) | (R[7] << (32 - 12)));
		R[2] += R[7] + M[Sigma256[(i * 16) + 13]];
		R[13] ^= R[2];
		R[13] = ((R[13] >> 8) | (R[13] << (32 - 8)));
		R[8] += R[13];
		R[7] ^= R[8];
		R[7] = ((R[7] >> 7) | (R[7] << (32 - 7)));

		R[3] += R[4] + M[Sigma256[(i * 16) + 14]];
		R[14] ^= R[3];
		R[14] = ((R[14] >> 16) | (R[14] << (32 - 16)));
		R[9] += R[14];
		R[4] ^= R[9];
		R[4] = ((R[4] >> 12) | (R[4] << (32 - 12)));
		R[3] += R[4] + M[Sigma256[(i * 16) + 15]];
		R[14] ^= R[3];
		R[14] = ((R[14] >> 8) | (R[14] << (32 - 8)));
		R[9] += R[14];
		R[4] ^= R[9];
		R[4] = ((R[4] >> 7) | (R[4] << (32 - 7)));
	}

	State[0] ^= R[0] ^ R[8];
	State[1] ^= R[1] ^ R[9];
	State[2] ^= R[2] ^ R[10];
	State[3] ^= R[3] ^ R[11];
	State[4] ^= R[4] ^ R[12];
	State[5] ^= R[5] ^ R[13];
	State[6] ^= R[6] ^ R[14];
	State[7] ^= R[7] ^ R[15];
}

void Blake2::PermuteR10P512U(const std::vector<byte> &Input, size_t InOffset, std::array<uint, 8> &State, const std::array<uint, 8> &IV)
{
	uint M0 = IntUtils::LeBytesTo32(Input, InOffset);
	uint M1 = IntUtils::LeBytesTo32(Input, InOffset + 4);
	uint M2 = IntUtils::LeBytesTo32(Input, InOffset + 8);
	uint M3 = IntUtils::LeBytesTo32(Input, InOffset + 12);
	uint M4 = IntUtils::LeBytesTo32(Input, InOffset + 16);
	uint M5 = IntUtils::LeBytesTo32(Input, InOffset + 20);
	uint M6 = IntUtils::LeBytesTo32(Input, InOffset + 24);
	uint M7 = IntUtils::LeBytesTo32(Input, InOffset + 28);
	uint M8 = IntUtils::LeBytesTo32(Input, InOffset + 32);
	uint M9 = IntUtils::LeBytesTo32(Input, InOffset + 36);
	uint M10 = IntUtils::LeBytesTo32(Input, InOffset + 40);
	uint M11 = IntUtils::LeBytesTo32(Input, InOffset + 44);
	uint M12 = IntUtils::LeBytesTo32(Input, InOffset + 48);
	uint M13 = IntUtils::LeBytesTo32(Input, InOffset + 52);
	uint M14 = IntUtils::LeBytesTo32(Input, InOffset + 56);
	uint M15 = IntUtils::LeBytesTo32(Input, InOffset + 60);
	uint R0 = State[0];
	uint R1 = State[1];
	uint R2 = State[2];
	uint R3 = State[3];
	uint R4 = State[4];
	uint R5 = State[5];
	uint R6 = State[6];
	uint R7 = State[7];
	uint R8 = IV[0];
	uint R9 = IV[1];
	uint R10 = IV[2];
	uint R11 = IV[3];
	uint R12 = IV[4];
	uint R13 = IV[5];
	uint R14 = IV[6];
	uint R15 = IV[7];

	// round 0
	R0 += R4 + M0;
	R12 ^= R0;
	R12 = ((R12 >> 16) | (R12 << (32 - 16)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 12) | (R4 << (32 - 12)));
	R0 += R4 + M1;
	R12 ^= R0;
	R12 = ((R12 >> 8) | (R12 << (32 - 8)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 7) | (R4 << (32 - 7)));

	R1 += R5 + M2;
	R13 ^= R1;
	R13 = ((R13 >> 16) | (R13 << (32 - 16)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 12) | (R5 << (32 - 12)));
	R1 += R5 + M3;
	R13 ^= R1;
	R13 = ((R13 >> 8) | (R13 << (32 - 8)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 7) | (R5 << (32 - 7)));

	R2 += R6 + M4;
	R14 ^= R2;
	R14 = ((R14 >> 16) | (R14 << (32 - 16)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 12) | (R6 << (32 - 12)));
	R2 += R6 + M5;
	R14 ^= R2;
	R14 = ((R14 >> 8) | (R14 << (32 - 8)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 7) | (R6 << (32 - 7)));

	R3 += R7 + M6;
	R15 ^= R3;
	R15 = ((R15 >> 16) | (R15 << (32 - 16)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 12) | (R7 << (32 - 12)));
	R3 += R7 + M7;
	R15 ^= R3;
	R15 = ((R15 >> 8) | (R15 << (32 - 8)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 7) | (R7 << (32 - 7)));

	R0 += R5 + M8;
	R15 ^= R0;
	R15 = ((R15 >> 16) | (R15 << (32 - 16)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 12) | (R5 << (32 - 12)));
	R0 += R5 + M9;
	R15 ^= R0;
	R15 = ((R15 >> 8) | (R15 << (32 - 8)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 7) | (R5 << (32 - 7)));

	R1 += R6 + M10;
	R12 ^= R1;
	R12 = ((R12 >> 16) | (R12 << (32 - 16)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 12) | (R6 << (32 - 12)));
	R1 += R6 + M11;
	R12 ^= R1;
	R12 = ((R12 >> 8) | (R12 << (32 - 8)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 7) | (R6 << (32 - 7)));

	R2 += R7 + M12;
	R13 ^= R2;
	R13 = ((R13 >> 16) | (R13 << (32 - 16)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 12) | (R7 << (32 - 12)));
	R2 += R7 + M13;
	R13 ^= R2;
	R13 = ((R13 >> 8) | (R13 << (32 - 8)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 7) | (R7 << (32 - 7)));

	R3 += R4 + M14;
	R14 ^= R3;
	R14 = ((R14 >> 16) | (R14 << (32 - 16)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 12) | (R4 << (32 - 12)));
	R3 += R4 + M15;
	R14 ^= R3;
	R14 = ((R14 >> 8) | (R14 << (32 - 8)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 7) | (R4 << (32 - 7)));

	// round 1
	R0 += R4 + M14;
	R12 ^= R0;
	R12 = ((R12 >> 16) | (R12 << (32 - 16)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 12) | (R4 << (32 - 12)));
	R0 += R4 + M10;
	R12 ^= R0;
	R12 = ((R12 >> 8) | (R12 << (32 - 8)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 7) | (R4 << (32 - 7)));

	R1 += R5 + M4;
	R13 ^= R1;
	R13 = ((R13 >> 16) | (R13 << (32 - 16)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 12) | (R5 << (32 - 12)));
	R1 += R5 + M8;
	R13 ^= R1;
	R13 = ((R13 >> 8) | (R13 << (32 - 8)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 7) | (R5 << (32 - 7)));

	R2 += R6 + M9;
	R14 ^= R2;
	R14 = ((R14 >> 16) | (R14 << (32 - 16)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 12) | (R6 << (32 - 12)));
	R2 += R6 + M15;
	R14 ^= R2;
	R14 = ((R14 >> 8) | (R14 << (32 - 8)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 7) | (R6 << (32 - 7)));

	R3 += R7 + M13;
	R15 ^= R3;
	R15 = ((R15 >> 16) | (R15 << (32 - 16)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 12) | (R7 << (32 - 12)));
	R3 += R7 + M6;
	R15 ^= R3;
	R15 = ((R15 >> 8) | (R15 << (32 - 8)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 7) | (R7 << (32 - 7)));

	R0 += R5 + M1;
	R15 ^= R0;
	R15 = ((R15 >> 16) | (R15 << (32 - 16)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 12) | (R5 << (32 - 12)));
	R0 += R5 + M12;
	R15 ^= R0;
	R15 = ((R15 >> 8) | (R15 << (32 - 8)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 7) | (R5 << (32 - 7)));

	R1 += R6 + M0;
	R12 ^= R1;
	R12 = ((R12 >> 16) | (R12 << (32 - 16)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 12) | (R6 << (32 - 12)));
	R1 += R6 + M2;
	R12 ^= R1;
	R12 = ((R12 >> 8) | (R12 << (32 - 8)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 7) | (R6 << (32 - 7)));

	R2 += R7 + M11;
	R13 ^= R2;
	R13 = ((R13 >> 16) | (R13 << (32 - 16)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 12) | (R7 << (32 - 12)));
	R2 += R7 + M7;
	R13 ^= R2;
	R13 = ((R13 >> 8) | (R13 << (32 - 8)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 7) | (R7 << (32 - 7)));

	R3 += R4 + M5;
	R14 ^= R3;
	R14 = ((R14 >> 16) | (R14 << (32 - 16)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 12) | (R4 << (32 - 12)));
	R3 += R4 + M3;
	R14 ^= R3;
	R14 = ((R14 >> 8) | (R14 << (32 - 8)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 7) | (R4 << (32 - 7)));

	// round 2
	R0 += R4 + M11;
	R12 ^= R0;
	R12 = ((R12 >> 16) | (R12 << (32 - 16)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 12) | (R4 << (32 - 12)));
	R0 += R4 + M8;
	R12 ^= R0;
	R12 = ((R12 >> 8) | (R12 << (32 - 8)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 7) | (R4 << (32 - 7)));

	R1 += R5 + M12;
	R13 ^= R1;
	R13 = ((R13 >> 16) | (R13 << (32 - 16)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 12) | (R5 << (32 - 12)));
	R1 += R5 + M0;
	R13 ^= R1;
	R13 = ((R13 >> 8) | (R13 << (32 - 8)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 7) | (R5 << (32 - 7)));

	R2 += R6 + M5;
	R14 ^= R2;
	R14 = ((R14 >> 16) | (R14 << (32 - 16)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 12) | (R6 << (32 - 12)));
	R2 += R6 + M2;
	R14 ^= R2;
	R14 = ((R14 >> 8) | (R14 << (32 - 8)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 7) | (R6 << (32 - 7)));

	R3 += R7 + M15;
	R15 ^= R3;
	R15 = ((R15 >> 16) | (R15 << (32 - 16)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 12) | (R7 << (32 - 12)));
	R3 += R7 + M13;
	R15 ^= R3;
	R15 = ((R15 >> 8) | (R15 << (32 - 8)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 7) | (R7 << (32 - 7)));

	R0 += R5 + M10;
	R15 ^= R0;
	R15 = ((R15 >> 16) | (R15 << (32 - 16)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 12) | (R5 << (32 - 12)));
	R0 += R5 + M14;
	R15 ^= R0;
	R15 = ((R15 >> 8) | (R15 << (32 - 8)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 7) | (R5 << (32 - 7)));

	R1 += R6 + M3;
	R12 ^= R1;
	R12 = ((R12 >> 16) | (R12 << (32 - 16)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 12) | (R6 << (32 - 12)));
	R1 += R6 + M6;
	R12 ^= R1;
	R12 = ((R12 >> 8) | (R12 << (32 - 8)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 7) | (R6 << (32 - 7)));

	R2 += R7 + M7;
	R13 ^= R2;
	R13 = ((R13 >> 16) | (R13 << (32 - 16)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 12) | (R7 << (32 - 12)));
	R2 += R7 + M1;
	R13 ^= R2;
	R13 = ((R13 >> 8) | (R13 << (32 - 8)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 7) | (R7 << (32 - 7)));

	R3 += R4 + M9;
	R14 ^= R3;
	R14 = ((R14 >> 16) | (R14 << (32 - 16)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 12) | (R4 << (32 - 12)));
	R3 += R4 + M4;
	R14 ^= R3;
	R14 = ((R14 >> 8) | (R14 << (32 - 8)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 7) | (R4 << (32 - 7)));

	// round 3
	R0 += R4 + M7;
	R12 ^= R0;
	R12 = ((R12 >> 16) | (R12 << (32 - 16)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 12) | (R4 << (32 - 12)));
	R0 += R4 + M9;
	R12 ^= R0;
	R12 = ((R12 >> 8) | (R12 << (32 - 8)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 7) | (R4 << (32 - 7)));

	R1 += R5 + M3;
	R13 ^= R1;
	R13 = ((R13 >> 16) | (R13 << (32 - 16)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 12) | (R5 << (32 - 12)));
	R1 += R5 + M1;
	R13 ^= R1;
	R13 = ((R13 >> 8) | (R13 << (32 - 8)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 7) | (R5 << (32 - 7)));

	R2 += R6 + M13;
	R14 ^= R2;
	R14 = ((R14 >> 16) | (R14 << (32 - 16)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 12) | (R6 << (32 - 12)));
	R2 += R6 + M12;
	R14 ^= R2;
	R14 = ((R14 >> 8) | (R14 << (32 - 8)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 7) | (R6 << (32 - 7)));

	R3 += R7 + M11;
	R15 ^= R3;
	R15 = ((R15 >> 16) | (R15 << (32 - 16)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 12) | (R7 << (32 - 12)));
	R3 += R7 + M14;
	R15 ^= R3;
	R15 = ((R15 >> 8) | (R15 << (32 - 8)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 7) | (R7 << (32 - 7)));

	R0 += R5 + M2;
	R15 ^= R0;
	R15 = ((R15 >> 16) | (R15 << (32 - 16)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 12) | (R5 << (32 - 12)));
	R0 += R5 + M6;
	R15 ^= R0;
	R15 = ((R15 >> 8) | (R15 << (32 - 8)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 7) | (R5 << (32 - 7)));

	R1 += R6 + M5;
	R12 ^= R1;
	R12 = ((R12 >> 16) | (R12 << (32 - 16)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 12) | (R6 << (32 - 12)));
	R1 += R6 + M10;
	R12 ^= R1;
	R12 = ((R12 >> 8) | (R12 << (32 - 8)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 7) | (R6 << (32 - 7)));

	R2 += R7 + M4;
	R13 ^= R2;
	R13 = ((R13 >> 16) | (R13 << (32 - 16)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 12) | (R7 << (32 - 12)));
	R2 += R7 + M0;
	R13 ^= R2;
	R13 = ((R13 >> 8) | (R13 << (32 - 8)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 7) | (R7 << (32 - 7)));

	R3 += R4 + M15;
	R14 ^= R3;
	R14 = ((R14 >> 16) | (R14 << (32 - 16)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 12) | (R4 << (32 - 12)));
	R3 += R4 + M8;
	R14 ^= R3;
	R14 = ((R14 >> 8) | (R14 << (32 - 8)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 7) | (R4 << (32 - 7)));

	// round 4
	R0 += R4 + M9;
	R12 ^= R0;
	R12 = ((R12 >> 16) | (R12 << (32 - 16)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 12) | (R4 << (32 - 12)));
	R0 += R4 + M0;
	R12 ^= R0;
	R12 = ((R12 >> 8) | (R12 << (32 - 8)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 7) | (R4 << (32 - 7)));

	R1 += R5 + M5;
	R13 ^= R1;
	R13 = ((R13 >> 16) | (R13 << (32 - 16)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 12) | (R5 << (32 - 12)));
	R1 += R5 + M7;
	R13 ^= R1;
	R13 = ((R13 >> 8) | (R13 << (32 - 8)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 7) | (R5 << (32 - 7)));

	R2 += R6 + M2;
	R14 ^= R2;
	R14 = ((R14 >> 16) | (R14 << (32 - 16)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 12) | (R6 << (32 - 12)));
	R2 += R6 + M4;
	R14 ^= R2;
	R14 = ((R14 >> 8) | (R14 << (32 - 8)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 7) | (R6 << (32 - 7)));

	R3 += R7 + M10;
	R15 ^= R3;
	R15 = ((R15 >> 16) | (R15 << (32 - 16)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 12) | (R7 << (32 - 12)));
	R3 += R7 + M15;
	R15 ^= R3;
	R15 = ((R15 >> 8) | (R15 << (32 - 8)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 7) | (R7 << (32 - 7)));

	R0 += R5 + M14;
	R15 ^= R0;
	R15 = ((R15 >> 16) | (R15 << (32 - 16)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 12) | (R5 << (32 - 12)));
	R0 += R5 + M1;
	R15 ^= R0;
	R15 = ((R15 >> 8) | (R15 << (32 - 8)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 7) | (R5 << (32 - 7)));

	R1 += R6 + M11;
	R12 ^= R1;
	R12 = ((R12 >> 16) | (R12 << (32 - 16)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 12) | (R6 << (32 - 12)));
	R1 += R6 + M12;
	R12 ^= R1;
	R12 = ((R12 >> 8) | (R12 << (32 - 8)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 7) | (R6 << (32 - 7)));

	R2 += R7 + M6;
	R13 ^= R2;
	R13 = ((R13 >> 16) | (R13 << (32 - 16)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 12) | (R7 << (32 - 12)));
	R2 += R7 + M8;
	R13 ^= R2;
	R13 = ((R13 >> 8) | (R13 << (32 - 8)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 7) | (R7 << (32 - 7)));

	R3 += R4 + M3;
	R14 ^= R3;
	R14 = ((R14 >> 16) | (R14 << (32 - 16)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 12) | (R4 << (32 - 12)));
	R3 += R4 + M13;
	R14 ^= R3;
	R14 = ((R14 >> 8) | (R14 << (32 - 8)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 7) | (R4 << (32 - 7)));

	// round 5
	R0 += R4 + M2;
	R12 ^= R0;
	R12 = ((R12 >> 16) | (R12 << (32 - 16)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 12) | (R4 << (32 - 12)));
	R0 += R4 + M12;
	R12 ^= R0;
	R12 = ((R12 >> 8) | (R12 << (32 - 8)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 7) | (R4 << (32 - 7)));

	R1 += R5 + M6;
	R13 ^= R1;
	R13 = ((R13 >> 16) | (R13 << (32 - 16)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 12) | (R5 << (32 - 12)));
	R1 += R5 + M10;
	R13 ^= R1;
	R13 = ((R13 >> 8) | (R13 << (32 - 8)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 7) | (R5 << (32 - 7)));

	R2 += R6 + M0;
	R14 ^= R2;
	R14 = ((R14 >> 16) | (R14 << (32 - 16)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 12) | (R6 << (32 - 12)));
	R2 += R6 + M11;
	R14 ^= R2;
	R14 = ((R14 >> 8) | (R14 << (32 - 8)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 7) | (R6 << (32 - 7)));

	R3 += R7 + M8;
	R15 ^= R3;
	R15 = ((R15 >> 16) | (R15 << (32 - 16)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 12) | (R7 << (32 - 12)));
	R3 += R7 + M3;
	R15 ^= R3;
	R15 = ((R15 >> 8) | (R15 << (32 - 8)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 7) | (R7 << (32 - 7)));

	R0 += R5 + M4;
	R15 ^= R0;
	R15 = ((R15 >> 16) | (R15 << (32 - 16)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 12) | (R5 << (32 - 12)));
	R0 += R5 + M13;
	R15 ^= R0;
	R15 = ((R15 >> 8) | (R15 << (32 - 8)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 7) | (R5 << (32 - 7)));

	R1 += R6 + M7;
	R12 ^= R1;
	R12 = ((R12 >> 16) | (R12 << (32 - 16)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 12) | (R6 << (32 - 12)));
	R1 += R6 + M5;
	R12 ^= R1;
	R12 = ((R12 >> 8) | (R12 << (32 - 8)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 7) | (R6 << (32 - 7)));

	R2 += R7 + M15;
	R13 ^= R2;
	R13 = ((R13 >> 16) | (R13 << (32 - 16)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 12) | (R7 << (32 - 12)));
	R2 += R7 + M14;
	R13 ^= R2;
	R13 = ((R13 >> 8) | (R13 << (32 - 8)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 7) | (R7 << (32 - 7)));

	R3 += R4 + M1;
	R14 ^= R3;
	R14 = ((R14 >> 16) | (R14 << (32 - 16)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 12) | (R4 << (32 - 12)));
	R3 += R4 + M9;
	R14 ^= R3;
	R14 = ((R14 >> 8) | (R14 << (32 - 8)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 7) | (R4 << (32 - 7)));

	// round 6
	R0 += R4 + M12;
	R12 ^= R0;
	R12 = ((R12 >> 16) | (R12 << (32 - 16)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 12) | (R4 << (32 - 12)));
	R0 += R4 + M5;
	R12 ^= R0;
	R12 = ((R12 >> 8) | (R12 << (32 - 8)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 7) | (R4 << (32 - 7)));

	R1 += R5 + M1;
	R13 ^= R1;
	R13 = ((R13 >> 16) | (R13 << (32 - 16)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 12) | (R5 << (32 - 12)));
	R1 += R5 + M15;
	R13 ^= R1;
	R13 = ((R13 >> 8) | (R13 << (32 - 8)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 7) | (R5 << (32 - 7)));

	R2 += R6 + M14;
	R14 ^= R2;
	R14 = ((R14 >> 16) | (R14 << (32 - 16)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 12) | (R6 << (32 - 12)));
	R2 += R6 + M13;
	R14 ^= R2;
	R14 = ((R14 >> 8) | (R14 << (32 - 8)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 7) | (R6 << (32 - 7)));

	R3 += R7 + M4;
	R15 ^= R3;
	R15 = ((R15 >> 16) | (R15 << (32 - 16)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 12) | (R7 << (32 - 12)));
	R3 += R7 + M10;
	R15 ^= R3;
	R15 = ((R15 >> 8) | (R15 << (32 - 8)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 7) | (R7 << (32 - 7)));

	R0 += R5 + M0;
	R15 ^= R0;
	R15 = ((R15 >> 16) | (R15 << (32 - 16)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 12) | (R5 << (32 - 12)));
	R0 += R5 + M7;
	R15 ^= R0;
	R15 = ((R15 >> 8) | (R15 << (32 - 8)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 7) | (R5 << (32 - 7)));

	R1 += R6 + M6;
	R12 ^= R1;
	R12 = ((R12 >> 16) | (R12 << (32 - 16)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 12) | (R6 << (32 - 12)));
	R1 += R6 + M3;
	R12 ^= R1;
	R12 = ((R12 >> 8) | (R12 << (32 - 8)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 7) | (R6 << (32 - 7)));

	R2 += R7 + M9;
	R13 ^= R2;
	R13 = ((R13 >> 16) | (R13 << (32 - 16)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 12) | (R7 << (32 - 12)));
	R2 += R7 + M2;
	R13 ^= R2;
	R13 = ((R13 >> 8) | (R13 << (32 - 8)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 7) | (R7 << (32 - 7)));

	R3 += R4 + M8;
	R14 ^= R3;
	R14 = ((R14 >> 16) | (R14 << (32 - 16)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 12) | (R4 << (32 - 12)));
	R3 += R4 + M11;
	R14 ^= R3;
	R14 = ((R14 >> 8) | (R14 << (32 - 8)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 7) | (R4 << (32 - 7)));

	// round 7
	R0 += R4 + M13;
	R12 ^= R0;
	R12 = ((R12 >> 16) | (R12 << (32 - 16)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 12) | (R4 << (32 - 12)));
	R0 += R4 + M11;
	R12 ^= R0;
	R12 = ((R12 >> 8) | (R12 << (32 - 8)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 7) | (R4 << (32 - 7)));

	R1 += R5 + M7;
	R13 ^= R1;
	R13 = ((R13 >> 16) | (R13 << (32 - 16)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 12) | (R5 << (32 - 12)));
	R1 += R5 + M14;
	R13 ^= R1;
	R13 = ((R13 >> 8) | (R13 << (32 - 8)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 7) | (R5 << (32 - 7)));

	R2 += R6 + M12;
	R14 ^= R2;
	R14 = ((R14 >> 16) | (R14 << (32 - 16)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 12) | (R6 << (32 - 12)));
	R2 += R6 + M1;
	R14 ^= R2;
	R14 = ((R14 >> 8) | (R14 << (32 - 8)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 7) | (R6 << (32 - 7)));

	R3 += R7 + M3;
	R15 ^= R3;
	R15 = ((R15 >> 16) | (R15 << (32 - 16)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 12) | (R7 << (32 - 12)));
	R3 += R7 + M9;
	R15 ^= R3;
	R15 = ((R15 >> 8) | (R15 << (32 - 8)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 7) | (R7 << (32 - 7)));

	R0 += R5 + M5;
	R15 ^= R0;
	R15 = ((R15 >> 16) | (R15 << (32 - 16)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 12) | (R5 << (32 - 12)));
	R0 += R5 + M0;
	R15 ^= R0;
	R15 = ((R15 >> 8) | (R15 << (32 - 8)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 7) | (R5 << (32 - 7)));

	R1 += R6 + M15;
	R12 ^= R1;
	R12 = ((R12 >> 16) | (R12 << (32 - 16)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 12) | (R6 << (32 - 12)));
	R1 += R6 + M4;
	R12 ^= R1;
	R12 = ((R12 >> 8) | (R12 << (32 - 8)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 7) | (R6 << (32 - 7)));

	R2 += R7 + M8;
	R13 ^= R2;
	R13 = ((R13 >> 16) | (R13 << (32 - 16)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 12) | (R7 << (32 - 12)));
	R2 += R7 + M6;
	R13 ^= R2;
	R13 = ((R13 >> 8) | (R13 << (32 - 8)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 7) | (R7 << (32 - 7)));

	R3 += R4 + M2;
	R14 ^= R3;
	R14 = ((R14 >> 16) | (R14 << (32 - 16)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 12) | (R4 << (32 - 12)));
	R3 += R4 + M10;
	R14 ^= R3;
	R14 = ((R14 >> 8) | (R14 << (32 - 8)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 7) | (R4 << (32 - 7)));

	// round 8
	R0 += R4 + M6;
	R12 ^= R0;
	R12 = ((R12 >> 16) | (R12 << (32 - 16)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 12) | (R4 << (32 - 12)));
	R0 += R4 + M15;
	R12 ^= R0;
	R12 = ((R12 >> 8) | (R12 << (32 - 8)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 7) | (R4 << (32 - 7)));

	R1 += R5 + M14;
	R13 ^= R1;
	R13 = ((R13 >> 16) | (R13 << (32 - 16)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 12) | (R5 << (32 - 12)));
	R1 += R5 + M9;
	R13 ^= R1;
	R13 = ((R13 >> 8) | (R13 << (32 - 8)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 7) | (R5 << (32 - 7)));

	R2 += R6 + M11;
	R14 ^= R2;
	R14 = ((R14 >> 16) | (R14 << (32 - 16)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 12) | (R6 << (32 - 12)));
	R2 += R6 + M3;
	R14 ^= R2;
	R14 = ((R14 >> 8) | (R14 << (32 - 8)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 7) | (R6 << (32 - 7)));

	R3 += R7 + M0;
	R15 ^= R3;
	R15 = ((R15 >> 16) | (R15 << (32 - 16)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 12) | (R7 << (32 - 12)));
	R3 += R7 + M8;
	R15 ^= R3;
	R15 = ((R15 >> 8) | (R15 << (32 - 8)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 7) | (R7 << (32 - 7)));

	R0 += R5 + M12;
	R15 ^= R0;
	R15 = ((R15 >> 16) | (R15 << (32 - 16)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 12) | (R5 << (32 - 12)));
	R0 += R5 + M2;
	R15 ^= R0;
	R15 = ((R15 >> 8) | (R15 << (32 - 8)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 7) | (R5 << (32 - 7)));

	R1 += R6 + M13;
	R12 ^= R1;
	R12 = ((R12 >> 16) | (R12 << (32 - 16)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 12) | (R6 << (32 - 12)));
	R1 += R6 + M7;
	R12 ^= R1;
	R12 = ((R12 >> 8) | (R12 << (32 - 8)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 7) | (R6 << (32 - 7)));

	R2 += R7 + M1;
	R13 ^= R2;
	R13 = ((R13 >> 16) | (R13 << (32 - 16)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 12) | (R7 << (32 - 12)));
	R2 += R7 + M4;
	R13 ^= R2;
	R13 = ((R13 >> 8) | (R13 << (32 - 8)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 7) | (R7 << (32 - 7)));

	R3 += R4 + M10;
	R14 ^= R3;
	R14 = ((R14 >> 16) | (R14 << (32 - 16)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 12) | (R4 << (32 - 12)));
	R3 += R4 + M5;
	R14 ^= R3;
	R14 = ((R14 >> 8) | (R14 << (32 - 8)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 7) | (R4 << (32 - 7)));

	// round 9
	R0 += R4 + M10;
	R12 ^= R0;
	R12 = ((R12 >> 16) | (R12 << (32 - 16)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 12) | (R4 << (32 - 12)));
	R0 += R4 + M2;
	R12 ^= R0;
	R12 = ((R12 >> 8) | (R12 << (32 - 8)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 7) | (R4 << (32 - 7)));

	R1 += R5 + M8;
	R13 ^= R1;
	R13 = ((R13 >> 16) | (R13 << (32 - 16)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 12) | (R5 << (32 - 12)));
	R1 += R5 + M4;
	R13 ^= R1;
	R13 = ((R13 >> 8) | (R13 << (32 - 8)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 7) | (R5 << (32 - 7)));

	R2 += R6 + M7;
	R14 ^= R2;
	R14 = ((R14 >> 16) | (R14 << (32 - 16)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 12) | (R6 << (32 - 12)));
	R2 += R6 + M6;
	R14 ^= R2;
	R14 = ((R14 >> 8) | (R14 << (32 - 8)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 7) | (R6 << (32 - 7)));

	R3 += R7 + M1;
	R15 ^= R3;
	R15 = ((R15 >> 16) | (R15 << (32 - 16)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 12) | (R7 << (32 - 12)));
	R3 += R7 + M5;
	R15 ^= R3;
	R15 = ((R15 >> 8) | (R15 << (32 - 8)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 7) | (R7 << (32 - 7)));

	R0 += R5 + M15;
	R15 ^= R0;
	R15 = ((R15 >> 16) | (R15 << (32 - 16)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 12) | (R5 << (32 - 12)));
	R0 += R5 + M11;
	R15 ^= R0;
	R15 = ((R15 >> 8) | (R15 << (32 - 8)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 7) | (R5 << (32 - 7)));

	R1 += R6 + M9;
	R12 ^= R1;
	R12 = ((R12 >> 16) | (R12 << (32 - 16)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 12) | (R6 << (32 - 12)));
	R1 += R6 + M14;
	R12 ^= R1;
	R12 = ((R12 >> 8) | (R12 << (32 - 8)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 7) | (R6 << (32 - 7)));

	R2 += R7 + M3;
	R13 ^= R2;
	R13 = ((R13 >> 16) | (R13 << (32 - 16)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 12) | (R7 << (32 - 12)));
	R2 += R7 + M12;
	R13 ^= R2;
	R13 = ((R13 >> 8) | (R13 << (32 - 8)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 7) | (R7 << (32 - 7)));

	R3 += R4 + M13;
	R14 ^= R3;
	R14 = ((R14 >> 16) | (R14 << (32 - 16)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 12) | (R4 << (32 - 12)));
	R3 += R4 + M0;
	R14 ^= R3;
	R14 = ((R14 >> 8) | (R14 << (32 - 8)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 7) | (R4 << (32 - 7)));

	State[0] ^= R0 ^ R8;
	State[1] ^= R1 ^ R9;
	State[2] ^= R2 ^ R10;
	State[3] ^= R3 ^ R11;
	State[4] ^= R4 ^ R12;
	State[5] ^= R5 ^ R13;
	State[6] ^= R6 ^ R14;
	State[7] ^= R7 ^ R15;
}

#if defined(__AVX__)

void Blake2::PermuteR10P512V(const std::vector<byte> &Input, size_t InOffset, std::array<uint, 8> &State, const std::array<uint, 8> &IV)
{
#if defined(__AVX__)

	__m128i R1, R2, R3, R4;
	__m128i B1, B2, B3, B4;
	__m128i FF0, FF1;
	__m128i T0, T1, T2;

	const __m128i R8 = _mm_set_epi8(12, 15, 14, 13, 8, 11, 10, 9, 4, 7, 6, 5, 0, 3, 2, 1);
	const __m128i R16 = _mm_set_epi8(13, 12, 15, 14, 9, 8, 11, 10, 5, 4, 7, 6, 1, 0, 3, 2);
	const __m128i M0 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&Input[InOffset]));
	const __m128i M1 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&Input[InOffset + 16]));
	const __m128i M2 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&Input[InOffset + 32]));
	const __m128i M3 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&Input[InOffset + 48]));

	R1 = FF0 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&State[0]));
	R2 = FF1 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&State[4]));
	R3 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&IV[0]));
	R4 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&IV[4]));

	// round 0
	// lm 0.1
	B1 = _mm_castps_si128(_mm_shuffle_ps(_mm_castsi128_ps(M0), _mm_castsi128_ps(M1), _MM_SHUFFLE(2, 0, 2, 0)));
	// g1
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B1), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -16);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -12);

	// lm 0.2
	B2 = _mm_castps_si128(_mm_shuffle_ps(_mm_castsi128_ps(M0), _mm_castsi128_ps(M1), _MM_SHUFFLE(3, 1, 3, 1)));
	// g2
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B2), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -8);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -7);

	// diag
	R4 = _mm_shuffle_epi32(R4, _MM_SHUFFLE(2, 1, 0, 3));
	R3 = _mm_shuffle_epi32(R3, _MM_SHUFFLE(1, 0, 3, 2));
	R2 = _mm_shuffle_epi32(R2, _MM_SHUFFLE(0, 3, 2, 1));

	// lm 0.3
	B3 = _mm_castps_si128(_mm_shuffle_ps(_mm_castsi128_ps(M2), _mm_castsi128_ps(M3), _MM_SHUFFLE(2, 0, 2, 0)));
	// g1
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B3), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -16);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -12);

	// lm 0.4
	B4 = _mm_castps_si128(_mm_shuffle_ps(_mm_castsi128_ps(M2), _mm_castsi128_ps(M3), _MM_SHUFFLE(3, 1, 3, 1)));
	// g2
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B4), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -8);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -7);
	R4 = _mm_shuffle_epi32(R4, _MM_SHUFFLE(0, 3, 2, 1));
	R3 = _mm_shuffle_epi32(R3, _MM_SHUFFLE(1, 0, 3, 2));
	R2 = _mm_shuffle_epi32(R2, _MM_SHUFFLE(2, 1, 0, 3));

	// round 1
	// lm 1.1
	T0 = _mm_blend_epi16(M1, M2, 0x0C);
	T1 = _mm_slli_si128(M3, 4);
	T2 = _mm_blend_epi16(T0, T1, 0xF0);
	B1 = _mm_shuffle_epi32(T2, _MM_SHUFFLE(2, 1, 0, 3));
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B1), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -16);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -12);

	// lm 1.2
	T0 = _mm_shuffle_epi32(M2, _MM_SHUFFLE(0, 0, 2, 0));
	T1 = _mm_blend_epi16(M1, M3, 0xC0);
	T2 = _mm_blend_epi16(T0, T1, 0xF0);
	B2 = _mm_shuffle_epi32(T2, _MM_SHUFFLE(2, 3, 0, 1));
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B2), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -8);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -7);
	R4 = _mm_shuffle_epi32(R4, _MM_SHUFFLE(2, 1, 0, 3));
	R3 = _mm_shuffle_epi32(R3, _MM_SHUFFLE(1, 0, 3, 2));
	R2 = _mm_shuffle_epi32(R2, _MM_SHUFFLE(0, 3, 2, 1));

	// lm 1.3
	T0 = _mm_slli_si128(M1, 4);
	T1 = _mm_blend_epi16(M2, T0, 0x30);
	T2 = _mm_blend_epi16(M0, T1, 0xF0);
	B3 = _mm_shuffle_epi32(T2, _MM_SHUFFLE(2, 3, 0, 1));
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B3), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -16);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -12);

	// lm 1.4
	T0 = _mm_unpackhi_epi32(M0, M1);
	T1 = _mm_slli_si128(M3, 4);
	T2 = _mm_blend_epi16(T0, T1, 0x0C);
	B4 = _mm_shuffle_epi32(T2, _MM_SHUFFLE(2, 3, 0, 1));
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B4), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -8);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -7);
	R4 = _mm_shuffle_epi32(R4, _MM_SHUFFLE(0, 3, 2, 1));
	R3 = _mm_shuffle_epi32(R3, _MM_SHUFFLE(1, 0, 3, 2));
	R2 = _mm_shuffle_epi32(R2, _MM_SHUFFLE(2, 1, 0, 3));

	// round 2
	// lm 2.1
	T0 = _mm_unpackhi_epi32(M2, M3);
	T1 = _mm_blend_epi16(M3, M1, 0x0C);
	T2 = _mm_blend_epi16(T0, T1, 0x0F);
	B1 = _mm_shuffle_epi32(T2, _MM_SHUFFLE(3, 1, 0, 2));
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B1), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -16);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -12);

	// lm 2.2
	T0 = _mm_unpacklo_epi32(M2, M0);
	T1 = _mm_blend_epi16(T0, M0, 0xF0);
	T2 = _mm_slli_si128(M3, 8);
	B2 = _mm_blend_epi16(T1, T2, 0xC0);
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B2), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -8);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -7);
	R4 = _mm_shuffle_epi32(R4, _MM_SHUFFLE(2, 1, 0, 3));
	R3 = _mm_shuffle_epi32(R3, _MM_SHUFFLE(1, 0, 3, 2));
	R2 = _mm_shuffle_epi32(R2, _MM_SHUFFLE(0, 3, 2, 1));

	// lm 2.3
	T0 = _mm_blend_epi16(M0, M2, 0x3C);
	T1 = _mm_srli_si128(M1, 12);
	T2 = _mm_blend_epi16(T0, T1, 0x03);
	B3 = _mm_shuffle_epi32(T2, _MM_SHUFFLE(1, 0, 3, 2));
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B3), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -16);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -12);

	// lm 2.4
	T0 = _mm_slli_si128(M3, 4);
	T1 = _mm_blend_epi16(M0, M1, 0x33);
	T2 = _mm_blend_epi16(T1, T0, 0xC0);
	B4 = _mm_shuffle_epi32(T2, _MM_SHUFFLE(0, 1, 2, 3));
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B4), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -8);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -7);
	R4 = _mm_shuffle_epi32(R4, _MM_SHUFFLE(0, 3, 2, 1));
	R3 = _mm_shuffle_epi32(R3, _MM_SHUFFLE(1, 0, 3, 2));
	R2 = _mm_shuffle_epi32(R2, _MM_SHUFFLE(2, 1, 0, 3));

	// round 3
	// lm 3.1
	T0 = _mm_unpackhi_epi32(M0, M1);
	T1 = _mm_unpackhi_epi32(T0, M2);
	T2 = _mm_blend_epi16(T1, M3, 0x0C);
	B1 = _mm_shuffle_epi32(T2, _MM_SHUFFLE(3, 1, 0, 2));
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B1), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -16);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -12);

	// lm 3.2
	T0 = _mm_slli_si128(M2, 8);
	T1 = _mm_blend_epi16(M3, M0, 0x0C);
	T2 = _mm_blend_epi16(T1, T0, 0xC0);
	B2 = _mm_shuffle_epi32(T2, _MM_SHUFFLE(2, 0, 1, 3));
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B2), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -8);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -7);
	R4 = _mm_shuffle_epi32(R4, _MM_SHUFFLE(2, 1, 0, 3));
	R3 = _mm_shuffle_epi32(R3, _MM_SHUFFLE(1, 0, 3, 2));
	R2 = _mm_shuffle_epi32(R2, _MM_SHUFFLE(0, 3, 2, 1));

	// lm 3.3
	T0 = _mm_blend_epi16(M0, M1, 0x0F);
	T1 = _mm_blend_epi16(T0, M3, 0xC0);
	B3 = _mm_shuffle_epi32(T1, _MM_SHUFFLE(3, 0, 1, 2));
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B3), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -16);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -12);

	// lm 3.4
	T0 = _mm_unpacklo_epi32(M0, M2);
	T1 = _mm_unpackhi_epi32(M1, M2);
	B4 = _mm_unpacklo_epi64(T1, T0);
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B4), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -8);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -7);
	R4 = _mm_shuffle_epi32(R4, _MM_SHUFFLE(0, 3, 2, 1));
	R3 = _mm_shuffle_epi32(R3, _MM_SHUFFLE(1, 0, 3, 2));
	R2 = _mm_shuffle_epi32(R2, _MM_SHUFFLE(2, 1, 0, 3));

	// round 4
	// lm 4.1
	T0 = _mm_unpacklo_epi64(M1, M2);
	T1 = _mm_unpackhi_epi64(M0, M2);
	T2 = _mm_blend_epi16(T0, T1, 0x33);
	B1 = _mm_shuffle_epi32(T2, _MM_SHUFFLE(2, 0, 1, 3));
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B1), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -16);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -12);

	// lm 4.2
	T0 = _mm_unpackhi_epi64(M1, M3);
	T1 = _mm_unpacklo_epi64(M0, M1);
	B2 = _mm_blend_epi16(T0, T1, 0x33);
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B2), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -8);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -7);
	R4 = _mm_shuffle_epi32(R4, _MM_SHUFFLE(2, 1, 0, 3));
	R3 = _mm_shuffle_epi32(R3, _MM_SHUFFLE(1, 0, 3, 2));
	R2 = _mm_shuffle_epi32(R2, _MM_SHUFFLE(0, 3, 2, 1));

	// lm 4.3
	T0 = _mm_unpackhi_epi64(M3, M1);
	T1 = _mm_unpackhi_epi64(M2, M0);
	B3 = _mm_blend_epi16(T1, T0, 0x33);
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B3), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -16);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -12);

	// lm 4.4
	T0 = _mm_blend_epi16(M0, M2, 0x03);
	T1 = _mm_slli_si128(T0, 8);
	T2 = _mm_blend_epi16(T1, M3, 0x0F);
	B4 = _mm_shuffle_epi32(T2, _MM_SHUFFLE(1, 2, 0, 3));
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B4), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -8);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -7);
	R4 = _mm_shuffle_epi32(R4, _MM_SHUFFLE(0, 3, 2, 1));
	R3 = _mm_shuffle_epi32(R3, _MM_SHUFFLE(1, 0, 3, 2));
	R2 = _mm_shuffle_epi32(R2, _MM_SHUFFLE(2, 1, 0, 3));

	// round 5
	// lm 5.1
	T0 = _mm_unpackhi_epi32(M0, M1);
	T1 = _mm_unpacklo_epi32(M0, M2);
	B1 = _mm_unpacklo_epi64(T0, T1);
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B1), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -16);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -12);

	// lm 5.2
	T0 = _mm_srli_si128(M2, 4);
	T1 = _mm_blend_epi16(M0, M3, 0x03);
	B2 = _mm_blend_epi16(T1, T0, 0x3C);
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B2), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -8);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -7);
	R4 = _mm_shuffle_epi32(R4, _MM_SHUFFLE(2, 1, 0, 3));
	R3 = _mm_shuffle_epi32(R3, _MM_SHUFFLE(1, 0, 3, 2));
	R2 = _mm_shuffle_epi32(R2, _MM_SHUFFLE(0, 3, 2, 1));

	// lm 5.3
	T0 = _mm_blend_epi16(M1, M0, 0x0C);
	T1 = _mm_srli_si128(M3, 4);
	T2 = _mm_blend_epi16(T0, T1, 0x30);
	B3 = _mm_shuffle_epi32(T2, _MM_SHUFFLE(1, 2, 3, 0));
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B3), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -16);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -12);

	// lm 5.4
	T0 = _mm_unpacklo_epi64(M1, M2);
	T1 = _mm_shuffle_epi32(M3, _MM_SHUFFLE(0, 2, 0, 1));
	B4 = _mm_blend_epi16(T0, T1, 0x33);
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B4), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -8);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -7);
	R4 = _mm_shuffle_epi32(R4, _MM_SHUFFLE(0, 3, 2, 1));
	R3 = _mm_shuffle_epi32(R3, _MM_SHUFFLE(1, 0, 3, 2));
	R2 = _mm_shuffle_epi32(R2, _MM_SHUFFLE(2, 1, 0, 3));

	// round 6
	// lm 6.1
	T0 = _mm_slli_si128(M1, 12);
	T1 = _mm_blend_epi16(M0, M3, 0x33);
	B1 = _mm_blend_epi16(T1, T0, 0xC0);
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B1), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -16);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -12);

	// lm 6.2
	T0 = _mm_blend_epi16(M3, M2, 0x30);
	T1 = _mm_srli_si128(M1, 4);
	T2 = _mm_blend_epi16(T0, T1, 0x03);
	B2 = _mm_shuffle_epi32(T2, _MM_SHUFFLE(2, 1, 3, 0));
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B2), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -8);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -7);
	R4 = _mm_shuffle_epi32(R4, _MM_SHUFFLE(2, 1, 0, 3));
	R3 = _mm_shuffle_epi32(R3, _MM_SHUFFLE(1, 0, 3, 2));
	R2 = _mm_shuffle_epi32(R2, _MM_SHUFFLE(0, 3, 2, 1));

	// lm 6.3
	T0 = _mm_unpacklo_epi64(M0, M2);
	T1 = _mm_srli_si128(M1, 4);
	B3 = _mm_shuffle_epi32(_mm_blend_epi16(T0, T1, 0x0C), _MM_SHUFFLE(2, 3, 1, 0));
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B3), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -16);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -12);

	// lm 6.4
	T0 = _mm_unpackhi_epi32(M1, M2);
	T1 = _mm_unpackhi_epi64(M0, T0);
	B4 = _mm_shuffle_epi32(T1, _MM_SHUFFLE(3, 0, 1, 2));
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B4), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -8);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -7);
	R4 = _mm_shuffle_epi32(R4, _MM_SHUFFLE(0, 3, 2, 1));
	R3 = _mm_shuffle_epi32(R3, _MM_SHUFFLE(1, 0, 3, 2));
	R2 = _mm_shuffle_epi32(R2, _MM_SHUFFLE(2, 1, 0, 3));

	// round 7
	// lm 7.1
	T0 = _mm_unpackhi_epi32(M0, M1);
	T1 = _mm_blend_epi16(T0, M3, 0x0F);
	B1 = _mm_shuffle_epi32(T1, _MM_SHUFFLE(2, 0, 3, 1));
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B1), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -16);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -12);

	// lm 7.2
	T0 = _mm_blend_epi16(M2, M3, 0x30);
	T1 = _mm_srli_si128(M0, 4);
	T2 = _mm_blend_epi16(T0, T1, 0x03);
	B2 = _mm_shuffle_epi32(T2, _MM_SHUFFLE(1, 0, 2, 3));
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B2), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -8);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -7);
	R4 = _mm_shuffle_epi32(R4, _MM_SHUFFLE(2, 1, 0, 3));
	R3 = _mm_shuffle_epi32(R3, _MM_SHUFFLE(1, 0, 3, 2));
	R2 = _mm_shuffle_epi32(R2, _MM_SHUFFLE(0, 3, 2, 1));

	// lm 7.3
	T0 = _mm_unpackhi_epi64(M0, M3);
	T1 = _mm_unpacklo_epi64(M1, M2);
	T2 = _mm_blend_epi16(T0, T1, 0x3C);
	B3 = _mm_shuffle_epi32(T2, _MM_SHUFFLE(0, 2, 3, 1));
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B3), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -16);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -12);

	// lm 7.4
	T0 = _mm_unpacklo_epi32(M0, M1);
	T1 = _mm_unpackhi_epi32(M1, M2);
	B4 = _mm_unpacklo_epi64(T0, T1);
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B4), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -8);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -7);
	R4 = _mm_shuffle_epi32(R4, _MM_SHUFFLE(0, 3, 2, 1));
	R3 = _mm_shuffle_epi32(R3, _MM_SHUFFLE(1, 0, 3, 2));
	R2 = _mm_shuffle_epi32(R2, _MM_SHUFFLE(2, 1, 0, 3));

	// round 8
	// lm 8.1
	T0 = _mm_unpackhi_epi32(M1, M3);
	T1 = _mm_unpacklo_epi64(T0, M0);
	T2 = _mm_blend_epi16(T1, M2, 0xC0);
	B1 = _mm_shufflehi_epi16(T2, _MM_SHUFFLE(1, 0, 3, 2));
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B1), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -16);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -12);

	// lm 8.2
	T0 = _mm_unpackhi_epi32(M0, M3);
	T1 = _mm_blend_epi16(M2, T0, 0xF0);
	B2 = _mm_shuffle_epi32(T1, _MM_SHUFFLE(0, 2, 1, 3));
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B2), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -8);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -7);
	R4 = _mm_shuffle_epi32(R4, _MM_SHUFFLE(2, 1, 0, 3));
	R3 = _mm_shuffle_epi32(R3, _MM_SHUFFLE(1, 0, 3, 2));
	R2 = _mm_shuffle_epi32(R2, _MM_SHUFFLE(0, 3, 2, 1));

	// lm 8.3
	T0 = _mm_blend_epi16(M2, M0, 0x0C);
	T1 = _mm_slli_si128(T0, 4);
	B3 = _mm_blend_epi16(T1, M3, 0x0F);
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B3), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -16);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -12);

	// lm 8.4
	T0 = _mm_blend_epi16(M1, M0, 0x30);
	B4 = _mm_shuffle_epi32(T0, _MM_SHUFFLE(1, 0, 3, 2));
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B4), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -8);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -7);
	R4 = _mm_shuffle_epi32(R4, _MM_SHUFFLE(0, 3, 2, 1));
	R3 = _mm_shuffle_epi32(R3, _MM_SHUFFLE(1, 0, 3, 2));
	R2 = _mm_shuffle_epi32(R2, _MM_SHUFFLE(2, 1, 0, 3));

	// round 9
	// lm 9.1
	T0 = _mm_blend_epi16(M0, M2, 0x03);
	T1 = _mm_blend_epi16(M1, M2, 0x30);
	T2 = _mm_blend_epi16(T1, T0, 0x0F);
	B1 = _mm_shuffle_epi32(T2, _MM_SHUFFLE(1, 3, 0, 2));
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B1), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -16);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -12);

	// lm 9.2
	T0 = _mm_slli_si128(M0, 4);
	T1 = _mm_blend_epi16(M1, T0, 0xC0);
	B2 = _mm_shuffle_epi32(T1, _MM_SHUFFLE(1, 2, 0, 3));
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B2), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -8);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -7);
	R4 = _mm_shuffle_epi32(R4, _MM_SHUFFLE(2, 1, 0, 3));
	R3 = _mm_shuffle_epi32(R3, _MM_SHUFFLE(1, 0, 3, 2));
	R2 = _mm_shuffle_epi32(R2, _MM_SHUFFLE(0, 3, 2, 1));

	// lm 9.3
	T0 = _mm_unpackhi_epi32(M0, M3);
	T1 = _mm_unpacklo_epi32(M2, M3);
	T2 = _mm_unpackhi_epi64(T0, T1);
	B3 = _mm_shuffle_epi32(T2, _MM_SHUFFLE(3, 0, 2, 1));
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B3), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -16);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -12);

	// lm 9.4
	T0 = _mm_blend_epi16(M3, M2, 0xC0);
	T1 = _mm_unpacklo_epi32(M0, M3);
	T2 = _mm_blend_epi16(T0, T1, 0x0F);
	B4 = _mm_shuffle_epi32(T2, _MM_SHUFFLE(0, 1, 2, 3));
	R1 = _mm_add_epi32(_mm_add_epi32(R1, B4), R2);
	R4 = _mm_xor_si128(R4, R1);
	R4 = _mm_roti_epi32(R4, -8);
	R3 = _mm_add_epi32(R3, R4);
	R2 = _mm_xor_si128(R2, R3);
	R2 = _mm_roti_epi32(R2, -7);
	R4 = _mm_shuffle_epi32(R4, _MM_SHUFFLE(0, 3, 2, 1));
	R3 = _mm_shuffle_epi32(R3, _MM_SHUFFLE(1, 0, 3, 2));
	R2 = _mm_shuffle_epi32(R2, _MM_SHUFFLE(2, 1, 0, 3));

	_mm_storeu_si128(reinterpret_cast<__m128i*>(&State[0]), _mm_xor_si128(FF0, _mm_xor_si128(R1, R3)));
	_mm_storeu_si128(reinterpret_cast<__m128i*>(&State[4]), _mm_xor_si128(FF1, _mm_xor_si128(R2, R4)));

#else
	PermuteR10P512U(Input, InOffset, State, IV);
#endif
}

void Blake2::PermuteR10P4096H(const std::vector<byte> &Input, size_t InOffset, std::vector<UInt256> &State, const std::vector<UInt256> &IV)
{
	std::array<UInt256, 16> M;
	std::array<UInt256, 16> R{
		State[0],
		State[1],
		State[2],
		State[3],
		State[4],
		State[5],
		State[6],
		State[7],
		IV[0],
		IV[1],
		IV[2],
		IV[3],
		IV[4],
		IV[5],
		IV[6],
		IV[7] };
	size_t i;

#if defined(CEX_IS_LITTLE_ENDIAN)
	MemUtils::Copy(Input, InOffset, M, 0, M.size() * sizeof(UInt256));
#else
	for (i = 0; i < 16; ++i)
	{
		M[i].Load(
			IntUtils::LeBytesTo32(Input, InOffset + (i * 4)),
			IntUtils::LeBytesTo32(Input, InOffset + (i * 4) + 64),
			IntUtils::LeBytesTo32(Input, InOffset + (i * 4) + 128),
			IntUtils::LeBytesTo32(Input, InOffset + (i * 4) + 196),
			IntUtils::LeBytesTo32(Input, InOffset + (i * 4) + 256),
			IntUtils::LeBytesTo32(Input, InOffset + (i * 4) + 320),
			IntUtils::LeBytesTo32(Input, InOffset + (i * 4) + 384),
			IntUtils::LeBytesTo32(Input, InOffset + (i * 4) + 448));
	}
#endif

	for (i = 0; i < 10; ++i)
	{
		// round n
		R[0] += R[4] + M[Sigma256[(i * 16)]];
		R[12] ^= R[0];
		R[12] = ((R[12] >> 16) | (R[12] << (32 - 16)));
		R[8] += R[12];
		R[4] ^= R[8];
		R[4] = ((R[4] >> 12) | (R[4] << (32 - 12)));
		R[0] += R[4] + M[Sigma256[(i * 16) + 1]];
		R[12] ^= R[0];
		R[12] = ((R[12] >> 8) | (R[12] << (32 - 8)));
		R[8] += R[12];
		R[4] ^= R[8];
		R[4] = ((R[4] >> 7) | (R[4] << (32 - 7)));

		R[1] += R[5] + M[Sigma256[(i * 16) + 2]];
		R[13] ^= R[1];
		R[13] = ((R[13] >> 16) | (R[13] << (32 - 16)));
		R[9] += R[13];
		R[5] ^= R[9];
		R[5] = ((R[5] >> 12) | (R[5] << (32 - 12)));
		R[1] += R[5] + M[Sigma256[(i * 16) + 3]];
		R[13] ^= R[1];
		R[13] = ((R[13] >> 8) | (R[13] << (32 - 8)));
		R[9] += R[13];
		R[5] ^= R[9];
		R[5] = ((R[5] >> 7) | (R[5] << (32 - 7)));

		R[2] += R[6] + M[Sigma256[(i * 16) + 4]];
		R[14] ^= R[2];
		R[14] = ((R[14] >> 16) | (R[14] << (32 - 16)));
		R[10] += R[14];
		R[6] ^= R[10];
		R[6] = ((R[6] >> 12) | (R[6] << (32 - 12)));
		R[2] += R[6] + M[Sigma256[(i * 16) + 5]];
		R[14] ^= R[2];
		R[14] = ((R[14] >> 8) | (R[14] << (32 - 8)));
		R[10] += R[14];
		R[6] ^= R[10];
		R[6] = ((R[6] >> 7) | (R[6] << (32 - 7)));

		R[3] += R[7] + M[Sigma256[(i * 16) + 6]];
		R[15] ^= R[3];
		R[15] = ((R[15] >> 16) | (R[15] << (32 - 16)));
		R[11] += R[15];
		R[7] ^= R[11];
		R[7] = ((R[7] >> 12) | (R[7] << (32 - 12)));
		R[3] += R[7] + M[Sigma256[(i * 16) + 7]];
		R[15] ^= R[3];
		R[15] = ((R[15] >> 8) | (R[15] << (32 - 8)));
		R[11] += R[15];
		R[7] ^= R[11];
		R[7] = ((R[7] >> 7) | (R[7] << (32 - 7)));

		R[0] += R[5] + M[Sigma256[(i * 16) + 8]];
		R[15] ^= R[0];
		R[15] = ((R[15] >> 16) | (R[15] << (32 - 16)));
		R[10] += R[15];
		R[5] ^= R[10];
		R[5] = ((R[5] >> 12) | (R[5] << (32 - 12)));
		R[0] += R[5] + M[Sigma256[(i * 16) + 9]];
		R[15] ^= R[0];
		R[15] = ((R[15] >> 8) | (R[15] << (32 - 8)));
		R[10] += R[15];
		R[5] ^= R[10];
		R[5] = ((R[5] >> 7) | (R[5] << (32 - 7)));

		R[1] += R[6] + M[Sigma256[(i * 16) + 10]];
		R[12] ^= R[1];
		R[12] = ((R[12] >> 16) | (R[12] << (32 - 16)));
		R[11] += R[12];
		R[6] ^= R[11];
		R[6] = ((R[6] >> 12) | (R[6] << (32 - 12)));
		R[1] += R[6] + M[Sigma256[(i * 16) + 11]];
		R[12] ^= R[1];
		R[12] = ((R[12] >> 8) | (R[12] << (32 - 8)));
		R[11] += R[12];
		R[6] ^= R[11];
		R[6] = ((R[6] >> 7) | (R[6] << (32 - 7)));

		R[2] += R[7] + M[Sigma256[(i * 16) + 12]];
		R[13] ^= R[2];
		R[13] = ((R[13] >> 16) | (R[13] << (32 - 16)));
		R[8] += R[13];
		R[7] ^= R[8];
		R[7] = ((R[7] >> 12) | (R[7] << (32 - 12)));
		R[2] += R[7] + M[Sigma256[(i * 16) + 13]];
		R[13] ^= R[2];
		R[13] = ((R[13] >> 8) | (R[13] << (32 - 8)));
		R[8] += R[13];
		R[7] ^= R[8];
		R[7] = ((R[7] >> 7) | (R[7] << (32 - 7)));

		R[3] += R[4] + M[Sigma256[(i * 16) + 14]];
		R[14] ^= R[3];
		R[14] = ((R[14] >> 16) | (R[14] << (32 - 16)));
		R[9] += R[14];
		R[4] ^= R[9];
		R[4] = ((R[4] >> 12) | (R[4] << (32 - 12)));
		R[3] += R[4] + M[Sigma256[(i * 16) + 15]];
		R[14] ^= R[3];
		R[14] = ((R[14] >> 8) | (R[14] << (32 - 8)));
		R[9] += R[14];
		R[4] ^= R[9];
		R[4] = ((R[4] >> 7) | (R[4] << (32 - 7)));
	}

	State[0] ^= R[0] ^ R[8];
	State[1] ^= R[1] ^ R[9];
	State[2] ^= R[2] ^ R[10];
	State[3] ^= R[3] ^ R[11];
	State[4] ^= R[4] ^ R[12];
	State[5] ^= R[5] ^ R[13];
	State[6] ^= R[6] ^ R[14];
	State[7] ^= R[7] ^ R[15];
}

#endif

#if defined(__AVX512__)

void Blake2::PermuteR10P8192H(const std::vector<byte> &Input, size_t InOffset, std::vector<UInt512> &State, const std::vector<UInt512> &IV)
{
	std::array<UInt512, 16> M;
	std::array<UInt512, 16> R{
		State[0],
		State[1],
		State[2],
		State[3],
		State[4],
		State[5],
		State[6],
		State[7],
		IV[0],
		IV[1],
		IV[2],
		IV[3],
		IV[4],
		IV[5],
		IV[6],
		IV[7] };
	size_t i;

#if defined(CEX_IS_LITTLE_ENDIAN)
	MemUtils::Copy(Input, InOffset, M, 0, M.size() * sizeof(UInt512));
#else
	for (i = 0; i < 16; ++i)
	{
		M[i].Load(
			IntUtils::LeBytesTo32(Input, InOffset + (i * 4)),
			IntUtils::LeBytesTo32(Input, InOffset + (i * 4) + 64),
			IntUtils::LeBytesTo32(Input, InOffset + (i * 4) + 128),
			IntUtils::LeBytesTo32(Input, InOffset + (i * 4) + 196),
			IntUtils::LeBytesTo32(Input, InOffset + (i * 4) + 256),
			IntUtils::LeBytesTo32(Input, InOffset + (i * 4) + 320),
			IntUtils::LeBytesTo32(Input, InOffset + (i * 4) + 384),
			IntUtils::LeBytesTo32(Input, InOffset + (i * 4) + 448),
			IntUtils::LeBytesTo32(Input, InOffset + (i * 4) + 512),
			IntUtils::LeBytesTo32(Input, InOffset + (i * 4) + 576),
			IntUtils::LeBytesTo32(Input, InOffset + (i * 4) + 640),
			IntUtils::LeBytesTo32(Input, InOffset + (i * 4) + 704),
			IntUtils::LeBytesTo32(Input, InOffset + (i * 4) + 768),
			IntUtils::LeBytesTo32(Input, InOffset + (i * 4) + 832),
			IntUtils::LeBytesTo32(Input, InOffset + (i * 4) + 896),
			IntUtils::LeBytesTo32(Input, InOffset + (i * 4) + 960));
	}
#endif

	for (i = 0; i < 10; ++i)
	{
		// round n
		R[0] += R[4] + M[Sigma256[(i * 16)]];
		R[12] ^= R[0];
		R[12] = ((R[12] >> 16) | (R[12] << (32 - 16)));
		R[8] += R[12];
		R[4] ^= R[8];
		R[4] = ((R[4] >> 12) | (R[4] << (32 - 12)));
		R[0] += R[4] + M[Sigma256[(i * 16) + 1]];
		R[12] ^= R[0];
		R[12] = ((R[12] >> 8) | (R[12] << (32 - 8)));
		R[8] += R[12];
		R[4] ^= R[8];
		R[4] = ((R[4] >> 7) | (R[4] << (32 - 7)));

		R[1] += R[5] + M[Sigma256[(i * 16) + 2]];
		R[13] ^= R[1];
		R[13] = ((R[13] >> 16) | (R[13] << (32 - 16)));
		R[9] += R[13];
		R[5] ^= R[9];
		R[5] = ((R[5] >> 12) | (R[5] << (32 - 12)));
		R[1] += R[5] + M[Sigma256[(i * 16) + 3]];
		R[13] ^= R[1];
		R[13] = ((R[13] >> 8) | (R[13] << (32 - 8)));
		R[9] += R[13];
		R[5] ^= R[9];
		R[5] = ((R[5] >> 7) | (R[5] << (32 - 7)));

		R[2] += R[6] + M[Sigma256[(i * 16) + 4]];
		R[14] ^= R[2];
		R[14] = ((R[14] >> 16) | (R[14] << (32 - 16)));
		R[10] += R[14];
		R[6] ^= R[10];
		R[6] = ((R[6] >> 12) | (R[6] << (32 - 12)));
		R[2] += R[6] + M[Sigma256[(i * 16) + 5]];
		R[14] ^= R[2];
		R[14] = ((R[14] >> 8) | (R[14] << (32 - 8)));
		R[10] += R[14];
		R[6] ^= R[10];
		R[6] = ((R[6] >> 7) | (R[6] << (32 - 7)));

		R[3] += R[7] + M[Sigma256[(i * 16) + 6]];
		R[15] ^= R[3];
		R[15] = ((R[15] >> 16) | (R[15] << (32 - 16)));
		R[11] += R[15];
		R[7] ^= R[11];
		R[7] = ((R[7] >> 12) | (R[7] << (32 - 12)));
		R[3] += R[7] + M[Sigma256[(i * 16) + 7]];
		R[15] ^= R[3];
		R[15] = ((R[15] >> 8) | (R[15] << (32 - 8)));
		R[11] += R[15];
		R[7] ^= R[11];
		R[7] = ((R[7] >> 7) | (R[7] << (32 - 7)));

		R[0] += R[5] + M[Sigma256[(i * 16) + 8]];
		R[15] ^= R[0];
		R[15] = ((R[15] >> 16) | (R[15] << (32 - 16)));
		R[10] += R[15];
		R[5] ^= R[10];
		R[5] = ((R[5] >> 12) | (R[5] << (32 - 12)));
		R[0] += R[5] + M[Sigma256[(i * 16) + 9]];
		R[15] ^= R[0];
		R[15] = ((R[15] >> 8) | (R[15] << (32 - 8)));
		R[10] += R[15];
		R[5] ^= R[10];
		R[5] = ((R[5] >> 7) | (R[5] << (32 - 7)));

		R[1] += R[6] + M[Sigma256[(i * 16) + 10]];
		R[12] ^= R[1];
		R[12] = ((R[12] >> 16) | (R[12] << (32 - 16)));
		R[11] += R[12];
		R[6] ^= R[11];
		R[6] = ((R[6] >> 12) | (R[6] << (32 - 12)));
		R[1] += R[6] + M[Sigma256[(i * 16) + 11]];
		R[12] ^= R[1];
		R[12] = ((R[12] >> 8) | (R[12] << (32 - 8)));
		R[11] += R[12];
		R[6] ^= R[11];
		R[6] = ((R[6] >> 7) | (R[6] << (32 - 7)));

		R[2] += R[7] + M[Sigma256[(i * 16) + 12]];
		R[13] ^= R[2];
		R[13] = ((R[13] >> 16) | (R[13] << (32 - 16)));
		R[8] += R[13];
		R[7] ^= R[8];
		R[7] = ((R[7] >> 12) | (R[7] << (32 - 12)));
		R[2] += R[7] + M[Sigma256[(i * 16) + 13]];
		R[13] ^= R[2];
		R[13] = ((R[13] >> 8) | (R[13] << (32 - 8)));
		R[8] += R[13];
		R[7] ^= R[8];
		R[7] = ((R[7] >> 7) | (R[7] << (32 - 7)));

		R[3] += R[4] + M[Sigma256[(i * 16) + 14]];
		R[14] ^= R[3];
		R[14] = ((R[14] >> 16) | (R[14] << (32 - 16)));
		R[9] += R[14];
		R[4] ^= R[9];
		R[4] = ((R[4] >> 12) | (R[4] << (32 - 12)));
		R[3] += R[4] + M[Sigma256[(i * 16) + 15]];
		R[14] ^= R[3];
		R[14] = ((R[14] >> 8) | (R[14] << (32 - 8)));
		R[9] += R[14];
		R[4] ^= R[9];
		R[4] = ((R[4] >> 7) | (R[4] << (32 - 7)));
	}

	State[0] ^= R[0] ^ R[8];
	State[1] ^= R[1] ^ R[9];
	State[2] ^= R[2] ^ R[10];
	State[3] ^= R[3] ^ R[11];
	State[4] ^= R[4] ^ R[12];
	State[5] ^= R[5] ^ R[13];
	State[6] ^= R[6] ^ R[14];
	State[7] ^= R[7] ^ R[15];
}

#endif

void Blake2::PermuteR12P1024C(const std::vector<byte> &Input, size_t InOffset, std::array<ulong, 8> &State, const std::array<ulong, 8> &IV)
{
	std::array<ulong, 16> M;
	std::array<ulong, 16> R{
		State[0],
		State[1],
		State[2],
		State[3],
		State[4],
		State[5],
		State[6],
		State[7],
		IV[0],
		IV[1],
		IV[2],
		IV[3],
		IV[4],
		IV[5],
		IV[6],
		IV[7] };
	size_t i;

	IntUtils::LeBytesToULL1024(Input, InOffset, M, 0);

	for (i = 0; i < 12; ++i)
	{
		// round 0
		R[0] += R[4] + M[Sigma512[(i * 16)]];
		R[12] ^= R[0];
		R[12] = ((R[12] >> 32) | (R[12] << (64 - 32)));
		R[8] += R[12];
		R[4] ^= R[8];
		R[4] = ((R[4] >> 24) | (R[4] << (64 - 24)));
		R[0] += R[4] + M[Sigma512[(i * 16) + 1]];
		R[12] ^= R[0];
		R[12] = ((R[12] >> 16) | (R[12] << (64 - 16)));
		R[8] += R[12];
		R[4] ^= R[8];
		R[4] = ((R[4] >> 63) | (R[4] << (64 - 63)));

		R[1] += R[5] + M[Sigma512[(i * 16) + 2]];
		R[13] ^= R[1];
		R[13] = ((R[13] >> 32) | (R[13] << (64 - 32)));
		R[9] += R[13];
		R[5] ^= R[9];
		R[5] = ((R[5] >> 24) | (R[5] << (64 - 24)));
		R[1] += R[5] + M[Sigma512[(i * 16) + 3]];
		R[13] ^= R[1];
		R[13] = ((R[13] >> 16) | (R[13] << (64 - 16)));
		R[9] += R[13];
		R[5] ^= R[9];
		R[5] = ((R[5] >> 63) | (R[5] << (64 - 63)));

		R[2] += R[6] + M[Sigma512[(i * 16) + 4]];
		R[14] ^= R[2];
		R[14] = ((R[14] >> 32) | (R[14] << (64 - 32)));
		R[10] += R[14];
		R[6] ^= R[10];
		R[6] = ((R[6] >> 24) | (R[6] << (64 - 24)));
		R[2] += R[6] + M[Sigma512[(i * 16) + 5]];
		R[14] ^= R[2];
		R[14] = ((R[14] >> 16) | (R[14] << (64 - 16)));
		R[10] += R[14];
		R[6] ^= R[10];
		R[6] = ((R[6] >> 63) | (R[6] << (64 - 63)));

		R[3] += R[7] + M[Sigma512[(i * 16) + 6]];
		R[15] ^= R[3];
		R[15] = ((R[15] >> 32) | (R[15] << (64 - 32)));
		R[11] += R[15];
		R[7] ^= R[11];
		R[7] = ((R[7] >> 24) | (R[7] << (64 - 24)));
		R[3] += R[7] + M[Sigma512[(i * 16) + 7]];
		R[15] ^= R[3];
		R[15] = ((R[15] >> 16) | (R[15] << (64 - 16)));
		R[11] += R[15];
		R[7] ^= R[11];
		R[7] = ((R[7] >> 63) | (R[7] << (64 - 63)));

		R[0] += R[5] + M[Sigma512[(i * 16) + 8]];
		R[15] ^= R[0];
		R[15] = ((R[15] >> 32) | (R[15] << (64 - 32)));
		R[10] += R[15];
		R[5] ^= R[10];
		R[5] = ((R[5] >> 24) | (R[5] << (64 - 24)));
		R[0] += R[5] + M[Sigma512[(i * 16) + 9]];
		R[15] ^= R[0];
		R[15] = ((R[15] >> 16) | (R[15] << (64 - 16)));
		R[10] += R[15];
		R[5] ^= R[10];
		R[5] = ((R[5] >> 63) | (R[5] << (64 - 63)));

		R[1] += R[6] + M[Sigma512[(i * 16) + 10]];
		R[12] ^= R[1];
		R[12] = ((R[12] >> 32) | (R[12] << (64 - 32)));
		R[11] += R[12];
		R[6] ^= R[11];
		R[6] = ((R[6] >> 24) | (R[6] << (64 - 24)));
		R[1] += R[6] + M[Sigma512[(i * 16) + 11]];
		R[12] ^= R[1];
		R[12] = ((R[12] >> 16) | (R[12] << (64 - 16)));
		R[11] += R[12];
		R[6] ^= R[11];
		R[6] = ((R[6] >> 63) | (R[6] << (64 - 63)));

		R[2] += R[7] + M[Sigma512[(i * 16) + 12]];
		R[13] ^= R[2];
		R[13] = ((R[13] >> 32) | (R[13] << (64 - 32)));
		R[8] += R[13];
		R[7] ^= R[8];
		R[7] = ((R[7] >> 24) | (R[7] << (64 - 24)));
		R[2] += R[7] + M[Sigma512[(i * 16) + 13]];
		R[13] ^= R[2];
		R[13] = ((R[13] >> 16) | (R[13] << (64 - 16)));
		R[8] += R[13];
		R[7] ^= R[8];
		R[7] = ((R[7] >> 63) | (R[7] << (64 - 63)));

		R[3] += R[4] + M[Sigma512[(i * 16) + 14]];
		R[14] ^= R[3];
		R[14] = ((R[14] >> 32) | (R[14] << (64 - 32)));
		R[9] += R[14];
		R[4] ^= R[9];
		R[4] = ((R[4] >> 24) | (R[4] << (64 - 24)));
		R[3] += R[4] + M[Sigma512[(i * 16) + 15]];
		R[14] ^= R[3];
		R[14] = ((R[14] >> 16) | (R[14] << (64 - 16)));
		R[9] += R[14];
		R[4] ^= R[9];
		R[4] = ((R[4] >> 63) | (R[4] << (64 - 63)));
	}

	State[0] ^= R[0] ^ R[8];
	State[1] ^= R[1] ^ R[9];
	State[2] ^= R[2] ^ R[10];
	State[3] ^= R[3] ^ R[11];
	State[4] ^= R[4] ^ R[12];
	State[5] ^= R[5] ^ R[13];
	State[6] ^= R[6] ^ R[14];
	State[7] ^= R[7] ^ R[15];
}

void Blake2::PermuteR12P1024U(const std::vector<byte> &Input, size_t InOffset, std::array<ulong, 8> &State, const std::array<ulong, 8> &IV)
{
	ulong M0 = IntUtils::LeBytesTo64(Input, InOffset);
	ulong M1 = IntUtils::LeBytesTo64(Input, InOffset + 8);
	ulong M2 = IntUtils::LeBytesTo64(Input, InOffset + 16);
	ulong M3 = IntUtils::LeBytesTo64(Input, InOffset + 24);
	ulong M4 = IntUtils::LeBytesTo64(Input, InOffset + 32);
	ulong M5 = IntUtils::LeBytesTo64(Input, InOffset + 40);
	ulong M6 = IntUtils::LeBytesTo64(Input, InOffset + 48);
	ulong M7 = IntUtils::LeBytesTo64(Input, InOffset + 56);
	ulong M8 = IntUtils::LeBytesTo64(Input, InOffset + 64);
	ulong M9 = IntUtils::LeBytesTo64(Input, InOffset + 72);
	ulong M10 = IntUtils::LeBytesTo64(Input, InOffset + 80);
	ulong M11 = IntUtils::LeBytesTo64(Input, InOffset + 88);
	ulong M12 = IntUtils::LeBytesTo64(Input, InOffset + 96);
	ulong M13 = IntUtils::LeBytesTo64(Input, InOffset + 104);
	ulong M14 = IntUtils::LeBytesTo64(Input, InOffset + 112);
	ulong M15 = IntUtils::LeBytesTo64(Input, InOffset + 120);
	ulong R0 = State[0];
	ulong R1 = State[1];
	ulong R2 = State[2];
	ulong R3 = State[3];
	ulong R4 = State[4];
	ulong R5 = State[5];
	ulong R6 = State[6];
	ulong R7 = State[7];
	ulong R8 = IV[0];
	ulong R9 = IV[1];
	ulong R10 = IV[2];
	ulong R11 = IV[3];
	ulong R12 = IV[4];
	ulong R13 = IV[5];
	ulong R14 = IV[6];
	ulong R15 = IV[7];

	// round 0
	R0 += R4 + M0;
	R12 ^= R0;
	R12 = ((R12 >> 32) | (R12 << (64 - 32)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 24) | (R4 << (64 - 24)));
	R0 += R4 + M1;
	R12 ^= R0;
	R12 = ((R12 >> 16) | (R12 << (64 - 16)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 63) | (R4 << (64 - 63)));

	R1 += R5 + M2;
	R13 ^= R1;
	R13 = ((R13 >> 32) | (R13 << (64 - 32)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 24) | (R5 << (64 - 24)));
	R1 += R5 + M3;
	R13 ^= R1;
	R13 = ((R13 >> 16) | (R13 << (64 - 16)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 63) | (R5 << (64 - 63)));

	R2 += R6 + M4;
	R14 ^= R2;
	R14 = ((R14 >> 32) | (R14 << (64 - 32)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 24) | (R6 << (64 - 24)));
	R2 += R6 + M5;
	R14 ^= R2;
	R14 = ((R14 >> 16) | (R14 << (64 - 16)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 63) | (R6 << (64 - 63)));

	R3 += R7 + M6;
	R15 ^= R3;
	R15 = ((R15 >> 32) | (R15 << (64 - 32)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 24) | (R7 << (64 - 24)));
	R3 += R7 + M7;
	R15 ^= R3;
	R15 = ((R15 >> 16) | (R15 << (64 - 16)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 63) | (R7 << (64 - 63)));

	R0 += R5 + M8;
	R15 ^= R0;
	R15 = ((R15 >> 32) | (R15 << (64 - 32)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 24) | (R5 << (64 - 24)));
	R0 += R5 + M9;
	R15 ^= R0;
	R15 = ((R15 >> 16) | (R15 << (64 - 16)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 63) | (R5 << (64 - 63)));

	R1 += R6 + M10;
	R12 ^= R1;
	R12 = ((R12 >> 32) | (R12 << (64 - 32)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 24) | (R6 << (64 - 24)));
	R1 += R6 + M11;
	R12 ^= R1;
	R12 = ((R12 >> 16) | (R12 << (64 - 16)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 63) | (R6 << (64 - 63)));

	R2 += R7 + M12;
	R13 ^= R2;
	R13 = ((R13 >> 32) | (R13 << (64 - 32)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 24) | (R7 << (64 - 24)));
	R2 += R7 + M13;
	R13 ^= R2;
	R13 = ((R13 >> 16) | (R13 << (64 - 16)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 63) | (R7 << (64 - 63)));

	R3 += R4 + M14;
	R14 ^= R3;
	R14 = ((R14 >> 32) | (R14 << (64 - 32)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 24) | (R4 << (64 - 24)));
	R3 += R4 + M15;
	R14 ^= R3;
	R14 = ((R14 >> 16) | (R14 << (64 - 16)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 63) | (R4 << (64 - 63)));

	// round 1
	R0 += R4 + M14;
	R12 ^= R0;
	R12 = ((R12 >> 32) | (R12 << (64 - 32)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 24) | (R4 << (64 - 24)));
	R0 += R4 + M10;
	R12 ^= R0;
	R12 = ((R12 >> 16) | (R12 << (64 - 16)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 63) | (R4 << (64 - 63)));

	R1 += R5 + M4;
	R13 ^= R1;
	R13 = ((R13 >> 32) | (R13 << (64 - 32)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 24) | (R5 << (64 - 24)));
	R1 += R5 + M8;
	R13 ^= R1;
	R13 = ((R13 >> 16) | (R13 << (64 - 16)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 63) | (R5 << (64 - 63)));

	R2 += R6 + M9;
	R14 ^= R2;
	R14 = ((R14 >> 32) | (R14 << (64 - 32)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 24) | (R6 << (64 - 24)));
	R2 += R6 + M15;
	R14 ^= R2;
	R14 = ((R14 >> 16) | (R14 << (64 - 16)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 63) | (R6 << (64 - 63)));

	R3 += R7 + M13;
	R15 ^= R3;
	R15 = ((R15 >> 32) | (R15 << (64 - 32)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 24) | (R7 << (64 - 24)));
	R3 += R7 + M6;
	R15 ^= R3;
	R15 = ((R15 >> 16) | (R15 << (64 - 16)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 63) | (R7 << (64 - 63)));

	R0 += R5 + M1;
	R15 ^= R0;
	R15 = ((R15 >> 32) | (R15 << (64 - 32)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 24) | (R5 << (64 - 24)));
	R0 += R5 + M12;
	R15 ^= R0;
	R15 = ((R15 >> 16) | (R15 << (64 - 16)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 63) | (R5 << (64 - 63)));

	R1 += R6 + M0;
	R12 ^= R1;
	R12 = ((R12 >> 32) | (R12 << (64 - 32)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 24) | (R6 << (64 - 24)));
	R1 += R6 + M2;
	R12 ^= R1;
	R12 = ((R12 >> 16) | (R12 << (64 - 16)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 63) | (R6 << (64 - 63)));

	R2 += R7 + M11;
	R13 ^= R2;
	R13 = ((R13 >> 32) | (R13 << (64 - 32)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 24) | (R7 << (64 - 24)));
	R2 += R7 + M7;
	R13 ^= R2;
	R13 = ((R13 >> 16) | (R13 << (64 - 16)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 63) | (R7 << (64 - 63)));

	R3 += R4 + M5;
	R14 ^= R3;
	R14 = ((R14 >> 32) | (R14 << (64 - 32)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 24) | (R4 << (64 - 24)));
	R3 += R4 + M3;
	R14 ^= R3;
	R14 = ((R14 >> 16) | (R14 << (64 - 16)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 63) | (R4 << (64 - 63)));

	// round 2
	R0 += R4 + M11;
	R12 ^= R0;
	R12 = ((R12 >> 32) | (R12 << (64 - 32)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 24) | (R4 << (64 - 24)));
	R0 += R4 + M8;
	R12 ^= R0;
	R12 = ((R12 >> 16) | (R12 << (64 - 16)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 63) | (R4 << (64 - 63)));

	R1 += R5 + M12;
	R13 ^= R1;
	R13 = ((R13 >> 32) | (R13 << (64 - 32)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 24) | (R5 << (64 - 24)));
	R1 += R5 + M0;
	R13 ^= R1;
	R13 = ((R13 >> 16) | (R13 << (64 - 16)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 63) | (R5 << (64 - 63)));

	R2 += R6 + M5;
	R14 ^= R2;
	R14 = ((R14 >> 32) | (R14 << (64 - 32)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 24) | (R6 << (64 - 24)));
	R2 += R6 + M2;
	R14 ^= R2;
	R14 = ((R14 >> 16) | (R14 << (64 - 16)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 63) | (R6 << (64 - 63)));

	R3 += R7 + M15;
	R15 ^= R3;
	R15 = ((R15 >> 32) | (R15 << (64 - 32)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 24) | (R7 << (64 - 24)));
	R3 += R7 + M13;
	R15 ^= R3;
	R15 = ((R15 >> 16) | (R15 << (64 - 16)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 63) | (R7 << (64 - 63)));

	R0 += R5 + M10;
	R15 ^= R0;
	R15 = ((R15 >> 32) | (R15 << (64 - 32)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 24) | (R5 << (64 - 24)));
	R0 += R5 + M14;
	R15 ^= R0;
	R15 = ((R15 >> 16) | (R15 << (64 - 16)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 63) | (R5 << (64 - 63)));

	R1 += R6 + M3;
	R12 ^= R1;
	R12 = ((R12 >> 32) | (R12 << (64 - 32)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 24) | (R6 << (64 - 24)));
	R1 += R6 + M6;
	R12 ^= R1;
	R12 = ((R12 >> 16) | (R12 << (64 - 16)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 63) | (R6 << (64 - 63)));

	R2 += R7 + M7;
	R13 ^= R2;
	R13 = ((R13 >> 32) | (R13 << (64 - 32)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 24) | (R7 << (64 - 24)));
	R2 += R7 + M1;
	R13 ^= R2;
	R13 = ((R13 >> 16) | (R13 << (64 - 16)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 63) | (R7 << (64 - 63)));

	R3 += R4 + M9;
	R14 ^= R3;
	R14 = ((R14 >> 32) | (R14 << (64 - 32)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 24) | (R4 << (64 - 24)));
	R3 += R4 + M4;
	R14 ^= R3;
	R14 = ((R14 >> 16) | (R14 << (64 - 16)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 63) | (R4 << (64 - 63)));

	// round 3
	R0 += R4 + M7;
	R12 ^= R0;
	R12 = ((R12 >> 32) | (R12 << (64 - 32)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 24) | (R4 << (64 - 24)));
	R0 += R4 + M9;
	R12 ^= R0;
	R12 = ((R12 >> 16) | (R12 << (64 - 16)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 63) | (R4 << (64 - 63)));

	R1 += R5 + M3;
	R13 ^= R1;
	R13 = ((R13 >> 32) | (R13 << (64 - 32)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 24) | (R5 << (64 - 24)));
	R1 += R5 + M1;
	R13 ^= R1;
	R13 = ((R13 >> 16) | (R13 << (64 - 16)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 63) | (R5 << (64 - 63)));

	R2 += R6 + M13;
	R14 ^= R2;
	R14 = ((R14 >> 32) | (R14 << (64 - 32)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 24) | (R6 << (64 - 24)));
	R2 += R6 + M12;
	R14 ^= R2;
	R14 = ((R14 >> 16) | (R14 << (64 - 16)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 63) | (R6 << (64 - 63)));

	R3 += R7 + M11;
	R15 ^= R3;
	R15 = ((R15 >> 32) | (R15 << (64 - 32)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 24) | (R7 << (64 - 24)));
	R3 += R7 + M14;
	R15 ^= R3;
	R15 = ((R15 >> 16) | (R15 << (64 - 16)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 63) | (R7 << (64 - 63)));

	R0 += R5 + M2;
	R15 ^= R0;
	R15 = ((R15 >> 32) | (R15 << (64 - 32)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 24) | (R5 << (64 - 24)));
	R0 += R5 + M6;
	R15 ^= R0;
	R15 = ((R15 >> 16) | (R15 << (64 - 16)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 63) | (R5 << (64 - 63)));

	R1 += R6 + M5;
	R12 ^= R1;
	R12 = ((R12 >> 32) | (R12 << (64 - 32)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 24) | (R6 << (64 - 24)));
	R1 += R6 + M10;
	R12 ^= R1;
	R12 = ((R12 >> 16) | (R12 << (64 - 16)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 63) | (R6 << (64 - 63)));

	R2 += R7 + M4;
	R13 ^= R2;
	R13 = ((R13 >> 32) | (R13 << (64 - 32)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 24) | (R7 << (64 - 24)));
	R2 += R7 + M0;
	R13 ^= R2;
	R13 = ((R13 >> 16) | (R13 << (64 - 16)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 63) | (R7 << (64 - 63)));

	R3 += R4 + M15;
	R14 ^= R3;
	R14 = ((R14 >> 32) | (R14 << (64 - 32)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 24) | (R4 << (64 - 24)));
	R3 += R4 + M8;
	R14 ^= R3;
	R14 = ((R14 >> 16) | (R14 << (64 - 16)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 63) | (R4 << (64 - 63)));

	// round 4
	R0 += R4 + M9;
	R12 ^= R0;
	R12 = ((R12 >> 32) | (R12 << (64 - 32)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 24) | (R4 << (64 - 24)));
	R0 += R4 + M0;
	R12 ^= R0;
	R12 = ((R12 >> 16) | (R12 << (64 - 16)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 63) | (R4 << (64 - 63)));

	R1 += R5 + M5;
	R13 ^= R1;
	R13 = ((R13 >> 32) | (R13 << (64 - 32)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 24) | (R5 << (64 - 24)));
	R1 += R5 + M7;
	R13 ^= R1;
	R13 = ((R13 >> 16) | (R13 << (64 - 16)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 63) | (R5 << (64 - 63)));

	R2 += R6 + M2;
	R14 ^= R2;
	R14 = ((R14 >> 32) | (R14 << (64 - 32)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 24) | (R6 << (64 - 24)));
	R2 += R6 + M4;
	R14 ^= R2;
	R14 = ((R14 >> 16) | (R14 << (64 - 16)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 63) | (R6 << (64 - 63)));

	R3 += R7 + M10;
	R15 ^= R3;
	R15 = ((R15 >> 32) | (R15 << (64 - 32)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 24) | (R7 << (64 - 24)));
	R3 += R7 + M15;
	R15 ^= R3;
	R15 = ((R15 >> 16) | (R15 << (64 - 16)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 63) | (R7 << (64 - 63)));

	R0 += R5 + M14;
	R15 ^= R0;
	R15 = ((R15 >> 32) | (R15 << (64 - 32)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 24) | (R5 << (64 - 24)));
	R0 += R5 + M1;
	R15 ^= R0;
	R15 = ((R15 >> 16) | (R15 << (64 - 16)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 63) | (R5 << (64 - 63)));

	R1 += R6 + M11;
	R12 ^= R1;
	R12 = ((R12 >> 32) | (R12 << (64 - 32)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 24) | (R6 << (64 - 24)));
	R1 += R6 + M12;
	R12 ^= R1;
	R12 = ((R12 >> 16) | (R12 << (64 - 16)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 63) | (R6 << (64 - 63)));

	R2 += R7 + M6;
	R13 ^= R2;
	R13 = ((R13 >> 32) | (R13 << (64 - 32)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 24) | (R7 << (64 - 24)));
	R2 += R7 + M8;
	R13 ^= R2;
	R13 = ((R13 >> 16) | (R13 << (64 - 16)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 63) | (R7 << (64 - 63)));

	R3 += R4 + M3;
	R14 ^= R3;
	R14 = ((R14 >> 32) | (R14 << (64 - 32)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 24) | (R4 << (64 - 24)));
	R3 += R4 + M13;
	R14 ^= R3;
	R14 = ((R14 >> 16) | (R14 << (64 - 16)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 63) | (R4 << (64 - 63)));

	// round 5
	R0 += R4 + M2;
	R12 ^= R0;
	R12 = ((R12 >> 32) | (R12 << (64 - 32)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 24) | (R4 << (64 - 24)));
	R0 += R4 + M12;
	R12 ^= R0;
	R12 = ((R12 >> 16) | (R12 << (64 - 16)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 63) | (R4 << (64 - 63)));

	R1 += R5 + M6;
	R13 ^= R1;
	R13 = ((R13 >> 32) | (R13 << (64 - 32)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 24) | (R5 << (64 - 24)));
	R1 += R5 + M10;
	R13 ^= R1;
	R13 = ((R13 >> 16) | (R13 << (64 - 16)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 63) | (R5 << (64 - 63)));

	R2 += R6 + M0;
	R14 ^= R2;
	R14 = ((R14 >> 32) | (R14 << (64 - 32)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 24) | (R6 << (64 - 24)));
	R2 += R6 + M11;
	R14 ^= R2;
	R14 = ((R14 >> 16) | (R14 << (64 - 16)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 63) | (R6 << (64 - 63)));

	R3 += R7 + M8;
	R15 ^= R3;
	R15 = ((R15 >> 32) | (R15 << (64 - 32)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 24) | (R7 << (64 - 24)));
	R3 += R7 + M3;
	R15 ^= R3;
	R15 = ((R15 >> 16) | (R15 << (64 - 16)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 63) | (R7 << (64 - 63)));

	R0 += R5 + M4;
	R15 ^= R0;
	R15 = ((R15 >> 32) | (R15 << (64 - 32)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 24) | (R5 << (64 - 24)));
	R0 += R5 + M13;
	R15 ^= R0;
	R15 = ((R15 >> 16) | (R15 << (64 - 16)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 63) | (R5 << (64 - 63)));

	R1 += R6 + M7;
	R12 ^= R1;
	R12 = ((R12 >> 32) | (R12 << (64 - 32)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 24) | (R6 << (64 - 24)));
	R1 += R6 + M5;
	R12 ^= R1;
	R12 = ((R12 >> 16) | (R12 << (64 - 16)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 63) | (R6 << (64 - 63)));

	R2 += R7 + M15;
	R13 ^= R2;
	R13 = ((R13 >> 32) | (R13 << (64 - 32)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 24) | (R7 << (64 - 24)));
	R2 += R7 + M14;
	R13 ^= R2;
	R13 = ((R13 >> 16) | (R13 << (64 - 16)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 63) | (R7 << (64 - 63)));

	R3 += R4 + M1;
	R14 ^= R3;
	R14 = ((R14 >> 32) | (R14 << (64 - 32)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 24) | (R4 << (64 - 24)));
	R3 += R4 + M9;
	R14 ^= R3;
	R14 = ((R14 >> 16) | (R14 << (64 - 16)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 63) | (R4 << (64 - 63)));

	// round 6
	R0 += R4 + M12;
	R12 ^= R0;
	R12 = ((R12 >> 32) | (R12 << (64 - 32)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 24) | (R4 << (64 - 24)));
	R0 += R4 + M5;
	R12 ^= R0;
	R12 = ((R12 >> 16) | (R12 << (64 - 16)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 63) | (R4 << (64 - 63)));

	R1 += R5 + M1;
	R13 ^= R1;
	R13 = ((R13 >> 32) | (R13 << (64 - 32)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 24) | (R5 << (64 - 24)));
	R1 += R5 + M15;
	R13 ^= R1;
	R13 = ((R13 >> 16) | (R13 << (64 - 16)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 63) | (R5 << (64 - 63)));

	R2 += R6 + M14;
	R14 ^= R2;
	R14 = ((R14 >> 32) | (R14 << (64 - 32)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 24) | (R6 << (64 - 24)));
	R2 += R6 + M13;
	R14 ^= R2;
	R14 = ((R14 >> 16) | (R14 << (64 - 16)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 63) | (R6 << (64 - 63)));

	R3 += R7 + M4;
	R15 ^= R3;
	R15 = ((R15 >> 32) | (R15 << (64 - 32)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 24) | (R7 << (64 - 24)));
	R3 += R7 + M10;
	R15 ^= R3;
	R15 = ((R15 >> 16) | (R15 << (64 - 16)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 63) | (R7 << (64 - 63)));

	R0 += R5 + M0;
	R15 ^= R0;
	R15 = ((R15 >> 32) | (R15 << (64 - 32)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 24) | (R5 << (64 - 24)));
	R0 += R5 + M7;
	R15 ^= R0;
	R15 = ((R15 >> 16) | (R15 << (64 - 16)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 63) | (R5 << (64 - 63)));

	R1 += R6 + M6;
	R12 ^= R1;
	R12 = ((R12 >> 32) | (R12 << (64 - 32)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 24) | (R6 << (64 - 24)));
	R1 += R6 + M3;
	R12 ^= R1;
	R12 = ((R12 >> 16) | (R12 << (64 - 16)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 63) | (R6 << (64 - 63)));

	R2 += R7 + M9;
	R13 ^= R2;
	R13 = ((R13 >> 32) | (R13 << (64 - 32)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 24) | (R7 << (64 - 24)));
	R2 += R7 + M2;
	R13 ^= R2;
	R13 = ((R13 >> 16) | (R13 << (64 - 16)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 63) | (R7 << (64 - 63)));

	R3 += R4 + M8;
	R14 ^= R3;
	R14 = ((R14 >> 32) | (R14 << (64 - 32)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 24) | (R4 << (64 - 24)));
	R3 += R4 + M11;
	R14 ^= R3;
	R14 = ((R14 >> 16) | (R14 << (64 - 16)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 63) | (R4 << (64 - 63)));

	// round 7
	R0 += R4 + M13;
	R12 ^= R0;
	R12 = ((R12 >> 32) | (R12 << (64 - 32)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 24) | (R4 << (64 - 24)));
	R0 += R4 + M11;
	R12 ^= R0;
	R12 = ((R12 >> 16) | (R12 << (64 - 16)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 63) | (R4 << (64 - 63)));

	R1 += R5 + M7;
	R13 ^= R1;
	R13 = ((R13 >> 32) | (R13 << (64 - 32)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 24) | (R5 << (64 - 24)));
	R1 += R5 + M14;
	R13 ^= R1;
	R13 = ((R13 >> 16) | (R13 << (64 - 16)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 63) | (R5 << (64 - 63)));

	R2 += R6 + M12;
	R14 ^= R2;
	R14 = ((R14 >> 32) | (R14 << (64 - 32)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 24) | (R6 << (64 - 24)));
	R2 += R6 + M1;
	R14 ^= R2;
	R14 = ((R14 >> 16) | (R14 << (64 - 16)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 63) | (R6 << (64 - 63)));

	R3 += R7 + M3;
	R15 ^= R3;
	R15 = ((R15 >> 32) | (R15 << (64 - 32)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 24) | (R7 << (64 - 24)));
	R3 += R7 + M9;
	R15 ^= R3;
	R15 = ((R15 >> 16) | (R15 << (64 - 16)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 63) | (R7 << (64 - 63)));

	R0 += R5 + M5;
	R15 ^= R0;
	R15 = ((R15 >> 32) | (R15 << (64 - 32)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 24) | (R5 << (64 - 24)));
	R0 += R5 + M0;
	R15 ^= R0;
	R15 = ((R15 >> 16) | (R15 << (64 - 16)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 63) | (R5 << (64 - 63)));

	R1 += R6 + M15;
	R12 ^= R1;
	R12 = ((R12 >> 32) | (R12 << (64 - 32)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 24) | (R6 << (64 - 24)));
	R1 += R6 + M4;
	R12 ^= R1;
	R12 = ((R12 >> 16) | (R12 << (64 - 16)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 63) | (R6 << (64 - 63)));

	R2 += R7 + M8;
	R13 ^= R2;
	R13 = ((R13 >> 32) | (R13 << (64 - 32)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 24) | (R7 << (64 - 24)));
	R2 += R7 + M6;
	R13 ^= R2;
	R13 = ((R13 >> 16) | (R13 << (64 - 16)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 63) | (R7 << (64 - 63)));

	R3 += R4 + M2;
	R14 ^= R3;
	R14 = ((R14 >> 32) | (R14 << (64 - 32)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 24) | (R4 << (64 - 24)));
	R3 += R4 + M10;
	R14 ^= R3;
	R14 = ((R14 >> 16) | (R14 << (64 - 16)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 63) | (R4 << (64 - 63)));

	// round 8
	R0 += R4 + M6;
	R12 ^= R0;
	R12 = ((R12 >> 32) | (R12 << (64 - 32)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 24) | (R4 << (64 - 24)));
	R0 += R4 + M15;
	R12 ^= R0;
	R12 = ((R12 >> 16) | (R12 << (64 - 16)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 63) | (R4 << (64 - 63)));

	R1 += R5 + M14;
	R13 ^= R1;
	R13 = ((R13 >> 32) | (R13 << (64 - 32)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 24) | (R5 << (64 - 24)));
	R1 += R5 + M9;
	R13 ^= R1;
	R13 = ((R13 >> 16) | (R13 << (64 - 16)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 63) | (R5 << (64 - 63)));

	R2 += R6 + M11;
	R14 ^= R2;
	R14 = ((R14 >> 32) | (R14 << (64 - 32)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 24) | (R6 << (64 - 24)));
	R2 += R6 + M3;
	R14 ^= R2;
	R14 = ((R14 >> 16) | (R14 << (64 - 16)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 63) | (R6 << (64 - 63)));

	R3 += R7 + M0;
	R15 ^= R3;
	R15 = ((R15 >> 32) | (R15 << (64 - 32)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 24) | (R7 << (64 - 24)));
	R3 += R7 + M8;
	R15 ^= R3;
	R15 = ((R15 >> 16) | (R15 << (64 - 16)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 63) | (R7 << (64 - 63)));

	R0 += R5 + M12;
	R15 ^= R0;
	R15 = ((R15 >> 32) | (R15 << (64 - 32)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 24) | (R5 << (64 - 24)));
	R0 += R5 + M2;
	R15 ^= R0;
	R15 = ((R15 >> 16) | (R15 << (64 - 16)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 63) | (R5 << (64 - 63)));

	R1 += R6 + M13;
	R12 ^= R1;
	R12 = ((R12 >> 32) | (R12 << (64 - 32)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 24) | (R6 << (64 - 24)));
	R1 += R6 + M7;
	R12 ^= R1;
	R12 = ((R12 >> 16) | (R12 << (64 - 16)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 63) | (R6 << (64 - 63)));

	R2 += R7 + M1;
	R13 ^= R2;
	R13 = ((R13 >> 32) | (R13 << (64 - 32)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 24) | (R7 << (64 - 24)));
	R2 += R7 + M4;
	R13 ^= R2;
	R13 = ((R13 >> 16) | (R13 << (64 - 16)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 63) | (R7 << (64 - 63)));

	R3 += R4 + M10;
	R14 ^= R3;
	R14 = ((R14 >> 32) | (R14 << (64 - 32)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 24) | (R4 << (64 - 24)));
	R3 += R4 + M5;
	R14 ^= R3;
	R14 = ((R14 >> 16) | (R14 << (64 - 16)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 63) | (R4 << (64 - 63)));

	// round 9
	R0 += R4 + M10;
	R12 ^= R0;
	R12 = ((R12 >> 32) | (R12 << (64 - 32)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 24) | (R4 << (64 - 24)));
	R0 += R4 + M2;
	R12 ^= R0;
	R12 = ((R12 >> 16) | (R12 << (64 - 16)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 63) | (R4 << (64 - 63)));

	R1 += R5 + M8;
	R13 ^= R1;
	R13 = ((R13 >> 32) | (R13 << (64 - 32)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 24) | (R5 << (64 - 24)));
	R1 += R5 + M4;
	R13 ^= R1;
	R13 = ((R13 >> 16) | (R13 << (64 - 16)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 63) | (R5 << (64 - 63)));

	R2 += R6 + M7;
	R14 ^= R2;
	R14 = ((R14 >> 32) | (R14 << (64 - 32)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 24) | (R6 << (64 - 24)));
	R2 += R6 + M6;
	R14 ^= R2;
	R14 = ((R14 >> 16) | (R14 << (64 - 16)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 63) | (R6 << (64 - 63)));

	R3 += R7 + M1;
	R15 ^= R3;
	R15 = ((R15 >> 32) | (R15 << (64 - 32)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 24) | (R7 << (64 - 24)));
	R3 += R7 + M5;
	R15 ^= R3;
	R15 = ((R15 >> 16) | (R15 << (64 - 16)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 63) | (R7 << (64 - 63)));

	R0 += R5 + M15;
	R15 ^= R0;
	R15 = ((R15 >> 32) | (R15 << (64 - 32)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 24) | (R5 << (64 - 24)));
	R0 += R5 + M11;
	R15 ^= R0;
	R15 = ((R15 >> 16) | (R15 << (64 - 16)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 63) | (R5 << (64 - 63)));

	R1 += R6 + M9;
	R12 ^= R1;
	R12 = ((R12 >> 32) | (R12 << (64 - 32)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 24) | (R6 << (64 - 24)));
	R1 += R6 + M14;
	R12 ^= R1;
	R12 = ((R12 >> 16) | (R12 << (64 - 16)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 63) | (R6 << (64 - 63)));

	R2 += R7 + M3;
	R13 ^= R2;
	R13 = ((R13 >> 32) | (R13 << (64 - 32)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 24) | (R7 << (64 - 24)));
	R2 += R7 + M12;
	R13 ^= R2;
	R13 = ((R13 >> 16) | (R13 << (64 - 16)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 63) | (R7 << (64 - 63)));

	R3 += R4 + M13;
	R14 ^= R3;
	R14 = ((R14 >> 32) | (R14 << (64 - 32)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 24) | (R4 << (64 - 24)));
	R3 += R4 + M0;
	R14 ^= R3;
	R14 = ((R14 >> 16) | (R14 << (64 - 16)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 63) | (R4 << (64 - 63)));

	// round 10
	R0 += R4 + M0;
	R12 ^= R0;
	R12 = ((R12 >> 32) | (R12 << (64 - 32)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 24) | (R4 << (64 - 24)));
	R0 += R4 + M1;
	R12 ^= R0;
	R12 = ((R12 >> 16) | (R12 << (64 - 16)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 63) | (R4 << (64 - 63)));

	R1 += R5 + M2;
	R13 ^= R1;
	R13 = ((R13 >> 32) | (R13 << (64 - 32)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 24) | (R5 << (64 - 24)));
	R1 += R5 + M3;
	R13 ^= R1;
	R13 = ((R13 >> 16) | (R13 << (64 - 16)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 63) | (R5 << (64 - 63)));

	R2 += R6 + M4;
	R14 ^= R2;
	R14 = ((R14 >> 32) | (R14 << (64 - 32)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 24) | (R6 << (64 - 24)));
	R2 += R6 + M5;
	R14 ^= R2;
	R14 = ((R14 >> 16) | (R14 << (64 - 16)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 63) | (R6 << (64 - 63)));

	R3 += R7 + M6;
	R15 ^= R3;
	R15 = ((R15 >> 32) | (R15 << (64 - 32)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 24) | (R7 << (64 - 24)));
	R3 += R7 + M7;
	R15 ^= R3;
	R15 = ((R15 >> 16) | (R15 << (64 - 16)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 63) | (R7 << (64 - 63)));

	R0 += R5 + M8;
	R15 ^= R0;
	R15 = ((R15 >> 32) | (R15 << (64 - 32)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 24) | (R5 << (64 - 24)));
	R0 += R5 + M9;
	R15 ^= R0;
	R15 = ((R15 >> 16) | (R15 << (64 - 16)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 63) | (R5 << (64 - 63)));

	R1 += R6 + M10;
	R12 ^= R1;
	R12 = ((R12 >> 32) | (R12 << (64 - 32)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 24) | (R6 << (64 - 24)));
	R1 += R6 + M11;
	R12 ^= R1;
	R12 = ((R12 >> 16) | (R12 << (64 - 16)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 63) | (R6 << (64 - 63)));

	R2 += R7 + M12;
	R13 ^= R2;
	R13 = ((R13 >> 32) | (R13 << (64 - 32)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 24) | (R7 << (64 - 24)));
	R2 += R7 + M13;
	R13 ^= R2;
	R13 = ((R13 >> 16) | (R13 << (64 - 16)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 63) | (R7 << (64 - 63)));

	R3 += R4 + M14;
	R14 ^= R3;
	R14 = ((R14 >> 32) | (R14 << (64 - 32)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 24) | (R4 << (64 - 24)));
	R3 += R4 + M15;
	R14 ^= R3;
	R14 = ((R14 >> 16) | (R14 << (64 - 16)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 63) | (R4 << (64 - 63)));

	// round 11
	R0 += R4 + M14;
	R12 ^= R0;
	R12 = ((R12 >> 32) | (R12 << (64 - 32)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 24) | (R4 << (64 - 24)));
	R0 += R4 + M10;
	R12 ^= R0;
	R12 = ((R12 >> 16) | (R12 << (64 - 16)));
	R8 += R12;
	R4 ^= R8;
	R4 = ((R4 >> 63) | (R4 << (64 - 63)));

	R1 += R5 + M4;
	R13 ^= R1;
	R13 = ((R13 >> 32) | (R13 << (64 - 32)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 24) | (R5 << (64 - 24)));
	R1 += R5 + M8;
	R13 ^= R1;
	R13 = ((R13 >> 16) | (R13 << (64 - 16)));
	R9 += R13;
	R5 ^= R9;
	R5 = ((R5 >> 63) | (R5 << (64 - 63)));

	R2 += R6 + M9;
	R14 ^= R2;
	R14 = ((R14 >> 32) | (R14 << (64 - 32)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 24) | (R6 << (64 - 24)));
	R2 += R6 + M15;
	R14 ^= R2;
	R14 = ((R14 >> 16) | (R14 << (64 - 16)));
	R10 += R14;
	R6 ^= R10;
	R6 = ((R6 >> 63) | (R6 << (64 - 63)));

	R3 += R7 + M13;
	R15 ^= R3;
	R15 = ((R15 >> 32) | (R15 << (64 - 32)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 24) | (R7 << (64 - 24)));
	R3 += R7 + M6;
	R15 ^= R3;
	R15 = ((R15 >> 16) | (R15 << (64 - 16)));
	R11 += R15;
	R7 ^= R11;
	R7 = ((R7 >> 63) | (R7 << (64 - 63)));

	R0 += R5 + M1;
	R15 ^= R0;
	R15 = ((R15 >> 32) | (R15 << (64 - 32)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 24) | (R5 << (64 - 24)));
	R0 += R5 + M12;
	R15 ^= R0;
	R15 = ((R15 >> 16) | (R15 << (64 - 16)));
	R10 += R15;
	R5 ^= R10;
	R5 = ((R5 >> 63) | (R5 << (64 - 63)));

	R1 += R6 + M0;
	R12 ^= R1;
	R12 = ((R12 >> 32) | (R12 << (64 - 32)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 24) | (R6 << (64 - 24)));
	R1 += R6 + M2;
	R12 ^= R1;
	R12 = ((R12 >> 16) | (R12 << (64 - 16)));
	R11 += R12;
	R6 ^= R11;
	R6 = ((R6 >> 63) | (R6 << (64 - 63)));

	R2 += R7 + M11;
	R13 ^= R2;
	R13 = ((R13 >> 32) | (R13 << (64 - 32)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 24) | (R7 << (64 - 24)));
	R2 += R7 + M7;
	R13 ^= R2;
	R13 = ((R13 >> 16) | (R13 << (64 - 16)));
	R8 += R13;
	R7 ^= R8;
	R7 = ((R7 >> 63) | (R7 << (64 - 63)));

	R3 += R4 + M5;
	R14 ^= R3;
	R14 = ((R14 >> 32) | (R14 << (64 - 32)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 24) | (R4 << (64 - 24)));
	R3 += R4 + M3;
	R14 ^= R3;
	R14 = ((R14 >> 16) | (R14 << (64 - 16)));
	R9 += R14;
	R4 ^= R9;
	R4 = ((R4 >> 63) | (R4 << (64 - 63)));

	State[0] ^= R0 ^ R8;
	State[1] ^= R1 ^ R9;
	State[2] ^= R2 ^ R10;
	State[3] ^= R3 ^ R11;
	State[4] ^= R4 ^ R12;
	State[5] ^= R5 ^ R13;
	State[6] ^= R6 ^ R14;
	State[7] ^= R7 ^ R15;
}


void Blake2::PermuteR12P1024V(const std::vector<byte> &Input, size_t InOffset, std::array<ulong, 8> &State, const std::array<ulong, 8> &IV)
{
#if defined(__AVX__)

	const __m128i M0 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&Input[InOffset]));
	const __m128i M1 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&Input[InOffset + 16]));
	const __m128i M2 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&Input[InOffset + 32]));
	const __m128i M3 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&Input[InOffset + 48]));
	const __m128i M4 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&Input[InOffset + 64]));
	const __m128i M5 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&Input[InOffset + 80]));
	const __m128i M6 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&Input[InOffset + 96]));
	const __m128i M7 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&Input[InOffset + 112]));
	const __m128i R16 = _mm_setr_epi8(2, 3, 4, 5, 6, 7, 0, 1, 10, 11, 12, 13, 14, 15, 8, 9);
	const __m128i R24 = _mm_setr_epi8(3, 4, 5, 6, 7, 0, 1, 2, 11, 12, 13, 14, 15, 8, 9, 10);
	__m128i RL1 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&State[0]));
	__m128i RH1 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&State[2]));
	__m128i RL2 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&State[4]));
	__m128i RH2 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&State[6]));
	__m128i RL3 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&IV[0]));
	__m128i RH3 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&IV[2]));
	__m128i RL4 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&IV[4]));
	__m128i RH4 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&IV[6]));
	__m128i B0, B1;

	// round 0
	// lm 0.1
	B0 = _mm_unpacklo_epi64(M0, M1);
	B1 = _mm_unpacklo_epi64(M2, M3);
	// g1
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -32);
	RH4 = _mm_roti_epi64(RH4, -32);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -24);
	RH2 = _mm_roti_epi64(RH2, -24);

	// lm 0.2
	B0 = _mm_unpackhi_epi64(M0, M1);
	B1 = _mm_unpackhi_epi64(M2, M3);
	// g2
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -16);
	RH4 = _mm_roti_epi64(RH4, -16);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -63);
	RH2 = _mm_roti_epi64(RH2, -63);
	// diag
	Diagonalize(RL1, RL2, RL3, RL4, RH1, RH2, RH3, RH4);

	// lm 0.3
	B0 = _mm_unpacklo_epi64(M4, M5);
	B1 = _mm_unpacklo_epi64(M6, M7);
	// g1
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -32);
	RH4 = _mm_roti_epi64(RH4, -32);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -24);
	RH2 = _mm_roti_epi64(RH2, -24);

	// lm 0.4
	B0 = _mm_unpackhi_epi64(M4, M5);
	B1 = _mm_unpackhi_epi64(M6, M7);
	// g2
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -16);
	RH4 = _mm_roti_epi64(RH4, -16);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -63);
	RH2 = _mm_roti_epi64(RH2, -63);
	// undiag
	UnDiagonalize(RL1, RL2, RL3, RL4, RH1, RH2, RH3, RH4);

	// round 2
	B0 = _mm_unpacklo_epi64(M7, M2);
	B1 = _mm_unpackhi_epi64(M4, M6);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -32);
	RH4 = _mm_roti_epi64(RH4, -32);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -24);
	RH2 = _mm_roti_epi64(RH2, -24);

	B0 = _mm_unpacklo_epi64(M5, M4);
	B1 = _mm_alignr_epi8(M3, M7, 8);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -16);
	RH4 = _mm_roti_epi64(RH4, -16);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -63);
	RH2 = _mm_roti_epi64(RH2, -63);

	Diagonalize(RL1, RL2, RL3, RL4, RH1, RH2, RH3, RH4);

	B0 = _mm_shuffle_epi32(M0, _MM_SHUFFLE(1, 0, 3, 2));
	B1 = _mm_unpackhi_epi64(M5, M2);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -32);
	RH4 = _mm_roti_epi64(RH4, -32);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -24);
	RH2 = _mm_roti_epi64(RH2, -24);

	B0 = _mm_unpacklo_epi64(M6, M1);
	B1 = _mm_unpackhi_epi64(M3, M1);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -16);
	RH4 = _mm_roti_epi64(RH4, -16);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -63);
	RH2 = _mm_roti_epi64(RH2, -63);

	UnDiagonalize(RL1, RL2, RL3, RL4, RH1, RH2, RH3, RH4);

	// round 3
	B0 = _mm_alignr_epi8(M6, M5, 8);
	B1 = _mm_unpackhi_epi64(M2, M7);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -32);
	RH4 = _mm_roti_epi64(RH4, -32);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -24);
	RH2 = _mm_roti_epi64(RH2, -24);

	B0 = _mm_unpacklo_epi64(M4, M0);
	B1 = _mm_blend_epi16(M1, M6, 0xF0);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -16);
	RH4 = _mm_roti_epi64(RH4, -16);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -63);
	RH2 = _mm_roti_epi64(RH2, -63);

	Diagonalize(RL1, RL2, RL3, RL4, RH1, RH2, RH3, RH4);

	B0 = _mm_blend_epi16(M5, M1, 0xF0);
	B1 = _mm_unpackhi_epi64(M3, M4);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -32);
	RH4 = _mm_roti_epi64(RH4, -32);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -24);
	RH2 = _mm_roti_epi64(RH2, -24);

	B0 = _mm_unpacklo_epi64(M7, M3);
	B1 = _mm_alignr_epi8(M2, M0, 8);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -16);
	RH4 = _mm_roti_epi64(RH4, -16);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -63);
	RH2 = _mm_roti_epi64(RH2, -63);

	UnDiagonalize(RL1, RL2, RL3, RL4, RH1, RH2, RH3, RH4);

	// round 4
	B0 = _mm_unpackhi_epi64(M3, M1);
	B1 = _mm_unpackhi_epi64(M6, M5);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -32);
	RH4 = _mm_roti_epi64(RH4, -32);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -24);
	RH2 = _mm_roti_epi64(RH2, -24);

	B0 = _mm_unpackhi_epi64(M4, M0);
	B1 = _mm_unpacklo_epi64(M6, M7);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -16);
	RH4 = _mm_roti_epi64(RH4, -16);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -63);
	RH2 = _mm_roti_epi64(RH2, -63);

	Diagonalize(RL1, RL2, RL3, RL4, RH1, RH2, RH3, RH4);

	B0 = _mm_blend_epi16(M1, M2, 0xF0);
	B1 = _mm_blend_epi16(M2, M7, 0xF0);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -32);
	RH4 = _mm_roti_epi64(RH4, -32);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -24);
	RH2 = _mm_roti_epi64(RH2, -24);

	B0 = _mm_unpacklo_epi64(M3, M5);
	B1 = _mm_unpacklo_epi64(M0, M4);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -16);
	RH4 = _mm_roti_epi64(RH4, -16);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -63);
	RH2 = _mm_roti_epi64(RH2, -63);

	UnDiagonalize(RL1, RL2, RL3, RL4, RH1, RH2, RH3, RH4);

	// round 5
	B0 = _mm_unpackhi_epi64(M4, M2);
	B1 = _mm_unpacklo_epi64(M1, M5);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -32);
	RH4 = _mm_roti_epi64(RH4, -32);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -24);
	RH2 = _mm_roti_epi64(RH2, -24);

	B0 = _mm_blend_epi16(M0, M3, 0xF0);
	B1 = _mm_blend_epi16(M2, M7, 0xF0);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -16);
	RH4 = _mm_roti_epi64(RH4, -16);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -63);
	RH2 = _mm_roti_epi64(RH2, -63);

	Diagonalize(RL1, RL2, RL3, RL4, RH1, RH2, RH3, RH4);

	B0 = _mm_blend_epi16(M7, M5, 0xF0);
	B1 = _mm_blend_epi16(M3, M1, 0xF0);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -32);
	RH4 = _mm_roti_epi64(RH4, -32);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -24);
	RH2 = _mm_roti_epi64(RH2, -24);

	B0 = _mm_alignr_epi8(M6, M0, 8);
	B1 = _mm_blend_epi16(M4, M6, 0xF0);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -16);
	RH4 = _mm_roti_epi64(RH4, -16);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -63);
	RH2 = _mm_roti_epi64(RH2, -63);

	UnDiagonalize(RL1, RL2, RL3, RL4, RH1, RH2, RH3, RH4);

	// round 6
	B0 = _mm_unpacklo_epi64(M1, M3);
	B1 = _mm_unpacklo_epi64(M0, M4);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -32);
	RH4 = _mm_roti_epi64(RH4, -32);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -24);
	RH2 = _mm_roti_epi64(RH2, -24);

	B0 = _mm_unpacklo_epi64(M6, M5);
	B1 = _mm_unpackhi_epi64(M5, M1);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -16);
	RH4 = _mm_roti_epi64(RH4, -16);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -63);
	RH2 = _mm_roti_epi64(RH2, -63);

	Diagonalize(RL1, RL2, RL3, RL4, RH1, RH2, RH3, RH4);

	B0 = _mm_blend_epi16(M2, M3, 0xF0);
	B1 = _mm_unpackhi_epi64(M7, M0);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -32);
	RH4 = _mm_roti_epi64(RH4, -32);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -24);
	RH2 = _mm_roti_epi64(RH2, -24);

	B0 = _mm_unpackhi_epi64(M6, M2);
	B1 = _mm_blend_epi16(M7, M4, 0xF0);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -16);
	RH4 = _mm_roti_epi64(RH4, -16);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -63);
	RH2 = _mm_roti_epi64(RH2, -63);

	UnDiagonalize(RL1, RL2, RL3, RL4, RH1, RH2, RH3, RH4);

	// round 7
	B0 = _mm_blend_epi16(M6, M0, 0xF0);
	B1 = _mm_unpacklo_epi64(M7, M2);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -32);
	RH4 = _mm_roti_epi64(RH4, -32);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -24);
	RH2 = _mm_roti_epi64(RH2, -24);

	B0 = _mm_unpackhi_epi64(M2, M7);
	B1 = _mm_alignr_epi8(M5, M6, 8);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -16);
	RH4 = _mm_roti_epi64(RH4, -16);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -63);
	RH2 = _mm_roti_epi64(RH2, -63);

	Diagonalize(RL1, RL2, RL3, RL4, RH1, RH2, RH3, RH4);

	B0 = _mm_unpacklo_epi64(M0, M3);
	B1 = _mm_shuffle_epi32(M4, _MM_SHUFFLE(1, 0, 3, 2));
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -32);
	RH4 = _mm_roti_epi64(RH4, -32);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -24);
	RH2 = _mm_roti_epi64(RH2, -24);

	B0 = _mm_unpackhi_epi64(M3, M1);
	B1 = _mm_blend_epi16(M1, M5, 0xF0);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -16);
	RH4 = _mm_roti_epi64(RH4, -16);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -63);
	RH2 = _mm_roti_epi64(RH2, -63);

	UnDiagonalize(RL1, RL2, RL3, RL4, RH1, RH2, RH3, RH4);

	// round 8
	B0 = _mm_unpackhi_epi64(M6, M3);
	B1 = _mm_blend_epi16(M6, M1, 0xF0);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -32);
	RH4 = _mm_roti_epi64(RH4, -32);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -24);
	RH2 = _mm_roti_epi64(RH2, -24);

	B0 = _mm_alignr_epi8(M7, M5, 8);
	B1 = _mm_unpackhi_epi64(M0, M4);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -16);
	RH4 = _mm_roti_epi64(RH4, -16);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -63);
	RH2 = _mm_roti_epi64(RH2, -63);

	Diagonalize(RL1, RL2, RL3, RL4, RH1, RH2, RH3, RH4);

	B0 = _mm_unpackhi_epi64(M2, M7);
	B1 = _mm_unpacklo_epi64(M4, M1);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -32);
	RH4 = _mm_roti_epi64(RH4, -32);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -24);
	RH2 = _mm_roti_epi64(RH2, -24);

	B0 = _mm_unpacklo_epi64(M0, M2);
	B1 = _mm_unpacklo_epi64(M3, M5);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -16);
	RH4 = _mm_roti_epi64(RH4, -16);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -63);
	RH2 = _mm_roti_epi64(RH2, -63);

	UnDiagonalize(RL1, RL2, RL3, RL4, RH1, RH2, RH3, RH4);

	// round 9
	B0 = _mm_unpacklo_epi64(M3, M7);
	B1 = _mm_alignr_epi8(M0, M5, 8);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -32);
	RH4 = _mm_roti_epi64(RH4, -32);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -24);
	RH2 = _mm_roti_epi64(RH2, -24);

	B0 = _mm_unpackhi_epi64(M7, M4);
	B1 = _mm_alignr_epi8(M4, M1, 8);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -16);
	RH4 = _mm_roti_epi64(RH4, -16);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -63);
	RH2 = _mm_roti_epi64(RH2, -63);

	Diagonalize(RL1, RL2, RL3, RL4, RH1, RH2, RH3, RH4);

	B0 = M6;
	B1 = _mm_alignr_epi8(M5, M0, 8);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -32);
	RH4 = _mm_roti_epi64(RH4, -32);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -24);
	RH2 = _mm_roti_epi64(RH2, -24);

	B0 = _mm_blend_epi16(M1, M3, 0xF0);
	B1 = M2;
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -16);
	RH4 = _mm_roti_epi64(RH4, -16);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -63);
	RH2 = _mm_roti_epi64(RH2, -63);

	UnDiagonalize(RL1, RL2, RL3, RL4, RH1, RH2, RH3, RH4);

	// round 10
	B0 = _mm_unpacklo_epi64(M5, M4);
	B1 = _mm_unpackhi_epi64(M3, M0);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -32);
	RH4 = _mm_roti_epi64(RH4, -32);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -24);
	RH2 = _mm_roti_epi64(RH2, -24);

	B0 = _mm_unpacklo_epi64(M1, M2);
	B1 = _mm_blend_epi16(M3, M2, 0xF0);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -16);
	RH4 = _mm_roti_epi64(RH4, -16);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -63);
	RH2 = _mm_roti_epi64(RH2, -63);

	Diagonalize(RL1, RL2, RL3, RL4, RH1, RH2, RH3, RH4);

	B0 = _mm_unpackhi_epi64(M7, M4);
	B1 = _mm_unpackhi_epi64(M1, M6);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -32);
	RH4 = _mm_roti_epi64(RH4, -32);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -24);
	RH2 = _mm_roti_epi64(RH2, -24);

	B0 = _mm_alignr_epi8(M7, M5, 8);
	B1 = _mm_unpacklo_epi64(M6, M0);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -16);
	RH4 = _mm_roti_epi64(RH4, -16);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -63);
	RH2 = _mm_roti_epi64(RH2, -63);

	UnDiagonalize(RL1, RL2, RL3, RL4, RH1, RH2, RH3, RH4);

	// round 11
	B0 = _mm_unpacklo_epi64(M0, M1);
	B1 = _mm_unpacklo_epi64(M2, M3);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -32);
	RH4 = _mm_roti_epi64(RH4, -32);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -24);
	RH2 = _mm_roti_epi64(RH2, -24);

	B0 = _mm_unpackhi_epi64(M0, M1);
	B1 = _mm_unpackhi_epi64(M2, M3);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -16);
	RH4 = _mm_roti_epi64(RH4, -16);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -63);
	RH2 = _mm_roti_epi64(RH2, -63);

	Diagonalize(RL1, RL2, RL3, RL4, RH1, RH2, RH3, RH4);

	B0 = _mm_unpacklo_epi64(M4, M5);
	B1 = _mm_unpacklo_epi64(M6, M7);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -32);
	RH4 = _mm_roti_epi64(RH4, -32);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -24);
	RH2 = _mm_roti_epi64(RH2, -24);

	B0 = _mm_unpackhi_epi64(M4, M5);
	B1 = _mm_unpackhi_epi64(M6, M7);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -16);
	RH4 = _mm_roti_epi64(RH4, -16);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -63);
	RH2 = _mm_roti_epi64(RH2, -63);

	UnDiagonalize(RL1, RL2, RL3, RL4, RH1, RH2, RH3, RH4);

	// round 12
	B0 = _mm_unpacklo_epi64(M7, M2);
	B1 = _mm_unpackhi_epi64(M4, M6);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -32);
	RH4 = _mm_roti_epi64(RH4, -32);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -24);
	RH2 = _mm_roti_epi64(RH2, -24);

	B0 = _mm_unpacklo_epi64(M5, M4);
	B1 = _mm_alignr_epi8(M3, M7, 8);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -16);
	RH4 = _mm_roti_epi64(RH4, -16);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -63);
	RH2 = _mm_roti_epi64(RH2, -63);

	Diagonalize(RL1, RL2, RL3, RL4, RH1, RH2, RH3, RH4);

	B0 = _mm_shuffle_epi32(M0, _MM_SHUFFLE(1, 0, 3, 2));
	B1 = _mm_unpackhi_epi64(M5, M2);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -32);
	RH4 = _mm_roti_epi64(RH4, -32);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -24);
	RH2 = _mm_roti_epi64(RH2, -24);

	B0 = _mm_unpacklo_epi64(M6, M1);
	B1 = _mm_unpackhi_epi64(M3, M1);
	RL1 = _mm_add_epi64(_mm_add_epi64(RL1, B0), RL2);
	RH1 = _mm_add_epi64(_mm_add_epi64(RH1, B1), RH2);
	RL4 = _mm_xor_si128(RL4, RL1);
	RH4 = _mm_xor_si128(RH4, RH1);
	RL4 = _mm_roti_epi64(RL4, -16);
	RH4 = _mm_roti_epi64(RH4, -16);
	RL3 = _mm_add_epi64(RL3, RL4);
	RH3 = _mm_add_epi64(RH3, RH4);
	RL2 = _mm_xor_si128(RL2, RL3);
	RH2 = _mm_xor_si128(RH2, RH3);
	RL2 = _mm_roti_epi64(RL2, -63);
	RH2 = _mm_roti_epi64(RH2, -63);

	UnDiagonalize(RL1, RL2, RL3, RL4, RH1, RH2, RH3, RH4);

	RL1 = _mm_xor_si128(RL3, RL1);
	RH1 = _mm_xor_si128(RH3, RH1);
	_mm_storeu_si128(reinterpret_cast<__m128i*>(&State[0]), _mm_xor_si128(_mm_loadu_si128(reinterpret_cast<const __m128i*>(&State[0])), RL1));
	_mm_storeu_si128(reinterpret_cast<__m128i*>(&State[2]), _mm_xor_si128(_mm_loadu_si128(reinterpret_cast<const __m128i*>(&State[2])), RH1));
	RL2 = _mm_xor_si128(RL4, RL2);
	RH2 = _mm_xor_si128(RH4, RH2);
	_mm_storeu_si128(reinterpret_cast<__m128i*>(&State[4]), _mm_xor_si128(_mm_loadu_si128(reinterpret_cast<const __m128i*>(&State[4])), RL2));
	_mm_storeu_si128(reinterpret_cast<__m128i*>(&State[6]), _mm_xor_si128(_mm_loadu_si128(reinterpret_cast<const __m128i*>(&State[6])), RH2));

#else
	PermuteR12P1024U(Input, InOffset, State, IV);
#endif
}

#if defined(__AVX__)

void Blake2::PermuteR12P4096H(const std::vector<byte> &Input, size_t InOffset, std::vector<ULong256> &State, const std::vector<ULong256> &IV)
{
	std::array<ULong256, 16> M;
	std::array<ULong256, 16> R{
		State[0],
		State[1],
		State[2],
		State[3],
		State[4],
		State[5],
		State[6],
		State[7],
		IV[0],
		IV[1],
		IV[2],
		IV[3],
		IV[4],
		IV[5],
		IV[6],
		IV[7] };
	size_t i;


#if defined(CEX_IS_LITTLE_ENDIAN)
	MemUtils::Copy(Input, InOffset, M, 0, M.size() * sizeof(ULong256));
#else
	for (i = 0; i < 16; ++i)
	{
		M[i].Load(
			IntUtils::LeBytesTo64(Input, InOffset + (i * 8)),
			IntUtils::LeBytesTo64(Input, InOffset + (i * 8) + 128),
			IntUtils::LeBytesTo64(Input, InOffset + (i * 8) + 256),
			IntUtils::LeBytesTo64(Input, InOffset + (i * 8) + 384));
	}
#endif

	for (i = 0; i < 12; ++i)
	{
		// round 0
		R[0] += R[4] + M[Sigma512[(i * 16)]];
		R[12] ^= R[0];
		R[12] = ((R[12] >> 32) | (R[12] << (64 - 32)));
		R[8] += R[12];
		R[4] ^= R[8];
		R[4] = ((R[4] >> 24) | (R[4] << (64 - 24)));
		R[0] += R[4] + M[Sigma512[(i * 16) + 1]];
		R[12] ^= R[0];
		R[12] = ((R[12] >> 16) | (R[12] << (64 - 16)));
		R[8] += R[12];
		R[4] ^= R[8];
		R[4] = ((R[4] >> 63) | (R[4] << (64 - 63)));

		R[1] += R[5] + M[Sigma512[(i * 16) + 2]];
		R[13] ^= R[1];
		R[13] = ((R[13] >> 32) | (R[13] << (64 - 32)));
		R[9] += R[13];
		R[5] ^= R[9];
		R[5] = ((R[5] >> 24) | (R[5] << (64 - 24)));
		R[1] += R[5] + M[Sigma512[(i * 16) + 3]];
		R[13] ^= R[1];
		R[13] = ((R[13] >> 16) | (R[13] << (64 - 16)));
		R[9] += R[13];
		R[5] ^= R[9];
		R[5] = ((R[5] >> 63) | (R[5] << (64 - 63)));

		R[2] += R[6] + M[Sigma512[(i * 16) + 4]];
		R[14] ^= R[2];
		R[14] = ((R[14] >> 32) | (R[14] << (64 - 32)));
		R[10] += R[14];
		R[6] ^= R[10];
		R[6] = ((R[6] >> 24) | (R[6] << (64 - 24)));
		R[2] += R[6] + M[Sigma512[(i * 16) + 5]];
		R[14] ^= R[2];
		R[14] = ((R[14] >> 16) | (R[14] << (64 - 16)));
		R[10] += R[14];
		R[6] ^= R[10];
		R[6] = ((R[6] >> 63) | (R[6] << (64 - 63)));

		R[3] += R[7] + M[Sigma512[(i * 16) + 6]];
		R[15] ^= R[3];
		R[15] = ((R[15] >> 32) | (R[15] << (64 - 32)));
		R[11] += R[15];
		R[7] ^= R[11];
		R[7] = ((R[7] >> 24) | (R[7] << (64 - 24)));
		R[3] += R[7] + M[Sigma512[(i * 16) + 7]];
		R[15] ^= R[3];
		R[15] = ((R[15] >> 16) | (R[15] << (64 - 16)));
		R[11] += R[15];
		R[7] ^= R[11];
		R[7] = ((R[7] >> 63) | (R[7] << (64 - 63)));

		R[0] += R[5] + M[Sigma512[(i * 16) + 8]];
		R[15] ^= R[0];
		R[15] = ((R[15] >> 32) | (R[15] << (64 - 32)));
		R[10] += R[15];
		R[5] ^= R[10];
		R[5] = ((R[5] >> 24) | (R[5] << (64 - 24)));
		R[0] += R[5] + M[Sigma512[(i * 16) + 9]];
		R[15] ^= R[0];
		R[15] = ((R[15] >> 16) | (R[15] << (64 - 16)));
		R[10] += R[15];
		R[5] ^= R[10];
		R[5] = ((R[5] >> 63) | (R[5] << (64 - 63)));

		R[1] += R[6] + M[Sigma512[(i * 16) + 10]];
		R[12] ^= R[1];
		R[12] = ((R[12] >> 32) | (R[12] << (64 - 32)));
		R[11] += R[12];
		R[6] ^= R[11];
		R[6] = ((R[6] >> 24) | (R[6] << (64 - 24)));
		R[1] += R[6] + M[Sigma512[(i * 16) + 11]];
		R[12] ^= R[1];
		R[12] = ((R[12] >> 16) | (R[12] << (64 - 16)));
		R[11] += R[12];
		R[6] ^= R[11];
		R[6] = ((R[6] >> 63) | (R[6] << (64 - 63)));

		R[2] += R[7] + M[Sigma512[(i * 16) + 12]];
		R[13] ^= R[2];
		R[13] = ((R[13] >> 32) | (R[13] << (64 - 32)));
		R[8] += R[13];
		R[7] ^= R[8];
		R[7] = ((R[7] >> 24) | (R[7] << (64 - 24)));
		R[2] += R[7] + M[Sigma512[(i * 16) + 13]];
		R[13] ^= R[2];
		R[13] = ((R[13] >> 16) | (R[13] << (64 - 16)));
		R[8] += R[13];
		R[7] ^= R[8];
		R[7] = ((R[7] >> 63) | (R[7] << (64 - 63)));

		R[3] += R[4] + M[Sigma512[(i * 16) + 14]];
		R[14] ^= R[3];
		R[14] = ((R[14] >> 32) | (R[14] << (64 - 32)));
		R[9] += R[14];
		R[4] ^= R[9];
		R[4] = ((R[4] >> 24) | (R[4] << (64 - 24)));
		R[3] += R[4] + M[Sigma512[(i * 16) + 15]];
		R[14] ^= R[3];
		R[14] = ((R[14] >> 16) | (R[14] << (64 - 16)));
		R[9] += R[14];
		R[4] ^= R[9];
		R[4] = ((R[4] >> 63) | (R[4] << (64 - 63)));
	}

	State[0] ^= R[0] ^ R[8];
	State[1] ^= R[1] ^ R[9];
	State[2] ^= R[2] ^ R[10];
	State[3] ^= R[3] ^ R[11];
	State[4] ^= R[4] ^ R[12];
	State[5] ^= R[5] ^ R[13];
	State[6] ^= R[6] ^ R[14];
	State[7] ^= R[7] ^ R[15];
}

#endif

#if defined(__AVX512__)

void Blake2::PermuteR12P8192H(const std::vector<byte> &Input, size_t InOffset, std::vector<ULong512> &State, const std::vector<ULong512> &IV)
{
	std::array<ULong512, 16> M;
	std::array<ULong512, 16> R{
		State[0],
		State[1],
		State[2],
		State[3],
		State[4],
		State[5],
		State[6],
		State[7],
		IV[0],
		IV[1],
		IV[2],
		IV[3],
		IV[4],
		IV[5],
		IV[6],
		IV[7] };
	size_t i;


#if defined(CEX_IS_LITTLE_ENDIAN)
	MemUtils::Copy(Input, InOffset, M, 0, M.size() * sizeof(ULong512));
#else
	for (i = 0; i < 16; ++i)
	{
		M[i].Load(
			IntUtils::LeBytesTo64(Input, InOffset + (i * 8)),
			IntUtils::LeBytesTo64(Input, InOffset + (i * 8) + 128),
			IntUtils::LeBytesTo64(Input, InOffset + (i * 8) + 256),
			IntUtils::LeBytesTo64(Input, InOffset + (i * 8) + 384),
			IntUtils::LeBytesTo64(Input, InOffset + (i * 8) + 512),
			IntUtils::LeBytesTo64(Input, InOffset + (i * 8) + 640),
			IntUtils::LeBytesTo64(Input, InOffset + (i * 8) + 768),
			IntUtils::LeBytesTo64(Input, InOffset + (i * 8) + 896));
	}
#endif

	for (i = 0; i < 12; ++i)
	{
		// round 0
		R[0] += R[4] + M[Sigma512[(i * 16)]];
		R[12] ^= R[0];
		R[12] = ((R[12] >> 32) | (R[12] << (64 - 32)));
		R[8] += R[12];
		R[4] ^= R[8];
		R[4] = ((R[4] >> 24) | (R[4] << (64 - 24)));
		R[0] += R[4] + M[Sigma512[(i * 16) + 1]];
		R[12] ^= R[0];
		R[12] = ((R[12] >> 16) | (R[12] << (64 - 16)));
		R[8] += R[12];
		R[4] ^= R[8];
		R[4] = ((R[4] >> 63) | (R[4] << (64 - 63)));

		R[1] += R[5] + M[Sigma512[(i * 16) + 2]];
		R[13] ^= R[1];
		R[13] = ((R[13] >> 32) | (R[13] << (64 - 32)));
		R[9] += R[13];
		R[5] ^= R[9];
		R[5] = ((R[5] >> 24) | (R[5] << (64 - 24)));
		R[1] += R[5] + M[Sigma512[(i * 16) + 3]];
		R[13] ^= R[1];
		R[13] = ((R[13] >> 16) | (R[13] << (64 - 16)));
		R[9] += R[13];
		R[5] ^= R[9];
		R[5] = ((R[5] >> 63) | (R[5] << (64 - 63)));

		R[2] += R[6] + M[Sigma512[(i * 16) + 4]];
		R[14] ^= R[2];
		R[14] = ((R[14] >> 32) | (R[14] << (64 - 32)));
		R[10] += R[14];
		R[6] ^= R[10];
		R[6] = ((R[6] >> 24) | (R[6] << (64 - 24)));
		R[2] += R[6] + M[Sigma512[(i * 16) + 5]];
		R[14] ^= R[2];
		R[14] = ((R[14] >> 16) | (R[14] << (64 - 16)));
		R[10] += R[14];
		R[6] ^= R[10];
		R[6] = ((R[6] >> 63) | (R[6] << (64 - 63)));

		R[3] += R[7] + M[Sigma512[(i * 16) + 6]];
		R[15] ^= R[3];
		R[15] = ((R[15] >> 32) | (R[15] << (64 - 32)));
		R[11] += R[15];
		R[7] ^= R[11];
		R[7] = ((R[7] >> 24) | (R[7] << (64 - 24)));
		R[3] += R[7] + M[Sigma512[(i * 16) + 7]];
		R[15] ^= R[3];
		R[15] = ((R[15] >> 16) | (R[15] << (64 - 16)));
		R[11] += R[15];
		R[7] ^= R[11];
		R[7] = ((R[7] >> 63) | (R[7] << (64 - 63)));

		R[0] += R[5] + M[Sigma512[(i * 16) + 8]];
		R[15] ^= R[0];
		R[15] = ((R[15] >> 32) | (R[15] << (64 - 32)));
		R[10] += R[15];
		R[5] ^= R[10];
		R[5] = ((R[5] >> 24) | (R[5] << (64 - 24)));
		R[0] += R[5] + M[Sigma512[(i * 16) + 9]];
		R[15] ^= R[0];
		R[15] = ((R[15] >> 16) | (R[15] << (64 - 16)));
		R[10] += R[15];
		R[5] ^= R[10];
		R[5] = ((R[5] >> 63) | (R[5] << (64 - 63)));

		R[1] += R[6] + M[Sigma512[(i * 16) + 10]];
		R[12] ^= R[1];
		R[12] = ((R[12] >> 32) | (R[12] << (64 - 32)));
		R[11] += R[12];
		R[6] ^= R[11];
		R[6] = ((R[6] >> 24) | (R[6] << (64 - 24)));
		R[1] += R[6] + M[Sigma512[(i * 16) + 11]];
		R[12] ^= R[1];
		R[12] = ((R[12] >> 16) | (R[12] << (64 - 16)));
		R[11] += R[12];
		R[6] ^= R[11];
		R[6] = ((R[6] >> 63) | (R[6] << (64 - 63)));

		R[2] += R[7] + M[Sigma512[(i * 16) + 12]];
		R[13] ^= R[2];
		R[13] = ((R[13] >> 32) | (R[13] << (64 - 32)));
		R[8] += R[13];
		R[7] ^= R[8];
		R[7] = ((R[7] >> 24) | (R[7] << (64 - 24)));
		R[2] += R[7] + M[Sigma512[(i * 16) + 13]];
		R[13] ^= R[2];
		R[13] = ((R[13] >> 16) | (R[13] << (64 - 16)));
		R[8] += R[13];
		R[7] ^= R[8];
		R[7] = ((R[7] >> 63) | (R[7] << (64 - 63)));

		R[3] += R[4] + M[Sigma512[(i * 16) + 14]];
		R[14] ^= R[3];
		R[14] = ((R[14] >> 32) | (R[14] << (64 - 32)));
		R[9] += R[14];
		R[4] ^= R[9];
		R[4] = ((R[4] >> 24) | (R[4] << (64 - 24)));
		R[3] += R[4] + M[Sigma512[(i * 16) + 15]];
		R[14] ^= R[3];
		R[14] = ((R[14] >> 16) | (R[14] << (64 - 16)));
		R[9] += R[14];
		R[4] ^= R[9];
		R[4] = ((R[4] >> 63) | (R[4] << (64 - 63)));
	}

	State[0] ^= R[0] ^ R[8];
	State[1] ^= R[1] ^ R[9];
	State[2] ^= R[2] ^ R[10];
	State[3] ^= R[3] ^ R[11];
	State[4] ^= R[4] ^ R[12];
	State[5] ^= R[5] ^ R[13];
	State[6] ^= R[6] ^ R[14];
	State[7] ^= R[7] ^ R[15];
}

#endif
NAMESPACE_DIGESTEND